<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="3hx6hRZ1EJWUbqkkXZ-vPQfpV0JdKNKT_aKcRjSixr0"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> An Introductory Look at Covariance and the Mean Vector | Shuhong Dai </title> <meta name="author" content="Shuhong Dai"> <meta name="description" content="If the mean vector gives us a sense of location, then the covariance matrix gives us a sense of shape..."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?6d57c5bac70ef6fae4bd96883a4eb4da"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shuhong</span> Dai </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">An Introductory Look at Covariance and the Mean Vector</h1> <p class="post-meta"> Created in June 11, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/tag/stochastic-processes"> <i class="fa-solid fa-hashtag fa-sm"></i> Stochastic Processes</a>   ·   <a href="/blog/category/a-column-on-gaussian-processes"> <i class="fa-solid fa-tag fa-sm"></i> A Column on Gaussian Processes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>When you first encounter the world of multivariate Gaussian distributions, it’s easy to feel like you’ve entered a labyrinth of equations, variables, and matrices. But beneath the mathematical machinery lies a beautifully structured framework that helps us understand complex data, and in many ways, it’s as elegant as it is powerful.</p> <p>In statistics, the Gaussian, or “normal” distribution, is often our first stop when we dive into data analysis. We’ve all seen its familiar bell-shaped curve, neatly centered around a mean, showing us the most probable values a single variable might take. But in reality, data rarely exists in a vacuum. Many variables are interconnected, forming a “multivariate” data landscape where each variable influences and interacts with others. Here, the multivariate Gaussian distribution steps in as a natural extension of the single-variable Gaussian, providing us a way to model these multidimensional relationships.</p> <p>At the heart of this distribution are two key players: the mean vector and the covariance matrix. The mean vector is the multivariate equivalent of the single-variable mean, summarizing the central tendencies of all variables in one go. It tells us where the “center” of our data cloud lies, capturing the average or typical values across all dimensions.</p> <blockquote> <p>Updated on June 22, 2024: Additionally, <a href="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/">a new blog post</a> has been published that expands on this topic with content covering correlation coefficients and the correlation coefficient matrix.</p> </blockquote> <p>The covariance matrix, on the other hand, is a bit like a backstage operator. It describes how variables interact with each other, revealing not just their individual spreads but also how they move in tandem. Each entry in this matrix provides insights into the relationship between pairs of variables, showing whether they rise and fall together or behave independently. Together, the mean vector and covariance matrix form a powerful duo, shaping the geometry of the distribution and giving us a complete picture of how our data points are scattered and related.</p> <p>In this post, we’ll explore the roles of the mean vector and covariance matrix within the multivariate Gaussian distribution, diving into how they help us grasp and model complex data structures.</p> <hr> <h2 id="the-mean-vector--definition-and-properties">The Mean Vector – Definition and Properties</h2> <p>Now that we’ve opened the door to the multivariate Gaussian, let’s take a closer look at one of its core components: the mean vector. You can think of the mean vector as the “centroid” or the “anchor” point of the distribution—a snapshot of where the average of each variable in a dataset tends to lie. In a multivariate world, we don’t just care about one mean; we want to know the average value in each dimension, and that’s where the mean vector steps in.</p> <h3 id="definition">Definition</h3> <p>For a multivariate random variable \(X\) with \(n\) dimensions, the mean vector \(\mu\) is defined as:</p> \[\mu = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix},\] <p>where each \(\mu_i = E[X_i]\) represents the expected value of the \(i\)-th variable. Essentially, the mean vector \(\mu\) gives us a one-stop summary of the “average” position of all dimensions, capturing the expected value along each axis of the multidimensional data space.</p> <h3 id="derivation-expectation-of-the-mean-vector">Derivation: Expectation of the Mean Vector</h3> <p>To fully appreciate the mean vector, let’s delve into its calculation using the expectation operator. Suppose \(X = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix}\) is our multivariate random variable, where each \(X_i\) is a random variable in itself. The mean vector \(\mu\) is simply the expected value of \(X\):</p> \[\mu = E[X].\] <p>Breaking this down, <strong>the expectation of \(X\) is computed component-wise</strong>. That is,</p> \[\mu = E[X] = \begin{bmatrix} E[X_1] \\ E[X_2] \\ \vdots \\ E[X_n] \end{bmatrix} = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix}.\] <p>This form allows us to treat each dimension independently when calculating the mean, <strong>even though they may be interdependent in terms of their distributions.</strong></p> <h3 id="properties-of-the-mean-vector">Properties of the Mean Vector</h3> <p>The mean vector isn’t just a passive summary of averages; it’s highly responsive to transformations, particularly linear ones. Let’s explore one of its key properties: how it behaves under a linear transformation. Consider a linear transformation where we define a new random vector \(Y\) based on \(X\) as:</p> \[Y = AX + b,\] <p>where \(A\) is a constant matrix of dimensions \(m \times n\) and \(b\) is a constant vector of dimension \(m \times 1\). This setup is common in multivariate analysis, where we often transform data to new coordinate systems or scales. Here, we want to understand how this transformation impacts the mean vector.</p> <p>To derive the expected value of \(Y\), we use the linearity of expectation:</p> \[E[Y] = E[AX + b].\] <p>Since \(A\) and \(b\) are constants, we can simplify:</p> \[E[Y] = AE[X] + b = A\mu + b.\] <p>So, the transformed mean vector of \(Y\) is given by \(A\mu + b\). This tells us that linear transformations shift and stretch the mean vector in predictable ways: multiplying by \(A\) scales or rotates \(\mu\), while adding \(b\) translates it. Let’s formalize this with a quick proof. Suppose \(X\) is a multivariate random variable with mean vector \(\mu = E[X]\), and we define \(Y = AX + b\). By the definition of expectation, we have:</p> \[E[Y] = E[AX + b] = E[AX] + E[b].\] <p>Since \(b\) is constant, \(E[b] = b\). Additionally, because expectation is a linear operator, we get \(E[AX] = A E[X] = A \mu\). Thus,</p> \[E[Y] = A \mu + b.\] <p>This property is not only elegant but incredibly useful. It implies that, regardless of the transformation (as long as it’s linear), we can predict how the mean vector shifts without recalculating everything from scratch. This “transformation invariance” simplifies a lot of practical work in data analysis, letting us predict and manipulate mean vectors in transformed spaces.</p> <p>In sum, the mean vector \(\mu\) isn’t just a set of averages. It’s a fundamental descriptor that tells us where our data is centered, and its behavior under transformations is both consistent and computationally friendly. <strong>Whether you’re scaling, rotating, or translating your data, the mean vector adjusts accordingly, maintaining its role as the central anchor of your multivariate Gaussian distribution.</strong></p> <h3 id="demo">Demo</h3> <p>To understand how the mean vector \(\mu\) behaves, particularly under transformations, let’s start by generating some multivariate data. We’ll create a simple 2D Gaussian distribution, calculate its mean vector, and then apply a linear transformation to see how \(\mu\) changes in response.</p> <p>Here’s a Python script that does exactly this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Set a random seed for reproducibility
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Step 1: Define the original mean vector and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>  <span class="c1"># Positive correlation between X1 and X2
</span>
<span class="c1"># Generate a sample of 500 points from the 2D Gaussian distribution
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Plot the original distribution
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Original Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector (Original)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Original Multivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, we initialize a mean vector \(\mu = [2, 3]\) and a covariance matrix with a positive correlation between the two variables. We then generate 500 points to visually represent the data cloud centered around \(\mu\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution-480.webp 480w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution-800.webp 800w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution.png" class="img-fluid" width="600" height="400" alt="Original Multivariate Gaussian Distribution" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Now, let’s apply a linear transformation to this data. For demonstration, we’ll use a transformation matrix \(A = \begin{bmatrix} 1.5 &amp; 0 \\ 0.5 &amp; 1 \end{bmatrix}\) and a translation vector \(b = \begin{bmatrix} -1 \\ 2 \end{bmatrix}\). According to our derivation, we expect the mean vector to change to \(A\mu + b\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 2: Define the transformation matrix A and translation vector b
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Apply the transformation to each data point
</span><span class="n">transformed_data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Calculate the transformed mean vector
</span><span class="n">transformed_mu</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Plot the transformed distribution
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Transformed Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">transformed_mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">transformed_mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector (Transformed)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Transformed Multivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>This second snippet applies our transformation and then calculates the new mean vector using \(A\mu + b\), aligning with our theoretical result. Here’s what we observe from the plot:</p> <ol> <li> <strong>Data Shift and Rotation</strong>: The data cloud shifts according to the translation vector \(b\) and stretches based on the transformation matrix \(A\).</li> <li> <strong>Mean Vector Update</strong>: The new mean vector \(\mu\) moves precisely to \(A\mu + b\), as predicted.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution-480.webp 480w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution-800.webp 800w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution.png" class="img-fluid" width="600" height="400" alt="Transformed Multivariate Gaussian Distribution" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <hr> <h2 id="covariance-matrix--definition-and-computation">Covariance Matrix – Definition and Computation</h2> <p>If the mean vector \(\mu\) gives us a sense of location, <strong>then the covariance matrix \(\Sigma\) gives us a sense of shape.</strong> It describes how variables are spread and how they relate to each other, capturing both individual variances and pairwise covariances.</p> <h3 id="definition-1">Definition</h3> <p>The covariance matrix \(\Sigma\) of a multivariate random variable \(X = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix}\) is defined as:</p> \[\Sigma = \begin{bmatrix} \text{Cov}(X_1, X_1) &amp; \text{Cov}(X_1, X_2) &amp; \cdots &amp; \text{Cov}(X_1, X_n) \\ \text{Cov}(X_2, X_1) &amp; \text{Cov}(X_2, X_2) &amp; \cdots &amp; \text{Cov}(X_2, X_n) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \text{Cov}(X_n, X_1) &amp; \text{Cov}(X_n, X_2) &amp; \cdots &amp; \text{Cov}(X_n, X_n) \end{bmatrix},\] <p>where each element \(\Sigma\_{ij} = \text{Cov}(X_i, X_j)\) is the covariance between \(X_i\) and \(X_j\).</p> <p>Covariance, in essence, measures the degree to which two variables vary together. When \(i = j\), \(\Sigma\_{ii} = \text{Var}(X_i)\), representing the variance of \(X_i\) itself.</p> <h3 id="derivation-the-formula-for-covariance">Derivation: The Formula for Covariance</h3> <p>To formally compute \(\Sigma\), let’s start with the mean vector \(\mu\) of \(X\), defined as:</p> \[\mu = E[X] = \begin{bmatrix} E[X_1] \\ E[X_2] \\ \vdots \\ E[X_n] \end{bmatrix}.\] <p>Now, the covariance matrix is calculated as the expectation of the outer product of the deviations of \(X\) from its mean:</p> \[\Sigma = E[(X - \mu)(X - \mu)^T].\] <p>This formula may look abstract, but it’s grounded in a straightforward concept: by centering \(X\) around its mean (i.e., subtracting \(\mu\)) and then taking the outer product, we capture the spread and co-spread of each variable pair.</p> <h4 id="step-by-step-derivation-of-sigma">Step-by-Step Derivation of \(\Sigma\)</h4> <p>Let’s expand the formula a bit to understand the inner workings. We have:</p> \[\Sigma = E \left[ \begin{bmatrix} X_1 - \mu_1 \\ X_2 - \mu_2 \\ \vdots \\ X_n - \mu_n \end{bmatrix} \begin{bmatrix} X_1 - \mu_1 &amp; X_2 - \mu_2 &amp; \cdots &amp; X_n - \mu_n \end{bmatrix} \right].\] <p>When we take the expectation, each element \(\Sigma\_{ij}\) becomes:</p> \[\Sigma_{ij} = E[(X_i - \mu_i)(X_j - \mu_j)].\] <p>This is simply the definition of covariance between \(X_i\) and \(X_j\). As such, \(\Sigma\) encodes all pairwise relationships in one matrix, giving us a complete picture of how our variables are interconnected.</p> <h3 id="properties-of-the-covariance-matrix">Properties of the Covariance Matrix</h3> <p>The covariance matrix isn’t just a convenient summary; it has some fascinating mathematical properties that make it a powerful tool in multivariate analysis.</p> <h4 id="symmetry">Symmetry</h4> <p>One of the most fundamental properties of \(\Sigma\) is that it is symmetric. To see why, consider the definition of covariance:</p> \[\Sigma_{ij} = \text{Cov}(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)].\] <p>By the commutative property of multiplication, \((X_i - \mu_i)(X_j - \mu_j) = (X_j - \mu_j)(X_i - \mu_i)\). Thus:</p> \[\Sigma_{ij} = E[(X_i - \mu_i)(X_j - \mu_j)] = E[(X_j - \mu_j)(X_i - \mu_i)] = \Sigma_{ji}.\] <p>This symmetry property tells us that \(\Sigma\) is equal to its own transpose, or \(\Sigma = \Sigma^T\). This is crucial because it ensures that the eigenvalues of \(\Sigma\) are real, a feature that will come in handy when interpreting the distribution’s geometry.</p> <h4 id="positive-semi-definiteness">Positive Semi-Definiteness</h4> <p>Another key property of the covariance matrix is that it is positive semi-definite. Mathematically, this means that for any vector \(z\), we have:</p> \[z^T \Sigma z \geq 0.\] <p><strong>Intuitively, this property tells us that the “spread” of the data is never negative</strong>—a fundamental requirement for any meaningful measure of variance. Let’s quickly prove this property.</p> <p>For any vector \(z \in \mathbb{R}^n\), we have:</p> \[z^T \Sigma z = z^T E[(X - \mu)(X - \mu)^T] z = E[z^T (X - \mu)(X - \mu)^T z] = E[(z^T(X - \mu))^2].\] <p>Since \((z^T(X - \mu))^2\) is a square, it is always non-negative, which implies that \(z^T \Sigma z \geq 0\). Thus, \(\Sigma\) is positive semi-definite, meaning it has non-negative eigenvalues, another crucial feature for understanding data spread.</p> <h4 id="covariance-under-linear-transformation">Covariance under Linear Transformation</h4> <p>One of the most powerful aspects of the covariance matrix is how it transforms under linear operations. Suppose we apply a linear transformation \(Y = AX + b\), where \(A\) is a constant matrix and \(b\) is a constant vector. Just as we saw with the mean vector, we want to understand how the covariance matrix \(\Sigma\) changes under this transformation.</p> <p>The covariance of \(Y\), denoted \(\text{Cov}(Y)\), is given by:</p> \[\text{Cov}(Y) = E[(Y - E[Y])(Y - E[Y])^T].\] <p>Since \(Y = AX + b\), we can substitute and simplify:</p> \[Y - E[Y] = AX + b - (AE[X] + b) = A(X - E[X]).\] <p>Thus:</p> \[\text{Cov}(Y) = E[A(X - E[X])(X - E[X])^T A^T] = A E[(X - E[X])(X - E[X])^T] A^T = A \Sigma A^T.\] <p>This result, \(\text{Cov}(Y) = A \Sigma A^T\), tells us that under a linear transformation, the covariance matrix \(\Sigma\) transforms in a predictable manner. This property is critical in fields like machine learning and statistics, where data is often scaled or rotated to enhance interpretability or improve model performance.</p> <h3 id="demo-1">Demo</h3> <p>To illustrate the covariance matrix in action, let’s revisit our earlier 2D Gaussian example, but now focus on how the covariance affects the spread and orientation of the data cloud. We’ll also examine how the covariance matrix changes under a linear transformation, connecting this back to our theoretical results.</p> <h4 id="step-1-visualize-the-original-covariance-structure">Step 1: Visualize the Original Covariance Structure</h4> <p>In this example, we’ll use the same mean vector \(\mu = \begin{bmatrix} 2 \\ 3 \end{bmatrix}\) and a covariance matrix \(\Sigma = \begin{bmatrix} 1 &amp; 0.8 \\ 0.8 &amp; 1 \end{bmatrix}\). The positive off-diagonal values indicate a positive correlation between the two variables, meaning they tend to vary together.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Define the mean vector and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Generate data points from a 2D Gaussian distribution
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Plot the data cloud and the covariance structure
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Original Multivariate Gaussian Distribution with Covariance Structure</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="c1"># Visualize the covariance as an ellipse
</span><span class="kn">from</span> <span class="n">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>

<span class="c1"># Eigenvalues and eigenvectors of the covariance matrix
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># Width and height of the ellipse based on the eigenvalues (scaled for visibility)
</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>
<span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">'</span><span class="s">None</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Covariance Ellipse</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, we:</p> <ol> <li>Generate a data cloud based on \(\Sigma\), capturing the positive correlation between \(X_1\) and \(X_2\).</li> <li>Plot the data points along with the mean vector.</li> <li>Use the eigenvalues and eigenvectors of \(\Sigma\) to plot an ellipse representing the covariance structure. The orientation and size of this ellipse reflect the spread and correlation encoded in \(\Sigma\), with the longer axis aligned along the direction of greatest variance.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure-480.webp 480w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure-800.webp 800w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure.png" class="img-fluid" width="600" height="400" alt="Original Multivariate Gaussian Distribution with Covariance Structure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h4 id="step-2-apply-a-linear-transformation-and-observe-the-covariance-matrix-change">Step 2: Apply a Linear Transformation and Observe the Covariance Matrix Change</h4> <p>Next, let’s apply a linear transformation to our data. We’ll use a transformation matrix \(A = \begin{bmatrix} 1.2 &amp; 0.5 \\ 0.3 &amp; 0.8 \end{bmatrix}\), which will stretch and rotate the data. According to our earlier derivation, the new covariance matrix should be \(A \Sigma A^T\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the transformation matrix A
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>

<span class="c1"># Transform the data
</span><span class="n">transformed_data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span>

<span class="c1"># Calculate the transformed covariance matrix
</span><span class="n">transformed_cov</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">cov</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span>

<span class="c1"># Plot the transformed data and new covariance structure
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Transformed Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Transformed Multivariate Gaussian Distribution with New Covariance Structure</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Calculate and plot the new covariance ellipse
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">transformed_cov</span><span class="p">)</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>
<span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">A</span> <span class="o">@</span> <span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">'</span><span class="s">None</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Transformed Covariance Ellipse</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this visualization:</p> <ol> <li>We transform the data by applying \(A\), which stretches and rotates the original distribution.</li> <li>We compute the transformed covariance matrix as \(A \Sigma A^T\) and plot the new covariance ellipse to represent its structure.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure-480.webp 480w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure-800.webp 800w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure.png" class="img-fluid" width="600" height="400" alt="Transformed Multivariate Gaussian Distribution with New Covariance Structure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="observations">Observations</h3> <p>With this code, we see the impact of a linear transformation on the covariance matrix:</p> <ul> <li> <strong>Spread and Orientation</strong>: The transformed covariance ellipse is reshaped and reoriented. The principal axes of the ellipse align with the directions of greatest and least variance in the transformed data, which reflect the new covariance structure encoded by \(A \Sigma A^T\).</li> <li> <strong>Predicted Transformation</strong>: The calculation \(A \Sigma A^T\) matches the new spread, showing that even though the data has shifted in space, we can predict exactly how its variability changes.</li> </ul> <hr> <h2 id="the-geometric-meaning-of-the-covariance-matrix">The Geometric Meaning of the Covariance Matrix</h2> <p>At this point, we know the covariance matrix \(\Sigma\) defines the structure of a multivariate Gaussian distribution in terms of variance and covariance. But what does \(\Sigma\) really <em>look like</em>? In a two-dimensional space, the covariance matrix paints an elegant picture of geometry, describing a distribution’s shape as an ellipse. This section is a journey into the geometric implications of \(\Sigma\), where each feature of the covariance matrix corresponds to a unique aspect of the data’s spatial spread.</p> <h3 id="elliptical-contours-the-shape-of-data-in-2d">Elliptical Contours: The Shape of Data in 2D</h3> <p>In the case of a two-dimensional Gaussian distribution, the contours of equal density—think of these as the “outlines” or “borders” where data points tend to cluster—form concentric ellipses. These ellipses reveal the spread of data around the mean vector \(\mu\) and are directly determined by the covariance matrix \(\Sigma\).</p> <p>Each ellipse’s geometry—the length and direction of its axes—offers a visual interpretation of \(\Sigma\):</p> <ol> <li> <strong>Principal Axes (Direction)</strong>: The directions of the ellipse’s axes correspond to the eigenvectors of \(\Sigma\). These eigenvectors are vectors in space that point along the directions where the data varies the most (the “principal directions”).</li> <li> <strong>Axis Lengths (Spread)</strong>: The lengths of these axes are proportional to the square roots of the eigenvalues of \(\Sigma\). The larger an eigenvalue, the longer the axis, meaning that data stretches more along this direction. Smaller eigenvalues correspond to shorter axes, indicating less variability in that direction.</li> </ol> <h3 id="eigenvalue-decomposition-of-the-covariance-matrix">Eigenvalue Decomposition of the Covariance Matrix</h3> <p>To understand the ellipse’s structure more deeply, let’s look at the eigenvalue decomposition of \(\Sigma\). The covariance matrix \(\Sigma\) is symmetric, meaning it can be decomposed as:</p> \[\Sigma = Q \Lambda Q^T,\] <p>where:</p> <ul> <li>\(Q\) is a matrix of eigenvectors of \(\Sigma\), and</li> <li>\(\Lambda\) is a diagonal matrix of eigenvalues of \(\Sigma\), with each eigenvalue corresponding to the variance along a principal direction.</li> </ul> <p>This decomposition provides a clean geometric interpretation. If we rewrite our multivariate random variable \(X\) (centered at the origin for simplicity) as:</p> \[X = Q D Z,\] <p>where \(D = \sqrt{\Lambda}\) is the matrix of square roots of the eigenvalues and \(Z\) is a vector of standard normal variables, we can see that \(Q\) rotates the data along the principal directions and \(D\) scales it according to the variances.</p> <p>Let’s illustrate this in two steps.</p> <h4 id="step-1-visualize-the-original-data-with-eigenvectors">Step 1: Visualize the Original Data with Eigenvectors</h4> <p>We can start by plotting the original data and overlaying the eigenvectors of \(\Sigma\) to visualize the principal directions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Define mean and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>

<span class="c1"># Generate data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Plot the data
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Multivariate Gaussian with Principal Directions</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Compute eigenvalues and eigenvectors
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

<span class="c1"># Plot eigenvectors as principal directions
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">eigvecs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]],</span>
             <span class="p">[</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">eigvecs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]],</span>
             <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Principal Direction </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, we:</p> <ol> <li>Generate data according to the mean \(\mu\) and covariance matrix \(\Sigma\).</li> <li>Plot the data along with the mean vector.</li> <li>Compute the eigenvalues and eigenvectors of \(\Sigma\), and overlay the eigenvectors on the data, scaled by the square root of their corresponding eigenvalues to represent the primary directions and spread.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions-480.webp 480w,/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions-800.webp 800w,/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions.png" class="img-fluid" width="600" height="400" alt="Multivariate Gaussian with Principal Directions" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The eigenvectors point along the primary axes of the data, while their lengths, scaled by the square roots of the eigenvalues, indicate how far the data stretches along these axes. This visualization immediately tells us the directions in which our data is most (or least) spread out.</p> <h4 id="step-2-visualizing-the-covariance-matrix-as-an-ellipse">Step 2: Visualizing the Covariance Matrix as an Ellipse</h4> <p>To highlight the covariance structure even further, we can draw the ellipse representing a particular level set of our Gaussian density. Here’s the code to do so, using the eigenvalues and eigenvectors to construct an ellipse around the data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot data with covariance ellipse
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Covariance Ellipse of the Multivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Draw the ellipse
</span><span class="kn">from</span> <span class="n">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>  <span class="c1"># Ellipse axes scaled to represent variance
</span><span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">'</span><span class="s">None</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Covariance Ellipse</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>This plot gives us a clear visual representation of the covariance matrix’s “footprint” on the data:</p> <ul> <li> <strong>Direction</strong>: The ellipse’s major and minor axes correspond to the principal directions (eigenvectors) of the data spread.</li> <li> <strong>Length of Axes</strong>: The lengths of these axes are proportional to the square roots of the eigenvalues of \(\Sigma\), indicating the variance along each direction.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution-480.webp 480w,/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution-800.webp 800w,/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution.png" class="img-fluid" width="600" height="400" alt="Covariance Ellipse of the Multivariate Gaussian Distribution" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="geometric-takeaways">Geometric Takeaways</h3> <p>So, what does this ellipse tell us about the data’s geometry?</p> <ol> <li> <strong>Primary Spread</strong>: The major axis shows where the data is most dispersed, aligned along the eigenvector with the largest eigenvalue. This direction represents the direction of greatest variance.</li> <li> <strong>Secondary Spread</strong>: The minor axis, perpendicular to the major axis, aligns with the eigenvector of the smaller eigenvalue, showing where the data is more tightly clustered.</li> </ol> <p>This elliptical geometry offers a powerful intuition: <strong>the covariance matrix defines an oriented and scaled ellipse that encapsulates the data’s spread and directionality. By examining this shape, we can quickly grasp the underlying structure of the distribution—its direction of spread, symmetry (or lack thereof), and how tightly data points are packed.</strong></p> <hr> <h2 id="a-practical-example-of-deriving-the-covariance-matrix">A Practical Example of Deriving the Covariance Matrix</h2> <p>To ground our understanding of the covariance matrix in something concrete, let’s work through a hands-on example. We’ll take a simple two-dimensional random variable, calculate its mean vector and covariance matrix from a small dataset, and observe how these values summarize our data.</p> <p>Imagine we have a dataset representing the measurements of two variables, say \(X_1\) and \(X_2\), for simplicity. This could be anything—a set of financial returns, height and weight pairs, or temperatures at different locations. Let’s define our random variable \(X\) as:</p> \[X = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix}.\] <p>We’ll calculate the sample mean vector \(\hat{\mu}\) and the sample covariance matrix \(\hat{\Sigma}\) based on observed data.</p> <h3 id="step-1-calculate-the-sample-mean-vector">Step 1: Calculate the Sample Mean Vector</h3> <p>Let’s start with the sample mean vector, which captures the “central location” of the data. Given \(N\) observations, we define the sample mean \(\hat{\mu}\) as:</p> \[\hat{\mu} = \frac{1}{N} \sum_{i=1}^N X_i,\] <p>where \(X_i\) represents the \(i\)-th observation of our random variable \(X\).</p> <p>For instance, suppose we have the following five observations:</p> \[X_1 = \begin{bmatrix} 2 \\ 3 \end{bmatrix}, \quad X_2 = \begin{bmatrix} 3 \\ 5 \end{bmatrix}, \quad X_3 = \begin{bmatrix} 5 \\ 7 \end{bmatrix}, \quad X_4 = \begin{bmatrix} 6 \\ 8 \end{bmatrix}, \quad X_5 = \begin{bmatrix} 8 \\ 10 \end{bmatrix}.\] <p>The sample mean vector \(\hat{\mu}\) is then calculated as:</p> \[\hat{\mu} = \frac{1}{5} \left( \begin{bmatrix} 2 \\ 3 \end{bmatrix} + \begin{bmatrix} 3 \\ 5 \end{bmatrix} + \begin{bmatrix} 5 \\ 7 \end{bmatrix} + \begin{bmatrix} 6 \\ 8 \end{bmatrix} + \begin{bmatrix} 8 \\ 10 \end{bmatrix} \right).\] <p>Breaking it down component-wise:</p> \[\hat{\mu}_1 = \frac{1}{5} (2 + 3 + 5 + 6 + 8) = 4.8, \quad \hat{\mu}_2 = \frac{1}{5} (3 + 5 + 7 + 8 + 10) = 6.6.\] <p>Thus, our mean vector is:</p> \[\hat{\mu} = \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix}.\] <h3 id="step-2-calculate-the-sample-covariance-matrix">Step 2: Calculate the Sample Covariance Matrix</h3> <p>Next, let’s calculate the sample covariance matrix, which tells us not only the variability of each variable but also how \(X_1\) and \(X_2\) move in relation to each other. The sample covariance matrix \(\hat{\Sigma}\) is defined as:</p> \[\hat{\Sigma} = \frac{1}{N-1} \sum_{i=1}^N (X_i - \hat{\mu})(X_i - \hat{\mu})^T.\] <p>For our five observations, \(N = 5\), so we’ll divide by \(4\) (that’s \(N - 1\)).</p> <p>Let’s compute each term \((X_i - \hat{\mu})(X_i - \hat{\mu})^T\) for each observation:</p> <ol> <li> <p><strong>For \(X_1 = \begin{bmatrix} 2 \\ 3 \end{bmatrix}\):</strong></p> \[X_1 - \hat{\mu} = \begin{bmatrix} 2 \\ 3 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} -2.8 \\ -3.6 \end{bmatrix},\] <p>and</p> \[(X_1 - \hat{\mu})(X_1 - \hat{\mu})^T = \begin{bmatrix} -2.8 \\ -3.6 \end{bmatrix} \begin{bmatrix} -2.8 &amp; -3.6 \end{bmatrix} = \begin{bmatrix} 7.84 &amp; 10.08 \\ 10.08 &amp; 12.96 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_2 = \begin{bmatrix} 3 \\ 5 \end{bmatrix}\):</strong></p> \[X_2 - \hat{\mu} = \begin{bmatrix} 3 \\ 5 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} -1.8 \\ -1.6 \end{bmatrix},\] <p>and</p> \[(X_2 - \hat{\mu})(X_2 - \hat{\mu})^T = \begin{bmatrix} -1.8 \\ -1.6 \end{bmatrix} \begin{bmatrix} -1.8 &amp; -1.6 \end{bmatrix} = \begin{bmatrix} 3.24 &amp; 2.88 \\ 2.88 &amp; 2.56 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_3 = \begin{bmatrix} 5 \\ 7 \end{bmatrix}\):</strong></p> \[X_3 - \hat{\mu} = \begin{bmatrix} 5 \\ 7 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.4 \end{bmatrix},\] <p>and</p> \[(X_3 - \hat{\mu})(X_3 - \hat{\mu})^T = \begin{bmatrix} 0.2 \\ 0.4 \end{bmatrix} \begin{bmatrix} 0.2 &amp; 0.4 \end{bmatrix} = \begin{bmatrix} 0.04 &amp; 0.08 \\ 0.08 &amp; 0.16 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_4 = \begin{bmatrix} 6 \\ 8 \end{bmatrix}\):</strong></p> \[X_4 - \hat{\mu} = \begin{bmatrix} 6 \\ 8 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 1.4 \end{bmatrix},\] <p>and</p> \[(X_4 - \hat{\mu})(X_4 - \hat{\mu})^T = \begin{bmatrix} 1.2 \\ 1.4 \end{bmatrix} \begin{bmatrix} 1.2 &amp; 1.4 \end{bmatrix} = \begin{bmatrix} 1.44 &amp; 1.68 \\ 1.68 &amp; 1.96 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_5 = \begin{bmatrix} 8 \\ 10 \end{bmatrix}\):</strong> \(X_5 - \hat{\mu} = \begin{bmatrix} 8 \\ 10 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} 3.2 \\ 3.4 \end{bmatrix},\) and \((X_5 - \hat{\mu})(X_5 - \hat{\mu})^T = \begin{bmatrix} 3.2 \\ 3.4 \end{bmatrix} \begin{bmatrix} 3.2 &amp; 3.4 \end{bmatrix} = \begin{bmatrix} 10.24 &amp; 10.88 \\ 10.88 &amp; 11.56 \end{bmatrix}.\)</p> </li> </ol> <p>Now, we add these matrices and divide by \(N - 1 = 4\) to get the sample covariance matrix:</p> \[\hat{\Sigma} = \frac{1}{4} \left( \begin{bmatrix} 7.84 &amp; 10.08 \\ 10.08 &amp; 12.96 \end{bmatrix} + \begin{bmatrix} 3.24 &amp; 2.88 \\ 2.88 &amp; 2.56 \end{bmatrix} + \begin{bmatrix} 0.04 &amp; 0.08 \\ 0.08 &amp; 0.16 \end{bmatrix} + \begin{bmatrix} 1.44 &amp; 1.68 \\ 1.68 &amp; 1.96 \end{bmatrix} + \begin{bmatrix} 10.24 &amp; 10.88 \\ 10.88 &amp; 11.56 \end{bmatrix} \right).\] <p>After summing the matrices:</p> \[\hat{\Sigma} = \frac{1}{4} \begin{bmatrix} 22.8 &amp; 25.6 \\ 25.6 &amp; 29.2 \end{bmatrix} = \begin{bmatrix} 5.7 &amp; 6.4 \\ 6.4 &amp; 7.3 \end{bmatrix}.\] <h3 id="summary">Summary</h3> <p>In this example, we’ve calculated both the sample mean vector and the sample covariance matrix. Our sample covariance matrix, \(\hat{\Sigma} = \begin{bmatrix} 5.7 &amp; 6.4 \\ 6.4 &amp; 7.3 \end{bmatrix}\), encapsulates the spread and relationship between \(X_1\) and \(X_2\). The off-diagonal terms (6.4) represent the covariance between \(X_1\) and \(X_2\), indicating a positive correlation, while the diagonal terms represent the variances of \(X_1\) and \(X_2\) individually.</p> <p>This simple calculation reinforces the power of the covariance matrix—it captures both the “spread” of each variable and their interaction, providing a complete picture of our data’s geometric and statistical structure.</p> <hr> <h2 id="covariance-matrix-and-independence">Covariance Matrix and Independence</h2> <p>As we wrap up our journey into the world of covariance matrices, it’s fitting to address one of the most common misconceptions: the relationship between <strong>independence</strong> and <strong>uncorrelatedness</strong>. In the world of multivariate distributions, understanding whether two variables are independent or merely uncorrelated is crucial. The covariance matrix can give us valuable insights, but it has its limitations—especially when it comes to verifying true independence.</p> <h3 id="defining-independence">Defining Independence</h3> <p>In probability theory, two random variables \(X_i\) and \(X_j\) are defined to be <strong>independent</strong> if the occurrence of one has no influence on the probability distribution of the other. Mathematically, this means:</p> \[P(X_i \leq x, X_j \leq y) = P(X_i \leq x) \cdot P(X_j \leq y),\] <p>for all \(x\) and \(y\). In simpler terms, knowing the value of \(X_i\) provides no information about \(X_j\) and vice versa.</p> <h3 id="covariance-and-independence-a-subtle-distinction">Covariance and Independence: A Subtle Distinction</h3> <p>The covariance matrix tells us about the <strong>linear relationships</strong> between variables, but not necessarily about their independence. For two variables \(X_i\) and \(X_j\), the covariance \(\text{Cov}(X_i, X_j)\) is defined as:</p> \[\text{Cov}(X_i, X_j) = E[(X_i - E[X_i])(X_j - E[X_j])].\] <p>If \(X_i\) and \(X_j\) are independent, then \(\text{Cov}(X_i, X_j) = 0\). Independence implies that there is no relationship between the variables at all, which includes a lack of linear correlation. However, the reverse is not true: <strong>zero covariance does not imply independence</strong>.</p> <p>To see why, let’s explore this distinction mathematically and with an example.</p> <h3 id="proof-that-zero-covariance-does-not-imply-independence">Proof that Zero Covariance Does Not Imply Independence</h3> <p>To understand why zero covariance does not necessarily mean independence, consider two random variables \(X\) and \(Y\) that are uncorrelated (i.e., \(\text{Cov}(X, Y) = 0\)) but not independent.</p> <p>Let \(X\) be a standard normal random variable: \(X \sim N(0, 1)\). Now define \(Y = X^2\). Clearly, \(Y\) depends on \(X\); in fact, \(Y\) is entirely determined by \(X\), so \(X\) and \(Y\) are not independent.</p> <ol> <li> <p><strong>Calculating the Covariance</strong>: We’ll calculate \(\text{Cov}(X, Y)\) to see if they’re uncorrelated.</p> \[\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])].\] <p>Since \(X \sim N(0, 1)\), we have \(E[X] = 0\) and \(E[Y] = E[X^2] = \text{Var}(X) = 1\). So,</p> \[\text{Cov}(X, Y) = E[X(Y - 1)] = E[X(X^2 - 1)] = E[X^3 - X].\] </li> <li> <p><strong>Expectation of Odd Moments</strong>: Given that \(X\) is normally distributed with mean 0, all odd moments of \(X\) (such as \(E[X]\) and \(E[X^3]\)) are zero. Therefore,</p> \[\text{Cov}(X, Y) = E[X^3] - E[X] = 0 - 0 = 0.\] <p>Thus, \(\text{Cov}(X, Y) = 0\), indicating that \(X\) and \(Y\) are uncorrelated. But as we know, \(Y = X^2\), which is entirely determined by \(X\). Therefore, they are not independent, even though they are uncorrelated.</p> </li> </ol> <p>This example highlights a key takeaway: <strong>uncorrelated variables are not necessarily independent</strong>. Covariance only captures linear relationships, so two variables could have a non-linear dependency and still exhibit zero covariance.</p> <h3 id="role-of-the-covariance-matrix-in-independence-analysis">Role of the Covariance Matrix in Independence Analysis</h3> <p>The covariance matrix \(\Sigma\) provides us with a way to examine linear dependencies between variables. Each off-diagonal element \(\Sigma\_{ij} = \text{Cov}(X_i, X_j)\) indicates the extent to which two variables vary together. If all off-diagonal entries of \(\Sigma\) are zero, we know that each pair of variables is <strong>uncorrelated</strong>. However, this does not guarantee independence.</p> <p>For multivariate normal distributions, however, the story is simpler. In a multivariate normal distribution, uncorrelated variables are indeed independent. Specifically, if \(X \sim N(\mu, \Sigma)\), then zero off-diagonal entries in \(\Sigma\) imply that the components of \(X\) are independent. This is a special property of the Gaussian distribution and does not hold in general.</p> <h3 id="practical-implications">Practical Implications</h3> <p>When analyzing real-world data, it’s essential to remember that zero covariance should not be mistaken for independence unless you’re specifically working with a multivariate Gaussian. In most other cases, zero covariance merely indicates a lack of linear relationship. Non-linear dependencies, which are common in fields like finance, biology, and machine learning, can often “hide” behind zero covariance.</p> <p>In summary, the covariance matrix is a powerful tool for understanding linear relationships but is limited in detecting true independence. When we need a full assessment of independence, especially in non-Gaussian contexts, we have to look beyond \(\Sigma\) and consider additional techniques, such as analyzing joint distributions or using tests for independence.</p> <hr> <h2 id="conclusion">Conclusion</h2> <p>The mean vector serves as the “center of gravity” for the data, shifting predictably under transformations and providing a concise summary of central tendencies across dimensions. Meanwhile, the covariance matrix is a lens into the data’s geometry. It encodes not only the individual variances of each variable but also the pairwise interactions that reveal whether variables rise and fall together or act independently. This matrix’s symmetry and positive semi-definiteness give it a unique role in shaping data contours and helping analysts visualize distribution shape, orientation, and spread.</p> <p>But as we’ve seen, the covariance matrix also has its limits. While it efficiently captures linear relationships, it cannot capture the entire complexity of data dependencies. As our exploration of independence versus uncorrelatedness shows, true independence requires more than just a zero covariance—especially in non-Gaussian settings where nonlinear dependencies can lie hidden. Understanding these nuances is crucial for effective data analysis. In real-world applications, whether in finance, biology, or machine learning, knowing when the covariance matrix can (and cannot) tell the full story allows us to apply these tools more accurately and creatively.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Enjoy Reading This Article? Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Boltzmann/">Maximal Entropy of the Boltzmann Distribution: A Quantum Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Rethinking-1/">Rethinking an Olefin Oligomerization from Three Years Ago – Chapter 1: Problem Statement and Some Initial Thoughts from Back Then</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Rethinking-0/">Rethinking an Olefin Oligomerization from Three Years Ago – Chapter 0: Preface</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Proofs-for-Some-Properties/">A Sketch of Proofs for Some Properties of Multivariate Gaussian Distribution</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/multivariate_Gaussian_distribution/">Is the Transition from Univariate to Multivariate Gaussian Distribution Linear?</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Shuhong Dai. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-maximal-entropy-of-the-boltzmann-distribution-a-quantum-perspective",title:"Maximal Entropy of the Boltzmann Distribution: A Quantum Perspective",description:"Specifically, in the classical limit (when \u03b2En &lt;&lt; 1), the quantum energy levels become very close to each other, and the partition function can be approximated by an integral over continuous energy states rather than a sum over discrete states. In this limit, the quantum Boltzmann distribution approaches the classical result...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Boltzmann/"}},{id:"post-rethinking-an-olefin-oligomerization-from-three-years-ago-chapter-1-problem-statement-and-some-initial-thoughts-from-back-then",title:"Rethinking an Olefin Oligomerization from Three Years Ago \u2013 Chapter 1: Problem Statement...",description:"Treating experimental conditions as single-variable parameters or ignoring \u201cminor\u201d byproducts that seemed irrelevant at the time. Were those assumptions valid? Did we truly grasp the core requirements of the problem?",section:"Posts",handler:()=>{window.location.href="/blog/2024/Rethinking-1/"}},{id:"post-rethinking-an-olefin-oligomerization-from-three-years-ago-chapter-0-preface",title:"Rethinking an Olefin Oligomerization from Three Years Ago \u2013 Chapter 0: Preface",description:"Perhaps I assumed, rather simplistically, that a mathematical modeling competition would primarily demand mathematical rigor, with little reliance on actual chemistry knowledge (an assumption that proved largely correct). Or perhaps I dismissed it as a straightforward data analysis exercise...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Rethinking-0/"}},{id:"post-a-sketch-of-proofs-for-some-properties-of-multivariate-gaussian-distribution",title:"A Sketch of Proofs for Some Properties of Multivariate Gaussian Distribution",description:"When we diagonalize the covariance matrix, we essentially rotate the space such that the axes align with the principal directions of variation...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Proofs-for-Some-Properties/"}},{id:"post-is-the-transition-from-univariate-to-multivariate-gaussian-distribution-linear",title:"Is the Transition from Univariate to Multivariate Gaussian Distribution Linear?",description:"Now that we have the foundation in place, let\u2019s shift gears and consider the generalization of the univariate Gaussian to higher dimensions. In the multivariate case, we are no longer dealing with a single random variable, but rather a vector of random variables...",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate_Gaussian_distribution/"}},{id:"post-a-supplementary-discussion-on-correlation-coefficients",title:"A Supplementary Discussion on Correlation Coefficients",description:"Yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Correlation_Coefficients/"}},{id:"post-an-introductory-look-at-covariance-and-the-mean-vector",title:"An Introductory Look at Covariance and the Mean Vector",description:"If the mean vector gives us a sense of location, then the covariance matrix gives us a sense of shape...",section:"Posts",handler:()=>{window.location.href="/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/"}},{id:"post-sampling-smarter-unlocking-the-power-of-latin-hypercube-sampling",title:"Sampling Smarter: Unlocking the Power of Latin Hypercube Sampling",description:"Unlike random sampling, which might leave some regions underrepresented while others get chosen repeatedly...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Latin_Hypercube_Sampling/"}},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-won-the-1st-prize-with-a-designed-four-switch-buck-boost-circuit-in-the-university-electronic-design-competition-sponsored-by-huawei",title:"Won the 1st prize with a designed four-switch buck-boost circuit in the university...",description:"",section:"News"},{id:"news-received-the-first-class-academic-scholarship",title:"Received the First-Class Academic Scholarship.",description:"",section:"News"},{id:"news-won-the-3rd-prize-for-problem-g-quot-non-contact-object-measurement-device-quot-at-the-8th-national-undergraduate-electronics-indesign-contest",title:"Won the 3rd prize for Problem G: &quot;Non-contact Object Measurement Device&quot; at the...",description:"",section:"News"},{id:"news-visited-the-key-laboratory-of-advanced-energy-traction-and-comprehensive-energy-saving-railway-industry-at-swjtu",title:"Visited the Key Laboratory of Advanced Energy Traction and Comprehensive Energy Saving Railway...",description:"",section:"News"},{id:"news-received-the-first-class-academic-scholarship",title:"Received the First-Class Academic Scholarship.",description:"",section:"News"},{id:"news-won-the-3rd-prize-in-problem-b-quot-ethanol-coupling-to-produce-c4-olefins-quot-at-the-30th-china-undergraduate-mathematical-contest-in-modeling",title:"Won the 3rd prize in Problem B: &quot;Ethanol Coupling to Produce C4 Olefins&quot;...",description:"",section:"News"},{id:"news-won-the-1st-prize-in-the-preliminary-national-round-of-the-5th-chinese-education-cup-mathematics-competition",title:"Won the 1st prize in the preliminary national round of the 5th Chinese...",description:"",section:"News"},{id:"news-won-the-2nd-prize-in-the-national-finals-of-the-5th-chinese-education-cup-mathematics-competition",title:"Won the 2nd prize in the national finals of the 5th Chinese Education...",description:"",section:"News"},{id:"news-won-the-1st-prize-at-the-provincial-level-in-the-13th-chinese-mathematics-competitions",title:"Won the 1st prize at the provincial level in the 13th Chinese Mathematics...",description:"",section:"News"},{id:"news-guided-the-quot-dual-car-following-system-quot-project-which-won-the-highest-award-the-quot-ti-cup-quot-in-the-10th-national-undergraduate-electronics-design-contest",title:"Guided the &quot;Dual-Car Following System&quot; project which won the highest award, the &quot;TI...",description:"",section:"News"},{id:"news-obtained-my-bachelor-39-s-degree-in-electrical-engineering",title:"Obtained my Bachelor&#39;s Degree in Electrical Engineering.",description:"",section:"News"},{id:"news-joined-the-distributed-systems-group-at-ncepu-to-pursue-my-master-39-s-degree-in-computer-science",title:"Joined the Distributed Systems Group at NCEPU to pursue my Master&#39;s degree in...",description:"",section:"News"},{id:"news-participated-in-world-robot-conference-2024-in-beijing",title:"Participated in World Robot Conference 2024 in Beijing.",description:"",section:"News"},{id:"news-a-paper-on-data-compression-https-ieeexplore-ieee-org-document-10376424-was-accepted-by-ieee-iot-journal",title:"[A paper on data compression](https://ieeexplore.ieee.org/document/10376424) was accepted by _IEEE IoT Journal_.",description:"",section:"News"},{id:"news-a-paper-on-energy-efficient-computing-https-www-sciencedirect-com-science-article-pii-s0140366423004802-via-3dihub-was-accepted-by-computer-communications",title:"[A paper on energy-efficient computing](https://www.sciencedirect.com/science/article/pii/S0140366423004802?via%3Dihub) was accepted by _Computer Communications_.",description:"",section:"News"},{id:"news-participated-in-microsoft-ai-day-beijing",title:"Participated in Microsoft AI Day Beijing.",description:"",section:"News"},{id:"news-a-paper-on-vehicle-road-cooperation-https-ieeexplore-ieee-org-document-10632106-was-accepted-by-ieee-iot-journal",title:"[A paper on vehicle-road cooperation](https://ieeexplore.ieee.org/document/10632106) was accepted by _IEEE IoT Journal_.",description:"",section:"News"},{id:"news-i-m-thrilled-to-announce-the-release-of-texhelper-https-pypi-org-project-texhelper-a-python-library-designed-to-optimize-and-beautify-tex-code-texhelper-makes-managing-your-latex-based-projects-easier-and-more-elegant-the-project-is-open-source-and-available-on-github-https-github-com-shuhongdai-texhelper-check-it-out",title:"\ud83c\udf89\ud83c\udf89\ud83c\udf89 I\u2019m thrilled to announce the release of [TexHelper](https://pypi.org/project/texhelper/), a Python library designed...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%64%61%69%73%68%75%68%6F%6E%67%30%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=lwCWmPUAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>