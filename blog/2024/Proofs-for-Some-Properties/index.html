<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="3hx6hRZ1EJWUbqkkXZ-vPQfpV0JdKNKT_aKcRjSixr0"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Sketch of Proofs for Some Properties of Multivariate Gaussian Distribution | Shuhong Dai </title> <meta name="author" content="Shuhong Dai"> <meta name="description" content="When we diagonalize the covariance matrix, we essentially rotate the space such that the axes align with the principal directions of variation..."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?6d57c5bac70ef6fae4bd96883a4eb4da"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shuhongdai.github.io/blog/2024/Proofs-for-Some-Properties/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shuhong</span> Dai </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Sketch of Proofs for Some Properties of Multivariate Gaussian Distribution</h1> <p class="post-meta"> Created in July 09, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/tag/stochastic-processes"> <i class="fa-solid fa-hashtag fa-sm"></i> Stochastic Processes</a>   ·   <a href="/blog/category/a-column-on-gaussian-processes"> <i class="fa-solid fa-tag fa-sm"></i> A Column on Gaussian Processes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>In the <a href="https://shuhongdai.github.io/blog/2024/multivariate_Gaussian_distribution/">previous blog post</a>, we discussed the extension from univariate to multivariate Gaussian distributions and examined the transformation in their functional form. Moving from a single variable to an N-dimensional space introduces a rich structure where properties like independence, marginal distributions, and conditional behaviors play a crucial role in understanding the multivariate Gaussian distribution in depth. In this post, we will provide a sketch of proofs for several key properties of multivariate Gaussians. These properties include the independence of Gaussian random vectors, the derivation of marginal and conditional distributions, the invariance of covariance under transformations, and the behavior of Gaussian distributions under linear transformations. Rather than delving into detailed, rigorous proofs, we aim to offer an intuitive and accessible overview, highlighting the underlying mathematical concepts without overwhelming technical detail.</p> <hr> <h2 id="proving-the-independence-of-gaussian-random-vectors">Proving the Independence of Gaussian Random Vectors</h2> <p>The relationship between the covariance structure of a random vector and its independence properties plays a key role in many machine learning algorithms, signal processing, and other applied fields. It is particularly relevant in Gaussian processes, where we assume that the random vectors representing function values are often independent, under certain conditions. So, let’s break down the problem mathematically, focusing on proving the independence of the components of a multivariate Gaussian vector. The path to this insight involves linear algebra, specifically understanding the covariance matrix and its diagonalization.</p> <h3 id="covariance-matrix-diagonalization-and-independence">Covariance Matrix Diagonalization and Independence</h3> <p>Consider a random vector \(\mathbf{X} = (X_1, X_2, \dots, X_k)^T\) following a multivariate normal distribution with mean vector \(\mu = (\mu_1, \mu_2, \dots, \mu_k)^T\) and covariance matrix \(\Sigma\).</p> <p>The probability density function (PDF) of this multivariate Gaussian distribution is given by:</p> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right)\] <p>Here, \(\Sigma\) is a \(k \times k\) covariance matrix, where the diagonal elements \(\sigma_i^2\) represent the variances of each component of the vector \(\mathbf{X}\), and the off-diagonal elements \(\sigma_{ij}\) represent the covariances between the components \(X_i\) and \(X_j\).</p> <p>Now, to examine the independence of the components of \(\mathbf{X}\), we need to focus on the structure of the covariance matrix \(\Sigma\). For random variables to be independent, their covariance must be zero. This leads to the crucial fact that if the covariance matrix is diagonal, then the components of \(\mathbf{X}\) must be independent. The question now is: when does this happen?</p> <h3 id="diagonalization-of-covariance-matrix">Diagonalization of Covariance Matrix</h3> <p>We know from linear algebra that any symmetric matrix, including the covariance matrix \(\Sigma\), can be diagonalized by an orthogonal transformation. This means that there exists an orthogonal matrix \(Q\) (i.e., \(Q^T Q = I\)) such that:</p> \[Q^T \Sigma Q = D\] <p>where \(D\) is a diagonal matrix. Each diagonal element of \(D\), say \(\sigma_i^2\), corresponds to the variance of the transformed variables, which are linear combinations of the original components of \(\mathbf{X}\).</p> <p>To unpack this geometrically: the matrix \(Q\) represents a rotation (or possibly reflection) of the coordinate axes in the space of the random vector \(\mathbf{X}\). After the transformation, the random vector \(\mathbf{Y} = Q^T \mathbf{X}\) will have components \(Y_1, Y_2, \dots, Y_k\), each with variance \(\sigma_i^2\), and no covariances between them. In other words, these transformed components are uncorrelated.</p> <p>But here’s the critical point: uncorrelated components in a multivariate normal distribution are independent. This result holds because the multivariate normal distribution has a special property: if its components are uncorrelated, they must also be independent. This follows from the fact that the joint distribution factorizes when the covariance matrix is diagonal.</p> <h3 id="formal-proof-of-independence">Formal Proof of Independence</h3> <p>Let’s now formalize this with a rigorous proof. Given that the covariance matrix \(\Sigma\) is diagonal, it can be written as:</p> \[\Sigma = \text{diag}(\sigma_1^2, \sigma_2^2, \dots, \sigma_k^2)\] <p>The off-diagonal elements of \(\Sigma\) are zero, indicating that the components of \(\mathbf{X}\) are uncorrelated.</p> <p>For two random variables \(X_i\) and \(X_j\), their covariance is defined as:</p> \[\text{Cov}(X_i, X_j) = \mathbb{E}[(X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])]\] <p>If the covariance matrix \(\Sigma\) is diagonal, then:</p> \[\text{Cov}(X_i, X_j) = 0 \quad \text{for} \quad i \neq j\] <p>This implies that \(X_i\) and \(X_j\) are uncorrelated. Since we are dealing with a multivariate normal distribution, this uncorrelation implies that \(X_i\) and \(X_j\) are independent.</p> <p>Thus, if the covariance matrix \(\Sigma\) is diagonal, the components of \(\mathbf{X}\) are independent, and we can conclude:</p> \[X_1, X_2, \dots, X_k \quad \text{are independent}.\] <h3 id="geometrical-intuition">Geometrical Intuition</h3> <p>To gain some geometrical intuition, think of a random vector \(\mathbf{X}\) in a high-dimensional space. The covariance matrix \(\Sigma\) captures how the components of \(\mathbf{X}\) vary together. If the covariance matrix is diagonal, this indicates that the components of \(\mathbf{X}\) do not “mix” with one another—they vary independently along different directions of the space.</p> <p>When we diagonalize the covariance matrix, we essentially rotate the space such that the axes align with the principal directions of variation. Along each axis, the corresponding component of \(\mathbf{X}\) varies independently of the others. This geometric interpretation aligns with the algebraic fact that the components are independent when the covariance matrix is diagonal.</p> <h3 id="demo">Demo</h3> <p>In this section, we explore how the covariance matrix influences the distribution of multivariate Gaussian random variables. Specifically, we’ll visualize the difference between correlated and independent random variables by examining two different covariance matrices: one that introduces correlation between the components and one that enforces independence.</p> <p>We will work with a bivariate Gaussian distribution, where the random vector \(\mathbf{x} = [X_1, X_2]^T\) has a mean vector \(\mu = [0, 0]^T\). The covariance matrix determines the shape and orientation of the distribution in the feature space.</p> <ol> <li> <p><strong>Non-diagonal Covariance Matrix (Correlated Case)</strong>: A covariance matrix with off-diagonal elements (non-zero values) implies that the two components \(X_1\) and \(X_2\) are correlated. For example, a covariance matrix like:</p> \[\Sigma = \begin{bmatrix} 1 &amp; 0.8 \\ 0.8 &amp; 1 \end{bmatrix}\] <p>indicates a positive correlation between \(X_1\) and \(X_2\). When plotted, this produces an elliptical distribution, where the data points are spread in a specific direction, showing a linear relationship between the two variables.</p> </li> <li> <p><strong>Diagonal Covariance Matrix (Independent Case)</strong>: A diagonal covariance matrix implies that the two variables are independent. For example:</p> \[\Sigma = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\] <p>In this case, there is no correlation between \(X_1\) and \(X_2\), and the distribution is circular. The points are spread equally in all directions, showing no linear dependence between the two variables.</p> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Set plotting style
</span><span class="n">sns</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="sh">"</span><span class="s">white</span><span class="sh">"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="sh">"</span><span class="s">muted</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Seed for reproducibility
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Define mean and covariance matrices
</span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Covariance Matrix 1 (Non-diagonal, correlated)
</span><span class="n">cov_1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>

<span class="c1"># Covariance Matrix 2 (Diagonal, independent)
</span><span class="n">cov_2</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>

<span class="c1"># Generate random samples
</span><span class="n">data_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov_1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">data_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov_2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Plotting the samples
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

<span class="c1"># Plotting the non-diagonal covariance samples
</span><span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Non-diagonal Covariance Matrix (Correlated)</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plotting the diagonal covariance samples
</span><span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data_2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">data_2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Diagonal Covariance Matrix (Independent)</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Visualizing%20the%20Impact%20of%20Covariance%20Matrices%20on%20the%20Distribution%20of%20Bivariate%20Gaussian%20Random%20Variables-480.webp 480w,/assets/posts_img/2024-07-09/Visualizing%20the%20Impact%20of%20Covariance%20Matrices%20on%20the%20Distribution%20of%20Bivariate%20Gaussian%20Random%20Variables-800.webp 800w,/assets/posts_img/2024-07-09/Visualizing%20the%20Impact%20of%20Covariance%20Matrices%20on%20the%20Distribution%20of%20Bivariate%20Gaussian%20Random%20Variables-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-07-09/Visualizing%20the%20Impact%20of%20Covariance%20Matrices%20on%20the%20Distribution%20of%20Bivariate%20Gaussian%20Random%20Variables.png" class="img-fluid" width="700" height="480" alt="Visualizing the Impact of Covariance Matrices on the Distribution of Bivariate Gaussian Random Variables" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p><strong>Non-diagonal Covariance Matrix (Correlated Case)</strong>: In the first plot (left), where the covariance matrix contains off-diagonal values (0.8), the points form an elongated ellipse. This shows that the two variables \(X_1\) and \(X_2\) are positively correlated. The data points are not spread equally in all directions but rather along the principal axes of the ellipse, which reflects the dependency between the variables.</p> </li> <li> <p><strong>Diagonal Covariance Matrix (Independent Case)</strong>: In the second plot (right), the covariance matrix is diagonal, indicating that \(X_1\) and \(X_2\) are independent. The points form a circular distribution, with no discernible directionality. This indicates that the changes in \(X_1\) do not depend on \(X_2\), and vice versa. The variables are independent, which is precisely what the diagonal covariance structure represents.</p> </li> </ol> <hr> <h2 id="deriving-marginal-and-conditional-distributions-of-multivariate-gaussians">Deriving Marginal and Conditional Distributions of Multivariate Gaussians</h2> <p>The fact that while the marginal distributions of a multivariate Gaussian are always Gaussian, the reverse inference is not true.</p> <h3 id="deriving-the-marginal-distribution">Deriving the Marginal Distribution</h3> <p>In probability theory, the marginal distribution is the distribution of a subset of variables, ignoring the others. Mathematically, for a random vector \(\mathbf{X} = [X_1, X_2, \dots, X_k]^T\) following a multivariate Gaussian distribution with mean vector \(\mu = [\mu_1, \mu_2, \dots, \mu_k]^T\) and covariance matrix \(\Sigma\), the marginal distribution of a subset of the components of \(\mathbf{X}\) is still a Gaussian distribution.</p> <p>Suppose we have a partition of the vector \(\mathbf{X}\) into two disjoint subsets: \(\mathbf{X}_S\) (the subset of interest) and \(\mathbf{X}_C\) (the complement of \(\mathbf{X}_S\)).</p> <p>The full vector \(\mathbf{X}\) follows a multivariate normal distribution:</p> \[\mathbf{X} \sim \mathcal{N}(\mu, \Sigma)\] <p>We want to derive the marginal distribution of \(\mathbf{X}_S\), i.e., the distribution of just the subset \(\mathbf{X}_S\) while marginalizing over the components of \(\mathbf{X}_C\). The marginal distribution is obtained by integrating out the components corresponding to \(\mathbf{X}_C\) from the joint distribution.</p> <p>The joint PDF of \(\mathbf{X}\) is given by:</p> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right)\] <p>Now, let’s partition the mean vector \(\mu\) and the covariance matrix \(\Sigma\) as follows:</p> \[\mu = \begin{bmatrix} \mu_S \\ \mu_C \end{bmatrix}, \quad \Sigma = \begin{bmatrix} \Sigma_{SS} &amp; \Sigma_{SC} \\ \Sigma_{CS} &amp; \Sigma_{CC} \end{bmatrix}\] <p>Where:</p> <ul> <li>\(\mu_S\) is the mean vector for the subset \(\mathbf{X}_S\),</li> <li>\(\mu_C\) is the mean vector for the complement \(\mathbf{X}_C\),</li> <li>\(\Sigma_{SS}\) is the covariance matrix between the components in \(\mathbf{X}_S\),</li> <li>\(\Sigma_{SC}\) and \(\Sigma_{CS}\) are the cross-covariances between \(\mathbf{X}_S\) and \(\mathbf{X}_C\),</li> <li>\(\Sigma_{CC}\) is the covariance matrix of \(\mathbf{X}_C\).</li> </ul> <p>To find the marginal distribution of \(\mathbf{X}_S\), we integrate out the complement \(\mathbf{X}_C\) from the joint PDF. That is we compute the integral:</p> \[f(\mathbf{x}_S) = \int f(\mathbf{x}) \, d\mathbf{x}_C\] <p>Substituting the partitioned PDF:</p> \[f(\mathbf{x}_S) = \int \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x}_S, \mathbf{x}_C - \begin{bmatrix} \mu_S \\ \mu_C \end{bmatrix})^T \begin{bmatrix} \Sigma_{SS} &amp; \Sigma_{SC} \\ \Sigma_{CS} &amp; \Sigma_{CC} \end{bmatrix}^{-1} (\mathbf{x}_S, \mathbf{x}_C - \begin{bmatrix} \mu_S \\ \mu_C \end{bmatrix}) \right) d\mathbf{x}_C\] <p>This integral can be solved, and the result is another Gaussian distribution for \(\mathbf{x}_S\), with mean \(\mu_S\) and covariance matrix \(\Sigma_{SS}\):</p> \[f(\mathbf{x}_S) = \frac{1}{(2\pi)^{|S|/2} |\Sigma_{SS}|^{1/2}} \exp \left( -\frac{1}{2} \mathbf{x}_S^T \Sigma_{SS}^{-1} \mathbf{x}_S \right)\] <p>Thus, the marginal distribution of \(\mathbf{X}_S\) is still Gaussian, as expected.</p> <h3 id="reverse-inference-marginal-distributions-do-not-imply-multivariate-normality">Reverse Inference: Marginal Distributions Do Not Imply Multivariate Normality</h3> <p>While it is clear from the above derivation that the marginal distribution of any subset of a multivariate Gaussian is also Gaussian, there is an interesting and subtle issue when trying to reverse the argument. That is, just because the marginal distributions of a random vector are Gaussian does not necessarily mean the entire vector follows a multivariate Gaussian distribution.</p> <p>Suppose we have a random vector \(\mathbf{X} = [X_1, X_2]^T\), and we know that each component \(X_1\) and \(X_2\) is distributed according to a Gaussian distribution:</p> \[X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2), \quad X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)\] <p>This implies that the marginal distributions of \(X_1\) and \(X_2\) are Gaussian. However, this does not imply that the joint distribution \((X_1, X_2)\) is necessarily Gaussian. To see this, we need to consider a counterexample.</p> <p><strong>Counterexample:</strong> Suppose that we have two random variables \(X_1\) and \(X_2\) which are both Gaussian marginally, but their joint distribution is not Gaussian. This can occur when their joint distribution involves nonlinear relationships or other higher-order dependencies that are not captured by the marginals alone.</p> <p>For instance, consider the following joint distribution:</p> \[f(X_1, X_2) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}X_1^2\right) \cdot \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}X_2^2\right) \cdot (1 + \sin(X_1X_2))\] <p>Even though the marginals of \(X_1\) and \(X_2\) are Gaussian, the joint distribution clearly isn’t. This demonstrates that knowing the marginal distributions are Gaussian does not provide sufficient information to conclude that the joint distribution is Gaussian.</p> <p>In simpler terms: marginal Gaussianity is a necessary but not sufficient condition for the joint distribution to be Gaussian.</p> <hr> <h2 id="invariance-of-the-multivariate-gaussian-distribution-under-linear-transformations">Invariance of the Multivariate Gaussian Distribution under Linear Transformations</h2> <p>In this section, we explore a crucial property of the multivariate Gaussian distribution: <strong>its invariance under linear transformations</strong>.</p> <h3 id="the-setup-a-multivariate-gaussian-vector">The Setup: A Multivariate Gaussian Vector</h3> <p>We begin with a random vector \(\mathbf{x}\) that follows a multivariate Gaussian distribution:</p> \[\mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x)\] <p>Here, \(\mu_x\) is the \(k\)-dimensional mean vector \(\mu_x = [\mu_{x1}, \mu_{x2}, \dots, \mu_{xk}]^T\), \(\Sigma_x\) is the \(k \times k\) covariance matrix \(\Sigma_x = \begin{bmatrix} \sigma_{x1}^2 &amp; \text{Cov}(X_1, X_2) &amp; \dots \\ \end{bmatrix}\).</p> <p>The PDF of \(\mathbf{x}\) is:</p> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2} |\Sigma_x|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x) \right)\] <p>We are interested in how the distribution of \(\mathbf{x}\) transforms when we apply a linear transformation to \(\mathbf{x}\).</p> <h3 id="the-linear-transformation">The Linear Transformation</h3> <p>Consider a linear transformation of the vector \(\mathbf{x}\) defined by:</p> \[\mathbf{y} = A\mathbf{x} + \mathbf{b}\] <p>Where \(A\) is a \(m \times k\) matrix representing the linear transformation, and \(\mathbf{b}\) represents a \(m\)-dimensional translation vector. \(\mathbf{y}\) is a new random vector, which we seek to analyze. Our goal is to show that \(\mathbf{y}\) is also Gaussian and derive the new mean and covariance matrix of \(\mathbf{y}\).</p> <p>To do this, we use the fact that the multivariate Gaussian distribution is closed under linear transformations. This means that if \(\mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x)\), then the transformed vector \(\mathbf{y}\) will follow a multivariate Gaussian distribution as well, albeit with a different mean and covariance matrix.</p> <hr> <h3 id="deriving-the-mean-and-covariance-of-mathbfy">Deriving the Mean and Covariance of \(\mathbf{y}\)</h3> <p><strong>Step 1: The New Mean Vector</strong></p> <p>The mean of \(\mathbf{y}\), denoted \(\mu_y\), can be derived by taking the expectation of \(\mathbf{y} = A\mathbf{x} + \mathbf{b}\):</p> \[\mathbb{E}[\mathbf{y}] = \mathbb{E}[A\mathbf{x} + \mathbf{b}] = A \mathbb{E}[\mathbf{x}] + \mathbf{b}\] <p>Since the expectation of \(\mathbf{x}\) is \(\mu_x\), we have:</p> \[\mu_y = A \mu_x + \mathbf{b}\] <p>Thus, the mean vector of \(\mathbf{y}\) is simply the linear transformation of the mean vector of \(\mathbf{x}\), plus the translation vector \(\mathbf{b}\).</p> <p><strong>Step 2: The New Covariance Matrix</strong></p> <p>Next, we derive the covariance matrix of \(\mathbf{y}\). The covariance matrix \(\Sigma_y\) of \(\mathbf{y}\) is given by the expected value of the outer product of \(\mathbf{y}\) with itself, minus the outer product of the mean of \(\mathbf{y}\) with itself:</p> \[\Sigma_y = \mathbb{E}[(\mathbf{y} - \mu_y)(\mathbf{y} - \mu_y)^T]\] <p>Substituting \(\mathbf{y} = A\mathbf{x} + \mathbf{b}\) into this equation, and noting that \(\mathbb{E}[\mathbf{y}] = \mu_y\), we get:</p> \[\Sigma_y = \mathbb{E}[(A\mathbf{x} + \mathbf{b} - \mu_y)(A\mathbf{x} + \mathbf{b} - \mu_y)^T]\] <p>Since \(\mathbf{b} - \mu_y = 0\) (by definition of \(\mu_y\)), this simplifies to:</p> \[\Sigma_y = \mathbb{E}[(A\mathbf{x})(A\mathbf{x})^T] = A \mathbb{E}[\mathbf{x} \mathbf{x}^T] A^T\] <p>Recall that \(\mathbb{E}[\mathbf{x} \mathbf{x}^T] = \Sigma_x\), the covariance matrix of \(\mathbf{x}\). Therefore:</p> \[\Sigma_y = A \Sigma_x A^T\] <p>This shows that the covariance matrix of the transformed vector \(\mathbf{y}\) is simply the original covariance matrix \(\Sigma_x\), transformed by the matrix \(A\) and its transpose.</p> <hr> <h3 id="demo-1">Demo</h3> <p>In this experiment, we explore the invariance of multivariate Gaussian distributions under linear transformations. Specifically, we start with a bivariate Gaussian distribution and apply a simple linear transformation, such as rotation, to the data. The transformation is visualized by comparing the original and transformed distributions using scatter plots.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Comparison%20of%20Covariance%20Ellipses%20for%20Original%20and%20Transformed%20Distributions-480.webp 480w,/assets/posts_img/2024-07-09/Comparison%20of%20Covariance%20Ellipses%20for%20Original%20and%20Transformed%20Distributions-800.webp 800w,/assets/posts_img/2024-07-09/Comparison%20of%20Covariance%20Ellipses%20for%20Original%20and%20Transformed%20Distributions-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-07-09/Comparison%20of%20Covariance%20Ellipses%20for%20Original%20and%20Transformed%20Distributions.png" class="img-fluid" width="600" height="400" alt="Comparison of Covariance Ellipses for Original and Transformed Distributions" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Additionally, we delve into how the covariance structure of the data changes under linear transformations. The covariance ellipses are plotted for both the original and transformed distributions, showing how the shape and orientation of the ellipses reflect the underlying correlations between the variables. This experiment demonstrates that, while the distribution remains Gaussian, the transformation alters the data’s spread and direction,</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Comparison%20of%20Original%20and%20Transformed%20Bivariate%20Gaussian%20Distributions-480.webp 480w,/assets/posts_img/2024-07-09/Comparison%20of%20Original%20and%20Transformed%20Bivariate%20Gaussian%20Distributions-800.webp 800w,/assets/posts_img/2024-07-09/Comparison%20of%20Original%20and%20Transformed%20Bivariate%20Gaussian%20Distributions-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-07-09/Comparison%20of%20Original%20and%20Transformed%20Bivariate%20Gaussian%20Distributions.png" class="img-fluid" width="600" height="400" alt="Comparison of Original and Transformed Bivariate Gaussian Distributions" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">matplotlib.patches</span> <span class="k">as</span> <span class="n">patches</span>

<span class="c1"># Set random seed for reproducibility
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Define original mean and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Mean vector
</span><span class="n">sigma</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>  <span class="c1"># Covariance matrix with positive correlation
</span>
<span class="c1"># Generate samples from the bivariate Gaussian distribution
</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mi">5000</span><span class="p">).</span><span class="n">T</span>

<span class="c1"># Define a simple rotation matrix (45 degrees)
</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span>  <span class="c1"># Rotation by 45 degrees
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># No translation
</span>
<span class="c1"># Apply the linear transformation
</span><span class="n">xy_transformed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]))</span>  <span class="c1"># Perform the linear transformation
</span><span class="n">xy_transformed</span> <span class="o">=</span> <span class="n">xy_transformed</span> <span class="o">+</span> <span class="n">b</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>  <span class="c1"># Apply the translation vector separately
</span>
<span class="c1"># Plot the original and transformed distributions on the same axes
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Plot the original distribution with transparency
</span><span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Blues</span><span class="sh">"</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Original Distribution</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plot the transformed distribution with transparency
</span><span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xy_transformed</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">xy_transformed</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Oranges</span><span class="sh">"</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Transformed Distribution</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Add titles and labels
</span><span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Comparison of Original and Transformed Bivariate Gaussian Distributions</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X2</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Display legend to differentiate the distributions
</span><span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Function to plot covariance ellipse
</span><span class="k">def</span> <span class="nf">plot_cov_ellipse</span><span class="p">(</span><span class="n">covariance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1"># Calculate eigenvalues and eigenvectors
</span>    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">covariance</span><span class="p">)</span>
    <span class="n">order</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Sort by eigenvalue size
</span>    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">order</span><span class="p">],</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">order</span><span class="p">]</span>
    
    <span class="c1"># Calculate rotation angle
</span>    <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigenvectors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    
    <span class="c1"># Plot the covariance ellipse
</span>    <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span>
    <span class="n">ell</span> <span class="o">=</span> <span class="n">patches</span><span class="p">.</span><span class="nc">Ellipse</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>

<span class="c1"># Plot covariance ellipses for both distributions on the same axes
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Covariance ellipse for original distribution
</span><span class="nf">plot_cov_ellipse</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">skyblue</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Original Samples</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Covariance ellipse for transformed distribution
</span><span class="n">transformed_sigma</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">sigma</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span>
<span class="nf">plot_cov_ellipse</span><span class="p">(</span><span class="n">transformed_sigma</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">xy_transformed</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy_transformed</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Transformed Samples</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Add titles and labels
</span><span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Comparison of Covariance Ellipses for Original and Transformed Distributions</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X2</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Display legend
</span><span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div> <hr> <h2 id="chi2-distribution-and-the-ellipsoid-theorem">\(\chi^2\) Distribution and the Ellipsoid Theorem</h2> <h3 id="the-chi2-distribution-derivation-from-multivariate-gaussian">The \(\chi^2\) Distribution: Derivation from Multivariate Gaussian</h3> <p>Let’s begin with the first concept: the \(\chi^2\) distribution. Suppose that \(\mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x)\) is a \(k\)-dimensional Gaussian vector, and we are interested in the quadratic form \(Q\):</p> \[Q = \mathbf{x}^T \Sigma^{-1} \mathbf{x}\] <p>This expression appears frequently in multivariate statistics, particularly when we are testing hypotheses about the mean vector \(\mu_x\). We’ll show that \(Q\) follows a \(\chi^2\) distribution with \(k\) degrees of freedom.</p> <p><strong>Step 1: The Transformation to Standard Normal</strong></p> <p>To analyze \(Q\), we first standardize the vector \(\mathbf{x}\). Let’s consider the transformation \(\mathbf{z} = \Sigma^{-1/2} (\mathbf{x} - \mu_x)\), where \(\Sigma^{-1/2}\) is the matrix square root of \(\Sigma^{-1}\). Under this transformation, \(\mathbf{z}\) follows the standard multivariate normal distribution:</p> \[\mathbf{z} \sim \mathcal{N}(0, I)\] <p>Here, \(I\) is the \(k \times k\) identity matrix. In this new coordinate system, the quadratic form becomes:</p> \[Q = (\Sigma^{-1/2} (\mathbf{x} - \mu_x))^T (\Sigma^{-1/2} (\mathbf{x} - \mu_x)) = \mathbf{z}^T \mathbf{z}\] <p><strong>Step 2: The \(\chi^2\) Distribution</strong></p> <p>Now, notice that \(\mathbf{z}^T \mathbf{z}\) is the sum of the squares of \(k\) independent standard normal random variables:</p> \[Q = \sum_{i=1}^k z_i^2\] <p>Each \(z_i \sim \mathcal{N}(0,1)\), so \(z_i^2\) follows a \(\chi^2\) distribution with 1 degree of freedom. Thus, the sum of these independent \(\chi^2\) variables, \(Q\), follows a \(\chi^2\) distribution with \(k\) degrees of freedom:</p> \[Q \sim \chi^2_k\] <p>This result tells us that any quadratic form \(\mathbf{x}^T \Sigma^{-1} \mathbf{x}\) in a multivariate Gaussian vector follows a \(\chi^2\) distribution with degrees of freedom equal to the dimensionality of the vector \(\mathbf{x}\).</p> <h3 id="demo-for--the-chi2-distribution">Demo for the \(\chi^2\) Distribution</h3> <p>To test this, we simulate 1,000 samples from a 3-dimensional multivariate Gaussian distribution with a mean vector of zeros and an identity covariance matrix. For each sample, we compute the quadratic form \(Q\), which essentially measures the “distance” of each sample from the mean in a scaled manner. By plotting the histogram of these values, we can compare it with the theoretical \(\chi^2\) distribution with 3 degrees of freedom.</p> <p>The resulting plot includes the simulated distribution of \(Q\), along with the theoretical \(\chi^2\) distribution curve, cumulative distribution function (CDF), the 95% critical value, and markers for the mean and standard deviation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Chi-Squared%20Distribution%20with%203%20Degrees%20of%20Freedom-480.webp 480w,/assets/posts_img/2024-07-09/Chi-Squared%20Distribution%20with%203%20Degrees%20of%20Freedom-800.webp 800w,/assets/posts_img/2024-07-09/Chi-Squared%20Distribution%20with%203%20Degrees%20of%20Freedom-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-07-09/Chi-Squared%20Distribution%20with%203%20Degrees%20of%20Freedom.png" class="img-fluid" width="700" height="487" alt="Chi-Squared Distribution with 3 Degrees of Freedom" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2</span>

<span class="c1"># Set experiment parameters
</span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Degrees of freedom, as we are using a 3-dimensional Gaussian
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># Mean vector (all zeros)
</span><span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># Covariance matrix (identity matrix for simplicity)
</span>
<span class="c1"># Generate samples from a multivariate normal distribution
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># Set seed for reproducibility
</span><span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">Sigma</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># 1,000 samples
</span>
<span class="c1"># Compute the quadratic form Q = x^T * Sigma_inv * x for each sample
</span><span class="n">Sigma_inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>  <span class="c1"># Inverse of the covariance matrix
</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">samples</span> <span class="o">@</span> <span class="n">Sigma_inv</span><span class="p">)</span> <span class="o">*</span> <span class="n">samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Quadratic form values
</span>
<span class="c1"># Plot the histogram of Q values with kernel density estimation (KDE)
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="sh">"</span><span class="s">density</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">skyblue</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Empirical Distribution</span><span class="sh">"</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1"># Plot the theoretical Chi-Squared PDF
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">Q</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># Range for theoretical curve
</span><span class="n">y</span> <span class="o">=</span> <span class="n">chi2</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># Chi-Squared  PDF with k degrees of freedom
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Chi-squared Distribution (df=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot the Chi-Squared  CDF for additional reference
</span><span class="n">y_cdf</span> <span class="o">=</span> <span class="n">chi2</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_cdf</span><span class="p">,</span> <span class="sh">'</span><span class="s">g--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Chi-squared CDF (df=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Mark the 95% critical value for the Chi-Squared distribution
</span><span class="n">critical_value_95</span> <span class="o">=</span> <span class="n">chi2</span><span class="p">.</span><span class="nf">ppf</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">critical_value_95</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">95% Critical Value (df=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Add text annotation for the 95% critical value
</span><span class="n">plt</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">critical_value_95</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Critical Value = </span><span class="si">{</span><span class="n">critical_value_95</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># Mark the mean and standard deviation lines
</span><span class="n">mean</span> <span class="o">=</span> <span class="n">k</span>  <span class="c1"># Mean of Chi-Squared  with k degrees of freedom
</span><span class="n">std_dev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">k</span><span class="p">)</span>  <span class="c1"># Standard deviation of Chi-Squared  with k degrees of freedom
</span><span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">purple</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean = </span><span class="si">{</span><span class="n">mean</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">mean</span> <span class="o">+</span> <span class="n">std_dev</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">purple</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean + 1 SD</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">mean</span> <span class="o">-</span> <span class="n">std_dev</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">purple</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean - 1 SD</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Beautify and finalize the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Chi-Squared Distribution with </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s"> Degrees of Freedom</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Q Value</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Display the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <hr> <h3 id="the-ellipsoid-theorem-geometrical-interpretation-of-gaussian-contours">The Ellipsoid Theorem: Geometrical Interpretation of Gaussian Contours</h3> <p>The second concept we explore is the ellipsoid theorem, which describes the shape of the level sets (contours) of a multivariate Gaussian distribution. Specifically, we will prove that the contour lines of a multivariate Gaussian distribution are ellipsoids, and we will derive their geometric properties.</p> <p>Let’s consider again a random vector \(\mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x)\). The PDF for \(\mathbf{x}\) is given by:</p> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2} |\Sigma_x|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x) \right)\] <p>The contour lines of the Gaussian distribution correspond to the set of points \(\mathbf{x}\) for which the PDF is constant. That is, we want to solve:</p> \[(\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x) = c\] <p>where \(c\) is a constant. This equation represents a set of points that lie on a surface with a constant value of the quadratic form \((\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x)\), which we will show is an ellipsoid.</p> <p><strong>Step 1: Eigenvalue Decomposition of the Covariance Matrix</strong></p> <p>To understand the geometry of this surface, we perform the eigenvalue decomposition of the covariance matrix \(\Sigma_x\):</p> \[\Sigma_x = Q \Lambda Q^T\] <p>where \(Q\) is the orthogonal matrix of eigenvectors of \(\Sigma_x\), and \(\Lambda\) is the diagonal matrix of eigenvalues of \(\Sigma_x\):</p> \[\Lambda = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_k)\] <p>Thus, the quadratic form can be rewritten as:</p> \[(\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x) = (\mathbf{z})^T \Lambda^{-1} \mathbf{z} = \sum_{i=1}^k \frac{z_i^2}{\lambda_i}\] <p>where \(\mathbf{z} = Q^T (\mathbf{x} - \mu_x)\) is the transformed vector in the eigenbasis of \(\Sigma_x\), and \(z_i\) are the components of \(\mathbf{z}\). This shows that the level sets of the multivariate Gaussian distribution are ellipsoids, with axes scaled by the square roots of the eigenvalues \(\lambda_i\) of \(\Sigma_x\).</p> <p><strong>Step 2: Geometry of the Ellipsoid</strong></p> <p>In the transformed space, the equation describing the level set becomes:</p> \[\sum_{i=1}^k \frac{z_i^2}{\lambda_i} = c\] <p>This is the equation of an ellipsoid in the \(z\)-coordinates. The shape of this ellipsoid depends on the eigenvalues \(\lambda_i\) of the covariance matrix \(\Sigma_x\). The axes of the ellipsoid are aligned with the eigenvectors of \(\Sigma_x\), and the lengths of the axes are proportional to the square roots of the corresponding eigenvalues.</p> <p>Thus, the geometry of the level sets (or contours) of a multivariate Gaussian distribution is determined by the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors determine the direction of the axes of the ellipsoid, and the eigenvalues determine their lengths (the standard deviations along those axes).</p> <h3 id="demo-for-the-ellipsoid-theorem">Demo for the Ellipsoid Theorem</h3> <p>To illustrate this, we generate samples from three different 2-dimensional Gaussian distributions, each with a unique covariance matrix. Each covariance matrix introduces a different level of correlation between the dimensions, which changes the orientation and shape of the ellipsoidal contours. We visualize these contours using concentric ellipses representing one, two, and three standard deviations from the mean. These ellipses are derived from the eigenvalues and eigenvectors of the covariance matrices, where the eigenvalues define the axis lengths, and the eigenvectors determine the rotation of each ellipse. The final plot overlays scatter plots of each Gaussian sample set to show sample density, and the ellipsoid contours at multiple standard deviations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Ellipsoid%20Contours%20for%20Different%20Covariance%20Matrices-480.webp 480w,/assets/posts_img/2024-07-09/Ellipsoid%20Contours%20for%20Different%20Covariance%20Matrices-800.webp 800w,/assets/posts_img/2024-07-09/Ellipsoid%20Contours%20for%20Different%20Covariance%20Matrices-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/posts_img/2024-07-09/Ellipsoid%20Contours%20for%20Different%20Covariance%20Matrices.png" class="img-fluid" width="700" height="450" alt="Ellipsoid Contours for Different Covariance Matrices" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">from</span> <span class="n">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>

<span class="c1"># Set experiment parameters
</span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Mean vector
</span><span class="n">covariances</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>  <span class="c1"># Covariance matrix 1
</span>    <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>  <span class="c1"># Covariance matrix 2
</span>    <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>  <span class="c1"># Covariance matrix 3
</span><span class="p">]</span>

<span class="c1"># Generate samples for each covariance matrix
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span> <span class="k">for</span> <span class="n">cov</span> <span class="ow">in</span> <span class="n">covariances</span><span class="p">]</span>  <span class="c1"># 500 samples per covariance matrix
</span>
<span class="c1"># Set up plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Define colormaps and colors for each covariance matrix
</span><span class="n">colormaps</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Blues</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Greens</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Reds</span><span class="sh">"</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Plot sample distributions and ellipsoid contours
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">covariances</span><span class="p">)):</span>
    <span class="c1"># Density plot for samples with KDE
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">colormaps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="c1"># Scatter plot for samples with explicit color
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Samples with Covariance </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Draw ellipsoids for 1, 2, and 3 standard deviations
</span>    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>  <span class="c1"># Eigenvalues and eigenvectors for covariance matrix
</span>    <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigenvectors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>  <span class="c1"># Rotation angle in degrees
</span>    <span class="k">for</span> <span class="n">n_std</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># 1, 2, 3 standard deviations
</span>        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span>  <span class="c1"># Ellipse width and height
</span>        <span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span>
                          <span class="n">edgecolor</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>

<span class="c1"># Add title and axis labels
</span><span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Ellipsoid Contours for Different Covariance Matrices</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X1</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X2</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">"</span><span class="s">upper right</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Beautify the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">"</span><span class="s">both</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div> <hr> <h2 id="conclusion">Conclusion</h2> <p>In this blog, we introduced the key properties of the multivariate Gaussian distribution, including the independence of random variables, the derivation of marginal and conditional distributions, closure under linear transformations, and its geometric characteristics. These properties form the foundation of probabilistic modeling and serve as building blocks for more complex models.</p> <p>As a natural extension of the multivariate Gaussian distribution, Gaussian Mixture Models (GMMs) combine multiple Gaussian components to flexibly capture multimodal characteristics in data. In the next blog, we will explore the mathematical principles behind GMMs and their applications as generative models.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Enjoy Reading This Article? Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Boltzmann/">Maximal Entropy of the Boltzmann Distribution: A Quantum Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/multivariate_Gaussian_distribution/">Is the Transition from Univariate to Multivariate Gaussian Distribution Linear?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Correlation_Coefficients/">A Supplementary Discussion on Correlation Coefficients</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">An Introductory Look at Covariance and the Mean Vector</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/EC525_4/">Problem Ⅳ: Dual Spaces and Functional Analysis</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Shuhong Dai. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-maximal-entropy-of-the-boltzmann-distribution-a-quantum-perspective",title:"Maximal Entropy of the Boltzmann Distribution: A Quantum Perspective",description:"Specifically, in the classical limit (when \u03b2En &lt;&lt; 1), the quantum energy levels become very close to each other, and the partition function can be approximated by an integral over continuous energy states rather than a sum over discrete states. In this limit, the quantum Boltzmann distribution approaches the classical result...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Boltzmann/"}},{id:"post-a-sketch-of-proofs-for-some-properties-of-multivariate-gaussian-distribution",title:"A Sketch of Proofs for Some Properties of Multivariate Gaussian Distribution",description:"When we diagonalize the covariance matrix, we essentially rotate the space such that the axes align with the principal directions of variation...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Proofs-for-Some-Properties/"}},{id:"post-is-the-transition-from-univariate-to-multivariate-gaussian-distribution-linear",title:"Is the Transition from Univariate to Multivariate Gaussian Distribution Linear?",description:"Now that we have the foundation in place, let\u2019s shift gears and consider the generalization of the univariate Gaussian to higher dimensions. In the multivariate case, we are no longer dealing with a single random variable, but rather a vector of random variables...",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate_Gaussian_distribution/"}},{id:"post-a-supplementary-discussion-on-correlation-coefficients",title:"A Supplementary Discussion on Correlation Coefficients",description:"Yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Correlation_Coefficients/"}},{id:"post-an-introductory-look-at-covariance-and-the-mean-vector",title:"An Introductory Look at Covariance and the Mean Vector",description:"If the mean vector gives us a sense of location, then the covariance matrix gives us a sense of shape...",section:"Posts",handler:()=>{window.location.href="/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/"}},{id:"post-problem-dual-spaces-and-functional-analysis",title:"Problem \u2163: Dual Spaces and Functional Analysis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/EC525_4/"}},{id:"post-sampling-smarter-unlocking-the-power-of-latin-hypercube-sampling",title:"Sampling Smarter: Unlocking the Power of Latin Hypercube Sampling",description:"Unlike random sampling, which might leave some regions underrepresented while others get chosen repeatedly...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Latin_Hypercube_Sampling/"}},{id:"post-problem-group-theory",title:"Problem \u2162: Group Theory",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/EC525_3/"}},{id:"post-problem-linear-transformations",title:"Problem \u2161: Linear Transformations",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/EC525_2/"}},{id:"post-problem-vector-spaces",title:"Problem \u2160: Vector Spaces",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/EC525_1/"}},{id:"post-preface-motivation-and-overview",title:"Preface: Motivation and Overview",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/EC525_0/"}},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-won-the-1st-prize-with-a-designed-four-switch-buck-boost-circuit-in-the-university-electronic-design-competition-sponsored-by-huawei",title:"Won the 1st prize with a designed four-switch buck-boost circuit in the university...",description:"",section:"News"},{id:"news-received-the-first-class-academic-scholarship",title:"Received the First-Class Academic Scholarship.",description:"",section:"News"},{id:"news-won-the-3rd-prize-for-problem-g-quot-non-contact-object-measurement-device-quot-at-the-8th-national-undergraduate-electronics-indesign-contest",title:"Won the 3rd prize for Problem G: &quot;Non-contact Object Measurement Device&quot; at the...",description:"",section:"News"},{id:"news-visited-the-key-laboratory-of-advanced-energy-traction-and-comprehensive-energy-saving-railway-industry-at-swjtu",title:"Visited the Key Laboratory of Advanced Energy Traction and Comprehensive Energy Saving Railway...",description:"",section:"News"},{id:"news-received-the-first-class-academic-scholarship",title:"Received the First-Class Academic Scholarship.",description:"",section:"News"},{id:"news-won-the-3rd-prize-in-problem-b-quot-ethanol-coupling-to-produce-c4-olefins-quot-at-the-30th-china-undergraduate-mathematical-contest-in-modeling",title:"Won the 3rd prize in Problem B: &quot;Ethanol Coupling to Produce C4 Olefins&quot;...",description:"",section:"News"},{id:"news-won-the-1st-prize-in-the-preliminary-national-round-of-the-5th-chinese-education-cup-mathematics-competition",title:"Won the 1st prize in the preliminary national round of the 5th Chinese...",description:"",section:"News"},{id:"news-won-the-2nd-prize-in-the-national-finals-of-the-5th-chinese-education-cup-mathematics-competition",title:"Won the 2nd prize in the national finals of the 5th Chinese Education...",description:"",section:"News"},{id:"news-won-the-1st-prize-at-the-provincial-level-in-the-13th-chinese-mathematics-competitions",title:"Won the 1st prize at the provincial level in the 13th Chinese Mathematics...",description:"",section:"News"},{id:"news-guided-the-quot-dual-car-following-system-quot-project-which-won-the-highest-award-the-quot-ti-cup-quot-in-the-10th-national-undergraduate-electronics-design-contest",title:"Guided the &quot;Dual-Car Following System&quot; project which won the highest award, the &quot;TI...",description:"",section:"News"},{id:"news-obtained-my-bachelor-39-s-degree-in-electrical-engineering",title:"Obtained my Bachelor&#39;s Degree in Electrical Engineering.",description:"",section:"News"},{id:"news-joined-the-distributed-systems-group-at-ncepu-to-pursue-my-master-39-s-degree-in-computer-science",title:"Joined the Distributed Systems Group at NCEPU to pursue my Master&#39;s degree in...",description:"",section:"News"},{id:"news-participated-in-world-robot-conference-2024-in-beijing",title:"Participated in World Robot Conference 2024 in Beijing.",description:"",section:"News"},{id:"news-a-paper-on-data-compression-https-ieeexplore-ieee-org-document-10376424-was-accepted-by-ieee-iot-journal",title:"[A paper on data compression](https://ieeexplore.ieee.org/document/10376424) was accepted by _IEEE IoT Journal_.",description:"",section:"News"},{id:"news-a-paper-on-energy-efficient-computing-https-www-sciencedirect-com-science-article-pii-s0140366423004802-via-3dihub-was-accepted-by-computer-communications",title:"[A paper on energy-efficient computing](https://www.sciencedirect.com/science/article/pii/S0140366423004802?via%3Dihub) was accepted by _Computer Communications_.",description:"",section:"News"},{id:"news-participated-in-microsoft-ai-day-beijing",title:"Participated in Microsoft AI Day Beijing.",description:"",section:"News"},{id:"news-a-paper-on-vehicle-road-cooperation-https-ieeexplore-ieee-org-document-10632106-was-accepted-by-ieee-iot-journal",title:"[A paper on vehicle-road cooperation](https://ieeexplore.ieee.org/document/10632106) was accepted by _IEEE IoT Journal_.",description:"",section:"News"},{id:"news-i-m-thrilled-to-announce-the-release-of-texhelper-https-pypi-org-project-texhelper-a-python-library-designed-to-optimize-and-beautify-tex-code-texhelper-makes-managing-your-latex-based-projects-easier-and-more-elegant-the-project-is-open-source-and-available-on-github-https-github-com-shuhongdai-texhelper-check-it-out",title:"\ud83c\udf89\ud83c\udf89\ud83c\udf89 I\u2019m thrilled to announce the release of [TexHelper](https://pypi.org/project/texhelper/), a Python library designed...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%64%61%69%73%68%75%68%6F%6E%67%30%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=lwCWmPUAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>