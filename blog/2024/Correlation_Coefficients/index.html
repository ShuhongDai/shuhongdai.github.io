<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="3hx6hRZ1EJWUbqkkXZ-vPQfpV0JdKNKT_aKcRjSixr0"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Supplementary Discussion on Correlation Coefficients | Shuhong Dai </title> <meta name="author" content="Shuhong Dai"> <meta name="description" content="Yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales..."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?6d57c5bac70ef6fae4bd96883a4eb4da"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shuhong</span> Dai </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Supplementary Discussion on Correlation Coefficients</h1> <p class="post-meta"> Created in June 22, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/tag/stochastic-processes"> <i class="fa-solid fa-hashtag fa-sm"></i> Stochastic Processes</a>   ·   <a href="/blog/category/a-column-on-gaussian-processes"> <i class="fa-solid fa-tag fa-sm"></i> A Column on Gaussian Processes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>In my <a href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">previous blog post</a>, we examined covariance matrices, looking into their derivation, properties, and the geometric insights they offer for understanding multidimensional data. Covariance matrices provide a solid foundation for understanding how variables interact, yet they have certain limitations—especially when we want a clearer measure of the strength of these relationships.</p> <p>This brings us to correlation and the correlation matrix, essential concepts for interpreting the strength of relationships between variables independently of their original units or scales. Given their importance, this follow-up post serves as a brief guide to correlation coefficients and the correlation matrix. Here, we’ll cover the basics of correlation, the structure and derivation of the correlation matrix, and some of its most useful properties and applications.</p> <p>In the previous discussion on covariance, we established a foundational understanding of how variables co-vary, yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales. Here, the correlation coefficient, \(\rho_{X,Y}\), refines this measure by standardizing the relationship between two variables, allowing a comparison of their linear association independent of units.</p> <hr> <h2 id="the-correlation-coefficient">The Correlation Coefficient</h2> <p>In the previous discussion on covariance, we established a foundational understanding of how variables co-vary, yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales. Here, the correlation coefficient, \(\rho_{X,Y}\), refines this measure by standardizing the relationship between two variables, allowing a comparison of their linear association independent of units.</p> <h3 id="definition">Definition</h3> <p>The correlation coefficient \(\rho_{X,Y}\) between two random variables \(X\) and \(Y\) is formally defined as:</p> \[\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\] <p>where \(\text{Cov}(X, Y)\) represents the covariance between \(X\) and \(Y\), \(\sigma_X\) and \(\sigma_Y\) are the standard deviations of \(X\) and \(Y\), respectively.</p> <p>This formula can be viewed as a “normalized” covariance, essentially adjusting the relationship between \(X\) and \(Y\) by their individual dispersions, or spreads. This normalization is the key: it removes the influence of scale, making \(\rho_{X,Y}\) a dimensionless quantity that ranges from -1 to 1.</p> <h3 id="derivation-from-covariance">Derivation from Covariance</h3> <p>To delve deeper, consider the covariance definition between two random variables \(X\) and \(Y\):</p> \[\text{Cov}(X, Y) = E[(X - \mu_X)(Y - \mu_Y)]\] <p>where \(\mu_X = E[X]\) and \(\mu_Y = E[Y]\) are the expected values (means) of \(X\) and \(Y\). Covariance, while useful, retains the units of \(X\) and \(Y\), which complicates comparisons across variables with differing units or scales.</p> <p>The correlation coefficient refines this by dividing covariance by the product of the standard deviations of \(X\) and \(Y\):</p> \[\sigma_X = \sqrt{E[(X - \mu_X)^2]}, \quad \sigma_Y = \sqrt{E[(Y - \mu_Y)^2]}\] <p>Thus, correlation can be expressed as:</p> \[\rho_{X,Y} = \frac{E[(X - \mu_X)(Y - \mu_Y)]}{\sigma_X \sigma_Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\] <p>This reformulation gives us a measure of association on a standardized scale. Specifically, if \(\rho_{X,Y} = 1\), there exists a perfect positive linear relationship between \(X\) and \(Y\): as \(X\) increases, \(Y\) increases proportionally. If \(\rho_{X,Y} = -1\), the variables exhibit perfect negative linear correlation, where \(Y\) decreases as \(X\) increases. If \(\rho_{X,Y} = 0\), there is no linear relationship between \(X\) and \(Y\).</p> <h3 id="other-properties">Other Properties</h3> <p>One of the most useful features of \(\rho_{X,Y}\) is its unit invariance. Unlike covariance, which scales with the units of \(X\) and \(Y\), correlation removes these effects by normalizing with standard deviations, making \(\rho_{X,Y}\) dimensionless. This quality enables comparisons across variables with different units or scales, ensuring that the strength of relationships is evaluated consistently, regardless of measurement. Furthermore, the symmetry of correlation is another noteworthy property: \(\rho_{X,Y} = \rho_{Y,X}\). This symmetry arises naturally from the covariance term and reinforces that correlation measures a mutual relationship between variables, independent of which one is considered first. These properties, together, establish the correlation coefficient as a powerful, standardized metric for interpreting linear dependencies across diverse contexts</p> <hr> <h2 id="the-correlation-matrix">The Correlation Matrix</h2> <p>When moving from a single pair of variables to multidimensional data, we naturally extend the concept of correlation to capture all pairwise relationships at once. This is where the correlation matrix \(R\) comes in, giving us a compact summary of the linear associations across an entire dataset. For a random vector \(X = [X_1, X_2, \dots, X_n]^T\), the correlation matrix \(R\) is an \(n \times n\) matrix, where each element \(R_{ij}\) represents the correlation coefficient between \(X_i\) and \(X_j\):</p> \[R_{ij} = \rho_{X_i, X_j} = \frac{\text{Cov}(X_i, X_j)}{\sigma_{X_i} \sigma_{X_j}}\] <h3 id="definition-and-structure">Definition and Structure</h3> <p>In other words, if \(X\) is a random vector with a covariance matrix \(\Sigma\), then \(R\) is constructed element-by-element as:</p> \[R = \begin{bmatrix} \rho_{X_1, X_1} &amp; \rho_{X_1, X_2} &amp; \dots &amp; \rho_{X_1, X_n} \\ \rho_{X_2, X_1} &amp; \rho_{X_2, X_2} &amp; \dots &amp; \rho_{X_2, X_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \rho_{X_n, X_1} &amp; \rho_{X_n, X_2} &amp; \dots &amp; \rho_{X_n, X_n} \end{bmatrix}.\] <p>Each diagonal entry \(R_{ii}\) equals 1, since \(\rho_{X_i, X_i} = \frac{\text{Cov}(X_i, X_i)}{\sigma_{X_i}^2} = 1\). This makes sense because a variable is perfectly correlated with itself. The off-diagonal elements \(R_{ij}\) (for \(i \neq j\)) give us the correlation between different variables, capturing their linear relationships in a standardized form that’s easy to interpret across the entire matrix.</p> <h3 id="relationship-to-the-covariance-matrix">Relationship to the Covariance Matrix</h3> <p>The correlation matrix \(R\) is directly derived from the covariance matrix \(\Sigma\) by a process of standardization. The idea here is to convert the units and scale-dependent covariances into unit-free, comparable correlation values. To achieve this, we divide each covariance by the product of the standard deviations of the relevant variables, so we arrive at the expression:</p> \[R = D^{-1} \Sigma D^{-1}\] <p>where \(D\) is the diagonal matrix of standard deviations, given by:</p> \[D = \text{diag}(\sigma_{X_1}, \sigma_{X_2}, \dots, \sigma_{X_n}).\] <p>Expanding this calculation, let’s see how each entry in \(R\) is computed in terms of \(\Sigma\) and \(D\):</p> <ol> <li>Start with the covariance matrix element \(\Sigma_{ij} = \text{Cov}(X_i, X_j)\).</li> <li>Divide \(\Sigma_{ij}\) by \(\sigma_{X_i} \sigma_{X_j}\), where \(\sigma_{X_i} = \sqrt{\Sigma_{ii}}\) and \(\sigma_{X_j} = \sqrt{\Sigma_{jj}}\).</li> <li> <p>This yields each element in \(R\) as:</p> \[R_{ij} = \frac{\Sigma_{ij}}{\sigma_{X_i} \sigma_{X_j}}.\] </li> </ol> <p>In matrix form, we achieve this by pre-multiplying and post-multiplying \(\Sigma\) with \(D^{-1}\), transforming the raw covariance entries into standardized, unitless correlation coefficients. This transformation is crucial when comparing variables measured on different scales, as it removes any units, letting us focus purely on the strength of the relationships.</p> <h3 id="some-properties">Some Properties</h3> <p>The correlation matrix \(R\) possesses a few key properties that make it an elegant and powerful tool for analyzing multidimensional data. First, it is symmetric by nature, since \(\rho_{X_i, X_j} = \rho_{X_j, X_i}\) for any pair of variables \(X_i\) and \(X_j\). This symmetry ensures that each pairwise correlation is mutual and gives the matrix a balanced, mirror-like structure around its diagonal. This diagonal, in turn, consists entirely of ones, as each variable is perfectly correlated with itself—a subtle reminder that correlation is inherently a self-consistent measure.</p> <p>In addition to its symmetry, \(R\) is positive semi-definite, meaning that for any vector \(z\), the quadratic form \(z^T R z\) is non-negative. This positive semi-definiteness implies that all eigenvalues of \(R\) are non-negative, which is significant because it confirms that \(R\) has a stable variance structure. This property becomes particularly valuable in applications like principal component analysis (PCA), where the correlation matrix’s eigenvalues reflect the spread of the data along different directions.</p> <p>Furthermore, every element in \(R\) falls within the interval \([-1, 1]\), a direct result of the correlation coefficient’s own bounded nature. This bounded range ensures that each entry in \(R\) is a pure, unitless indicator of linear association strength. Regardless of the scale or units of the original variables, the values in \(R\) give a consistent, standardized view of how variables align with each other.</p> <hr> <h2 id="the-geometric-meaning">The Geometric Meaning</h2> <h3 id="geometric-interpretation-angles-and-alignments">Geometric Interpretation: Angles and Alignments</h3> <p>Consider two random variables \(X_i\) and \(X_j\), each represented as vectors in an \(n\)-dimensional data space. The correlation coefficient \(\rho_{X_i, X_j}\) between \(X_i\) and \(X_j\) can be understood as the cosine of the angle \(\theta\) between these two vectors. Formally, this relationship is given by:</p> \[\rho_{X_i, X_j} = \cos \theta_{ij}\] <p>where \(\theta_{ij}\) is the angle between the vectors corresponding to \(X_i\) and \(X_j\). When \(\rho_{X_i, X_j} = 1\), the vectors point in the same direction (\(\theta = 0^\circ\)), indicating a perfect positive linear relationship. Conversely, if \(\rho_{X_i, X_j} = -1\), the vectors point in opposite directions (\(\theta = 180^\circ\)), representing a perfect negative linear relationship. A correlation of zero corresponds to \(\theta = 90^\circ\), suggesting that the vectors are orthogonal and thus linearly uncorrelated.</p> <p>This geometric interpretation gives a clear, visual sense of the relationships encoded in \(R\): the closer the angle between two variables’ vectors is to zero, the stronger and more positive their correlation; the closer the angle is to \(180^\circ\), the stronger and more negative the correlation. And when the vectors are perpendicular, they are uncorrelated in a linear sense, even though nonlinear relationships might still exist.</p> <h3 id="eigenvalues-and-eigenvectors-the-structure-of-r">Eigenvalues and Eigenvectors: The Structure of \(R\)</h3> <p>The geometric story of \(R\) deepens when we consider its eigenvalues and eigenvectors. By performing an eigen-decomposition on \(R\), we can break it down as follows:</p> \[R = Q \Lambda Q^T\] <p>where \(Q\) is an orthogonal matrix whose columns are the eigenvectors of \(R\), and \(\Lambda\) is a diagonal matrix containing the eigenvalues of \(R\). Each eigenvalue \(\lambda_i\) in \(\Lambda\) represents the variance explained along the direction specified by the corresponding eigenvector \(q_i\) in \(Q\).</p> <p>Geometrically, the eigenvectors of \(R\) indicate the principal directions of variance in the data. The eigenvalues, on the other hand, tell us the “lengths” or “strengths” of these directions. A large eigenvalue associated with an eigenvector indicates that the data has significant variance along that direction, meaning the variables exhibit strong alignment with each other in this space. Conversely, smaller eigenvalues correspond to directions with less variance, suggesting that the data is more tightly clustered or linearly dependent in those directions.</p> <p>Since \(R\) is positive semi-definite, all eigenvalues are non-negative, and the sum of the eigenvalues equals the dimensionality of the space. Each eigenvalue-eigenvector pair offers a glimpse into the “shape” of the data cloud in terms of its stretch and orientation in different directions.</p> <h3 id="pca-extracting-linear-patterns">PCA: Extracting Linear Patterns</h3> <p>The correlation matrix \(R\) is a natural tool for conducting PCA, a technique used to identify the main linear patterns in data by transforming it into a new coordinate system based on the eigenvectors of \(R\). In PCA, the data is projected onto the eigenvectors of \(R\), with each projection representing a principal component. The eigenvalue associated with each eigenvector measures the variance along that principal component, so ordering the eigenvalues from largest to smallest provides a ranking of the directions of greatest variability.</p> <p>To perform PCA, we start by obtaining the eigen-decomposition of \(R\):</p> \[R = Q \Lambda Q^T\] <p>The columns of \(Q\), the eigenvectors, form the new basis in which the data is represented. Each eigenvector corresponds to a principal component, and the associated eigenvalue indicates the amount of variance captured by that component. For instance, the first principal component (corresponding to the largest eigenvalue) captures the direction of maximum variance in the data, providing the best one-dimensional summary of the data’s spread. Adding successive principal components gives progressively refined approximations of the data, capturing most of the structure with far fewer dimensions than the original dataset.</p> <p>By using \(R\) in PCA, we effectively perform dimensionality reduction based on the strength of the linear relationships between variables, preserving the most informative aspects of the data while discarding redundancy.</p> <h3 id="demo">Demo</h3> <p>To illustrate how PCA leverages the correlation matrix \(R\) to extract meaningful linear patterns, let’s walk through a concrete example. Suppose we have a dataset with three variables—say, height, weight, and age—measured across a sample of individuals. Let’s assume that after standardizing the data, we calculate the correlation matrix \(R\) as follows:</p> \[R = \begin{bmatrix} 1 &amp; 0.8 &amp; 0.5 \\ 0.8 &amp; 1 &amp; 0.4 \\ 0.5 &amp; 0.4 &amp; 1 \end{bmatrix}\] <p>Each entry \(R_{ij}\) represents the correlation between pairs of variables. For example, the correlation between height and weight is 0.8, indicating a strong positive linear relationship, while the correlation between height and age is 0.5, a moderate positive association.</p> <p><strong>Step 1: Eigen-Decomposition of the Correlation Matrix</strong></p> <p>The first step in PCA is to perform an eigen-decomposition of \(R\) to identify the principal directions of variance. By finding the eigenvalues and eigenvectors of \(R\), we can understand the main directions in which the data varies most.</p> <p>For our example, suppose the eigenvalues and their corresponding eigenvectors for \(R\) are as follows:</p> <ul> <li>Eigenvalue \(\lambda_1 = 1.8\), with eigenvector \(q_1 = \begin{bmatrix} 0.7 \\ 0.6 \\ 0.4 \end{bmatrix}\)</li> <li>Eigenvalue \(\lambda_2 = 0.9\), with eigenvector \(q_2 = \begin{bmatrix} -0.5 \\ 0.7 \\ 0.5 \end{bmatrix}\)</li> <li>Eigenvalue \(\lambda_3 = 0.3\), with eigenvector \(q_3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}\)</li> </ul> <p>Each eigenvalue represents the variance explained by its corresponding eigenvector direction. The largest eigenvalue, \(\lambda_1 = 1.8\), tells us that the first principal component explains the most variance in the data—roughly \(\frac{1.8}{3} = 60\%\) of the total variance (since the sum of the eigenvalues is 3 in a three-variable system). The second eigenvalue \(\lambda_2 = 0.9\) accounts for \(30\%\) of the variance, and the third eigenvalue \(\lambda_3 = 0.3\) contributes \(10\%\).</p> <p><strong>Step 2: Interpreting the Principal Components</strong></p> <p>Each eigenvector represents a direction in the original variable space (height, weight, age) along which there is a certain level of variability in the data. Let’s break down what each principal component reveals:</p> <ol> <li> <p><strong>First Principal Component (PC1)</strong>: The first eigenvector \(q_1 = \begin{bmatrix} 0.7 \\ 0.6 \\ 0.4 \end{bmatrix}\) suggests that PC1 is a weighted combination of all three variables, with the largest weights on height and weight. This component captures the common variability between height and weight, reflecting a general “size” factor—individuals with larger heights tend to have larger weights. Since this component explains 60% of the variance, it’s the most informative single direction for summarizing the data.</p> </li> <li> <p><strong>Second Principal Component (PC2)</strong>: The second eigenvector \(q_2 = \begin{bmatrix} -0.5 \\ 0.7 \\ 0.5 \end{bmatrix}\) has significant contributions from all three variables, but with a negative sign for height and positive signs for weight and age. This suggests that PC2 captures a contrast between height and a combined weight-age factor, which might reflect a tendency where, for a given age, people with smaller heights have relatively higher weights.</p> </li> <li> <p><strong>Third Principal Component (PC3)</strong>: The third eigenvector \(q_3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}\) explains only 10% of the total variance. This component primarily reflects variability in age with some influence from height and a negative contribution from weight, capturing a pattern that is less pronounced in the data.</p> </li> </ol> <p><strong>Step 3: Projecting the Data onto the Principal Components</strong></p> <p>With the principal components identified, we can project our original data onto these components to transform it into a new coordinate system defined by \(q_1\), \(q_2\), and \(q_3\). This projection can be computed as:</p> \[\text{Projected Data} = Q^T X\] <p>where \(Q\) is the matrix of eigenvectors:</p> \[Q = \begin{bmatrix} 0.7 &amp; -0.5 &amp; 0.5 \\ 0.6 &amp; 0.7 &amp; -0.2 \\ 0.4 &amp; 0.5 &amp; 0.8 \end{bmatrix}\] <p>For each individual in the dataset, their original measurements in terms of height, weight, and age are transformed into scores along PC1, PC2, and PC3. These scores reflect the new, simplified representation of each individual in terms of the most significant patterns in the data. For example, projecting the original data onto PC1 (the direction of greatest variance) gives a one-dimensional summary of “size” variation across individuals, effectively condensing the information from three variables into a single informative score.</p> <p><strong>Step 4: Reducing Dimensionality</strong></p> <p>Since PC1 and PC2 together account for 90% of the total variance, we might decide to approximate our data using only these two components, discarding PC3, which contributes relatively little. By retaining only PC1 and PC2, we reduce the dimensionality of the dataset from three to two while preserving the bulk of the information.</p> <p>This reduced representation is particularly useful for visualization: each individual can now be represented by a point in a two-dimensional plane defined by PC1 and PC2. The positions of these points reflect the primary structure and relationships in the original data, without the “noise” from minor variations that PC3 captures. Moreover, patterns and clusters within the data often become more apparent in this reduced-dimensional space, revealing insights that might not be obvious in the original three-dimensional view.</p> <script type="text/tikz">
\begin{tikzpicture}

% Darker blue fill for the front Weight-Height square plane
\fill[blue!40, opacity=0.6] (0,0,0) -- (2.8,0,0) -- (2.8,2.8,0) -- (0,2.8,0) -- cycle;

% Lighter blue fill for the Weight-Age side parallelogram
\fill[blue!30, opacity=0.4] (0,0,0) -- (0,0,2.8) -- (0,2.8,2.8) -- (0,2.8,0) -- cycle;

% Lighter blue fill for the Height-Age top parallelogram
\fill[blue!30, opacity=0.4] (0,0,2.8) -- (2.8,0,2.8) -- (2.8,2.8,2.8) -- (0,2.8,2.8) -- cycle;

% Original 3D coordinate system (Height, Weight, Age)
\draw[->, thick] (0,0,0) -- (2.8,0,0) node[anchor=north east] {Height};
\draw[->, thick] (0,0,0) -- (0,2.8,0) node[anchor=north west] {Weight};
\draw[->, thick] (0,0,0) -- (0,0,2.8) node[anchor=south] {Age};

% Original data points in the 3D coordinate system (blue)
\fill[blue] (1,2,1.5) circle (2pt);
\fill[blue] (1.8,1.2,1.8) circle (2pt);
\fill[blue] (1.3,1.8,1.3) circle (2pt);
\fill[blue] (1.6,1.5,1) circle (2pt);

% Arrow indicating projection to the PC1-PC2 plane, with reduced distance
\draw[->, thick, dashed] (3, 1.8, 1.5) -- (5.8, 1.8, 0) node[midway, above, sloped] {Projection onto PC1-PC2 plane};

% Darker orange fill for the 2D PC1-PC2 plane, with slight shadow effect
\fill[orange!40, opacity=0.5] (6.5,0,0) -- (9,0,0) -- (8.7,2.3,0) -- (6.2,2.3,0) -- cycle;

% 2D PC1-PC2 plane axes
\draw[->, color=orange, thick] (6.5,0,0) -- (9,0,0) node[anchor=north east] {PC1(60\%)};
\draw[->, color=orange, thick] (6.5,0,0) -- (6.2,2.3,0) node[anchor=north west] {PC2(30\%)};

% Projected data points on the PC1-PC2 plane (red)
\fill[red] (7.2,1.8,0) circle (2pt);
\fill[red] (7.9,1,0) circle (2pt);
\fill[red] (7.4,1.5,0) circle (2pt);
\fill[red] (7.8,1.2,0) circle (2pt);

% Dashed lines connecting original points to their projections
\draw[dashed, gray] (1,2,1.5) -- (7.2,1.8,0);
\draw[dashed, gray] (1.8,1.2,1.8) -- (7.9,1,0);
\draw[dashed, gray] (1.3,1.8,1.3) -- (7.4,1.5,0);
\draw[dashed, gray] (1.6,1.5,1) -- (7.8,1.2,0);

\end{tikzpicture}
</script> <p> </p> <p><strong>Summary</strong></p> <p>In this example, PCA has taken a dataset with three interrelated variables and transformed it into a new set of uncorrelated components that reveal the primary patterns of variability. By examining the eigenvalues and eigenvectors of the correlation matrix \(R\), we extracted principal components that provided both a compact and interpretable representation of the data.</p> <hr> <h2 id="discussion">Discussion</h2> <p>In conclusion, this post serves as a natural extension to <a href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">our previous discussion on covariance</a>, diving into the aspects of correlation that were left unexplored. While the covariance matrix lays the groundwork for understanding how variables interact, it lacks the standardization needed for clear, direct comparisons. The correlation matrix fills this gap, distilling complex relationships into a standardized, unit-free format that provides immediate insights into linear dependencies across variables. Each entry in the matrix reflects a pairwise relationship as an angle or alignment.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Enjoy Reading This Article? Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Rethinking-1/">Rethinking an Olefin Oligomerization from Three Years Ago – Chapter 1: Problem Statement and Some Initial Thoughts from Back Then</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Rethinking-0/">Rethinking an Olefin Oligomerization from Three Years Ago – Chapter 0: Preface</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Proofs-for-Some-Properties/">A Sketch of Proofs for Some Properties of Multivariate Gaussian Distribution</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/multivariate_Gaussian_distribution/">Is the Transition from Univariate to Multivariate Gaussian Distribution Linear?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">An Introductory Look at Covariance and the Mean Vector</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Shuhong Dai. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-rethinking-an-olefin-oligomerization-from-three-years-ago-chapter-1-problem-statement-and-some-initial-thoughts-from-back-then",title:"Rethinking an Olefin Oligomerization from Three Years Ago \u2013 Chapter 1: Problem Statement...",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2024/Rethinking-1/"}},{id:"post-rethinking-an-olefin-oligomerization-from-three-years-ago-chapter-0-preface",title:"Rethinking an Olefin Oligomerization from Three Years Ago \u2013 Chapter 0: Preface",description:"Perhaps I assumed, rather simplistically, that a mathematical modeling competition would primarily demand mathematical rigor, with little reliance on actual chemistry knowledge (an assumption that proved largely correct). Or perhaps I dismissed it as a straightforward data analysis exercise...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Rethinking-0/"}},{id:"post-a-sketch-of-proofs-for-some-properties-of-multivariate-gaussian-distribution",title:"A Sketch of Proofs for Some Properties of Multivariate Gaussian Distribution",description:"After the transformation, the random vector $$ mathbf{Y} = Q^T mathbf{X} $$ will have components $$ Y_1, Y_2, dots, Y_k $$, each with variance $$ sigma_i^2 $$, and no covariances between them...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Proofs-for-Some-Properties/"}},{id:"post-is-the-transition-from-univariate-to-multivariate-gaussian-distribution-linear",title:"Is the Transition from Univariate to Multivariate Gaussian Distribution Linear?",description:"Now that we have the foundation in place, let\u2019s shift gears and consider the generalization of the univariate Gaussian to higher dimensions. In the multivariate case, we are no longer dealing with a single random variable, but rather a vector of random variables...",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate_Gaussian_distribution/"}},{id:"post-a-supplementary-discussion-on-correlation-coefficients",title:"A Supplementary Discussion on Correlation Coefficients",description:"Yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Correlation_Coefficients/"}},{id:"post-an-introductory-look-at-covariance-and-the-mean-vector",title:"An Introductory Look at Covariance and the Mean Vector",description:"If the mean vector gives us a sense of location, then the covariance matrix gives us a sense of shape...",section:"Posts",handler:()=>{window.location.href="/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/"}},{id:"post-sampling-smarter-unlocking-the-power-of-latin-hypercube-sampling",title:"Sampling Smarter: Unlocking the Power of Latin Hypercube Sampling",description:"Unlike random sampling, which might leave some regions underrepresented while others get chosen repeatedly...",section:"Posts",handler:()=>{window.location.href="/blog/2024/Latin_Hypercube_Sampling/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-won-the-1st-prize-with-a-designed-four-switch-buck-boost-circuit-in-the-university-electronic-design-competition-sponsored-by-huawei",title:"Won the 1st prize with a designed four-switch buck-boost circuit in the university...",description:"",section:"News"},{id:"news-received-the-first-class-academic-scholarship",title:"Received the First-Class Academic Scholarship.",description:"",section:"News"},{id:"news-won-the-3rd-prize-for-problem-g-quot-non-contact-object-measurement-device-quot-at-the-8th-national-undergraduate-electronics-indesign-contest",title:"Won the 3rd prize for Problem G: &quot;Non-contact Object Measurement Device&quot; at the...",description:"",section:"News"},{id:"news-visited-the-key-laboratory-of-advanced-energy-traction-and-comprehensive-energy-saving-railway-industry-at-swjtu",title:"Visited the Key Laboratory of Advanced Energy Traction and Comprehensive Energy Saving Railway...",description:"",section:"News"},{id:"news-received-the-first-class-academic-scholarship",title:"Received the First-Class Academic Scholarship.",description:"",section:"News"},{id:"news-won-the-3rd-prize-in-problem-b-quot-ethanol-coupling-to-produce-c4-olefins-quot-at-the-30th-china-undergraduate-mathematical-contest-in-modeling",title:"Won the 3rd prize in Problem B: &quot;Ethanol Coupling to Produce C4 Olefins&quot;...",description:"",section:"News"},{id:"news-won-the-1st-prize-in-the-preliminary-national-round-of-the-5th-chinese-education-cup-mathematics-competition",title:"Won the 1st prize in the preliminary national round of the 5th Chinese...",description:"",section:"News"},{id:"news-won-the-2nd-prize-in-the-national-finals-of-the-5th-chinese-education-cup-mathematics-competition",title:"Won the 2nd prize in the national finals of the 5th Chinese Education...",description:"",section:"News"},{id:"news-won-the-1st-prize-at-the-provincial-level-in-the-13th-chinese-mathematics-competitions",title:"Won the 1st prize at the provincial level in the 13th Chinese Mathematics...",description:"",section:"News"},{id:"news-guided-the-quot-dual-car-following-system-quot-project-which-won-the-highest-award-the-quot-ti-cup-quot-in-the-10th-national-undergraduate-electronics-design-contest",title:"Guided the &quot;Dual-Car Following System&quot; project which won the highest award, the &quot;TI...",description:"",section:"News"},{id:"news-obtained-my-bachelor-39-s-degree-in-electrical-engineering",title:"Obtained my Bachelor&#39;s Degree in Electrical Engineering.",description:"",section:"News"},{id:"news-joined-the-distributed-systems-group-at-ncepu-to-pursue-my-master-39-s-degree-in-computer-science",title:"Joined the Distributed Systems Group at NCEPU to pursue my Master&#39;s degree in...",description:"",section:"News"},{id:"news-participated-in-world-robot-conference-2024-in-beijing",title:"Participated in World Robot Conference 2024 in Beijing.",description:"",section:"News"},{id:"news-a-paper-on-data-compression-https-ieeexplore-ieee-org-document-10376424-was-accepted-by-ieee-iot-journal",title:"[A paper on data compression](https://ieeexplore.ieee.org/document/10376424) was accepted by _IEEE IoT Journal_.",description:"",section:"News"},{id:"news-a-paper-on-energy-efficient-computing-https-www-sciencedirect-com-science-article-pii-s0140366423004802-via-3dihub-was-accepted-by-computer-communications",title:"[A paper on energy-efficient computing](https://www.sciencedirect.com/science/article/pii/S0140366423004802?via%3Dihub) was accepted by _Computer Communications_.",description:"",section:"News"},{id:"news-participated-in-microsoft-ai-day-beijing",title:"Participated in Microsoft AI Day Beijing.",description:"",section:"News"},{id:"news-a-paper-on-vehicle-road-cooperation-https-ieeexplore-ieee-org-document-10632106-was-accepted-by-ieee-iot-journal",title:"[A paper on vehicle-road cooperation](https://ieeexplore.ieee.org/document/10632106) was accepted by _IEEE IoT Journal_.",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%64%61%69%73%68%75%68%6F%6E%67%30%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=lwCWmPUAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>