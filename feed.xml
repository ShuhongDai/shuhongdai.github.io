<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shuhongdai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shuhongdai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-09T05:45:19+00:00</updated><id>https://shuhongdai.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Maximal Entropy of the Boltzmann Distribution: A Quantum Perspective</title><link href="https://shuhongdai.github.io/blog/2024/Boltzmann/" rel="alternate" type="text/html" title="Maximal Entropy of the Boltzmann Distribution: A Quantum Perspective"/><published>2024-11-28T00:00:00+00:00</published><updated>2024-11-28T00:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Boltzmann</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Boltzmann/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In statistical physics, the Boltzmann distribution is one of the most fundamental and widely applied concepts. It describes how particles in a system‚Äîwhether gas molecules, atoms, or even subatomic particles‚Äîdistribute themselves among different energy states in thermal equilibrium. This simple-looking equation, however, is far more than just a tool for calculating temperature or pressure; it links the microscopic, quantum behavior of individual particles to the macroscopic thermodynamic properties we observe in everyday life.</p> <p>But how exactly do we derive this distribution? While it‚Äôs commonly presented as a well-known result, the path to its formulation is anything but straightforward. The Boltzmann distribution doesn‚Äôt emerge from any single law of nature, but from a set of assumptions and principles. One of the key principles behind its derivation is the maximum entropy principle, a concept that originates in information theory but has profound implications in statistical mechanics.</p> <blockquote> <p><strong>Information theory must precede probability theory and not be based on it.</strong></p> <p>\(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\) \(~~~~~~~~~~~~~~~~~~~~~~~\)by Andrey Nikolaevich Kolmogorov</p> </blockquote> <p>At its core, the maximum entropy principle suggests that, when constrained by known macroscopic quantities like energy or particle number, the most probable state of a system is the one that maximizes the system‚Äôs entropy. In simpler terms, the most likely distribution of particles is the one that reflects the greatest uncertainty about the system‚Äôs microscopic state, given the constraints we have. This approach not only provides a way to derive the Boltzmann distribution but also unites classical and quantum statistical mechanics.</p> <p>In classical systems, where particles can occupy continuous energy levels, the Boltzmann distribution directly describes how particles are spread across those levels. But in the quantum world, where particles are confined to discrete energy states, the situation becomes more complex. The introduction of the Quantum Canonical Ensemble allows us to extend the classical Boltzmann distribution into the quantum domain, where energy quantization and other quantum effects must be taken into account.</p> <p>In the following sections, we‚Äôll explore how the maximum entropy principle provides the key to deriving the Boltzmann distribution, and how it can be extended to the quantum realm through the Quantum Canonical Ensemble.</p> <blockquote> <p>Nov 28, 2024: Some of the quantum concepts mentioned in this article give me an excuse to shamelessly plug my upcoming paper, <strong>‚ÄúQuantum Reinforcement Learning for ü´£,‚Äù</strong> particularly the preliminary section. It‚Äôs currently undergoing major revision‚Äîstay tuned!</p> </blockquote> <hr/> <h2 id="maximizing-entropy-deriving-the-boltzmann-distribution">Maximizing Entropy: Deriving the Boltzmann Distribution</h2> <p>In the quest to uncover the probability distribution that governs a system in thermodynamic equilibrium, we turn to the principle of maximum entropy. This elegant idea, first formulated in information theory, tells us that the best description of a system‚Äôs state is the one that maximizes uncertainty‚Äîsubject to the constraints we know about the system. In statistical mechanics, those constraints often relate to conserved quantities like total particle number or average energy.</p> <p>Let‚Äôs begin by considering a system with a set of discrete energy levels $ { \epsilon_i }_{i=1}^N $. Our goal is to find the probability distribution $ { p_i } $ that describes the likelihood of the system occupying each energy level $ \epsilon_i $, with the constraint that the total probability sums to one, and the system has a fixed average energy.</p> <h3 id="measuring-uncertainty">Measuring Uncertainty</h3> <p>To quantify this uncertainty, we use Shannon entropy. The entropy of a probability distribution $ { p_i } $ is defined as:</p> \[S = -k_B \sum_{i=1}^N p_i \ln p_i,\] <p>where $ k_B $ is the Boltzmann constant. This formula captures the degree of disorder or uncertainty within the system‚Äîthe more uncertain we are about the system‚Äôs microstate, the higher the entropy.</p> <blockquote> <p>Of course, you can also use other definitions or designed entropies, because the logarithmic function is not determined by the entropy itself, but rather by how we approximate entropy based on the variation properties of different functions according to personalized needs.</p> </blockquote> <p>Our task is to find the distribution $ { p_i } $ that maximizes this entropy, while also satisfying two important constraints:</p> <ol> <li> <p>Normalization: The total probability must sum to one:</p> \[\sum_{i=1}^N p_i = 1.\] </li> <li> <p>Energy constraint: The system‚Äôs average energy must be fixed at some value $ U $:</p> \[\langle E \rangle = \sum_{i=1}^N p_i \epsilon_i = U.\] </li> </ol> <h3 id="the-lagrange-multiplier-method">The Lagrange Multiplier Method</h3> <p>To incorporate these constraints, we use Lagrange multipliers. We want to maximize the entropy $ S $ subject to the above constraints, so we introduce two Lagrange multipliers, $ \alpha $ (for the normalization constraint) and $ \beta $ (for the energy constraint). We now define the Lagrangian functional $ \mathcal{L} $ as:</p> \[\mathcal{L} = -k_B \sum_{i=1}^N p_i \ln p_i - \alpha \left( \sum_{i=1}^N p_i - 1 \right) - \beta \left( \sum_{i=1}^N p_i \epsilon_i - U \right).\] <p>We seek the values of $ p_i $ that make $ \mathcal{L} $ stationary, which we do by differentiating with respect to $ p_i $ and setting the result equal to zero. The partial derivative of $ \mathcal{L} $ with respect to $ p_i $ is:</p> \[\frac{\partial \mathcal{L}}{\partial p_i} = -k_B (1 + \ln p_i) - \alpha - \beta \epsilon_i = 0.\] <p>Solving for $ p_i $, we get:</p> \[\ln p_i = -1 - \frac{\alpha}{k_B} - \frac{\beta}{k_B} \epsilon_i,\] <p>which simplifies to:</p> \[p_i = C e^{-\frac{\beta}{k_B} \epsilon_i},\] <p>where $ C $ is a constant to be determined later. This form suggests that the probability $ p_i $ is exponentially related to the energy $ \epsilon_i $, which is exactly what we expect from a system in equilibrium.</p> <blockquote> <p>For the details and principles of the Lagrange multiplier method, you can refer to any textbook on operations research and optimization.</p> <p>Here, we just used it as a tool for solving the problem. <strong>En r√©alit√©, j‚Äôai un v√©ritableement brillant raisonnement pour cette proposition, mais cette marge est bien trop √©troite pour le contenir.</strong> üòé</p> </blockquote> <h3 id="determining-the-constant--c-">Determining the Constant $ C $</h3> <p>To find the constant $ C $, we use the <strong>normalization condition</strong>. Summing over all states, we require that:</p> \[\sum_{i=1}^N p_i = 1.\] <p>Substituting the expression for $ p_i $, we obtain:</p> \[\sum_{i=1}^N C e^{-\frac{\beta}{k_B} \epsilon_i} = 1.\] <p>This implies that $ C $ must be:</p> \[C = \frac{1}{Z},\] <p>where $ Z $ is the partition function defined as:</p> \[Z = \sum_{i=1}^N e^{-\frac{\beta}{k_B} \epsilon_i}.\] <p>Thus, the probability distribution that maximizes entropy, subject to the constraints, is:</p> \[p_i = \frac{e^{-\frac{\beta}{k_B} \epsilon_i}}{Z}.\] <p>This is the Boltzmann distribution, where $ \beta $ is a constant related to the temperature $ T $ by $ \beta = \frac{1}{k_B T} $. This result tells us that the likelihood of the system being in a particular state is exponentially weighted by the energy of that state, with higher-energy states being exponentially less probable than lower-energy states at higher temperatures.</p> <h3 id="physical-interpretation">Physical Interpretation</h3> <p>The Boltzmann distribution not only maximizes the entropy but also satisfies the energy constraint: the average energy of the system $ \langle E \rangle $ is the weighted sum of the energies of the individual states, with the Boltzmann distribution providing the correct weighting. The factor $ \beta $ controls the distribution‚Äôs dependence on energy and temperature, with $ \beta $ decreasing as the temperature increases, meaning that at high temperatures, the system is more likely to occupy higher-energy states.</p> <p>We can also gain deeper insight into the partition function $ Z $. It plays a central role in thermodynamics, encapsulating all the information about the system‚Äôs statistical properties. In fact, the partition function is directly related to the Helmholtz free energy $ F $, which governs the system‚Äôs thermodynamic behavior:</p> \[F = -k_B T \ln Z.\] <p>From this, we can derive other thermodynamic quantities, like entropy $ S $ and internal energy $ U $, by differentiating $ F $ with respect to temperature or other variables.</p> <h3 id="demo">Demo</h3> <p>Now that we‚Äôve mathematically derived the Boltzmann distribution using the maximum entropy principle, let‚Äôs bring that theory to life with a simple Demo. The goal here is to simulate how the distribution behaves at different temperatures and visualize how particles are more likely to occupy higher energy states as temperature increases.</p> <p>We‚Äôll model a system with discrete energy levels $ \epsilon_i $ (let‚Äôs use values from 0 to 5 for simplicity). Our main task is to compute the probabilities $ p_i $ for each energy level using the Boltzmann distribution:</p> \[p_i = \frac{e^{-\frac{\beta}{k_B} \epsilon_i}}{Z},\] <p>where $ \beta = \frac{1}{k_B T} $ and $ Z $ is the partition function:</p> \[Z = \sum_{i=1}^N e^{-\frac{\beta}{k_B} \epsilon_i}.\] <d-code block="" language="python"> import numpy as np import matplotlib.pyplot as plt import seaborn as sns # Set up the energy levels (discrete levels) energy_levels = np.array([0, 1, 2, 3, 4, 5]) # Example energy levels # Function to calculate Boltzmann distribution probabilities def boltzmann_distribution(energy_levels, temperature): beta = 1 / temperature # Œ≤ = 1 / k_B T, assume k_B = 1 Z = np.sum(np.exp(-beta * energy_levels)) # Partition function probabilities = np.exp(-beta * energy_levels) / Z # Boltzmann distribution return probabilities # Set up the temperatures to explore temperatures = [0.5, 1, 2, 5, 10] # Different temperatures to visualize # Set up the plot plt.figure(figsize=(10, 6)) sns.set(style="whitegrid") # Plot the Boltzmann distribution for each temperature for T in temperatures: probabilities = boltzmann_distribution(energy_levels, T) plt.plot(energy_levels, probabilities, label=f"T = {T}", marker='o', linestyle='-', markersize=6) # Customize the plot plt.title('Boltzmann Distribution at Different Temperatures', fontsize=16) plt.xlabel('Energy Level $\epsilon_i$', fontsize=14) plt.ylabel('Probability $p_i$', fontsize=14) plt.legend(title="Temperature (T)", loc='upper right') plt.grid(True) plt.show() </d-code> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-11-28/Boltzmann%20Distribution%20at%20Different%20Temperatures-480.webp 480w,/assets/posts_img/2024-11-28/Boltzmann%20Distribution%20at%20Different%20Temperatures-800.webp 800w,/assets/posts_img/2024-11-28/Boltzmann%20Distribution%20at%20Different%20Temperatures-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-11-28/Boltzmann%20Distribution%20at%20Different%20Temperatures.png" class="img-fluid" width="600" height="400" alt="Boltzmann Distribution at Different Temperatures" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>What you‚Äôll notice is that at low temperatures (e.g., $ T = 0.5 $), the distribution is sharply peaked at the lowest energy level‚Äîmost particles occupy the ground state. As the temperature increases (e.g., $ T = 10 $), the distribution spreads out, and the probability of occupying higher energy states increases. This is the Boltzmann distribution in action: at higher temperatures, particles are more likely to be found in higher energy states because there is less exponential suppression of those states.</p> <hr/> <h2 id="quantum-canonical-ensemble-derivation-of-the-boltzmann-distribution">Quantum Canonical Ensemble Derivation of the Boltzmann Distribution</h2> <p>After deriving the classical Boltzmann distribution using the maximum entropy principle, we‚Äôve arrived at a powerful tool for understanding systems in thermal equilibrium. But here‚Äôs where things get really interesting: what happens when we shift our attention from classical to quantum systems? Classical statistics, as we know, assumes that energy levels are continuous, particles behave as distinguishable entities, and the laws of thermodynamics apply seamlessly. However, quantum systems operate under different rules, with discrete energy levels, indistinguishable particles, and, crucially, quantum mechanical phenomena like superposition and entanglement.</p> <h3 id="from-classical-to-quantum">From Classical to Quantum</h3> <p>In classical systems, the energy levels are treated as continuous, and particles are distinguishable. For example, in classical thermodynamics, the system‚Äôs behavior is described by average quantities (like pressure, volume, and temperature) that emerge from statistical averages over all possible microstates. But when dealing with quantum systems, things get much more intricate.</p> <p>Quantum systems are governed by a Hilbert space, where the energy states are discrete, and the particles are indistinguishable. This means the states of the system are no longer described by simple averages but by a density operator. Furthermore, quantum systems obey the Pauli exclusion principle (for fermions) or Bose-Einstein statistics (for bosons), which impose additional constraints on how particles can occupy different energy states.</p> <p>In quantum statistical mechanics, we must therefore account for these discrete states and their probabilistic occupation. Rather than dealing with a smooth distribution over energy states, we deal with specific quantum states, each of which has a corresponding energy eigenvalue $ E_n $. These eigenstates $ \vert n \mathpunct{\rangle} $ can be occupied with different probabilities depending on the temperature, which leads us to the concept of the partition function and the density matrix.</p> <h3 id="the-quantum-partition-function">The Quantum Partition Function</h3> <p>In quantum statistical mechanics, the partition function plays a similar role to what it did in the classical derivation of the Boltzmann distribution. However, in quantum systems, we express this as a sum over all possible quantum states, each weighted by a factor related to its energy:</p> \[Z = \sum_n e^{- \beta E_n},\] <p>where $ Z $ is the quantum partition function, $ \beta = \frac{1}{k_B T} $ is the inverse temperature, and $ E_n $ are the energy eigenvalues of the system. This sum takes into account all possible quantum states, where the probability of being in a particular state $ \vert n \mathpunct{\rangle} $ depends on its energy $ E_n $.</p> <p>The density operator $ \hat{\rho} $, which describes the quantum state of the system, is given by:</p> \[\hat{\rho} = \frac{1}{Z} e^{-\beta \hat{H}},\] <p>where $ \hat{H} $ is the Hamiltonian operator (which encapsulates the total energy of the system). The density operator governs the probability distribution of the system‚Äôs quantum states, and we can calculate the probability $ P_n $ of finding the system in state $ \vert n \mathpunct{\rangle} $ as:</p> \[P_n = \langle n | \hat{\rho} | n \rangle = \frac{e^{- \beta E_n}}{Z}.\] <h3 id="deriving-the-quantum-boltzmann-distribution">Deriving the Quantum Boltzmann Distribution</h3> <p>Now that we have the quantum density matrix $ \hat{\rho} $, the next step is to derive the quantum version of the Boltzmann distribution. For a system in thermal equilibrium, the probability of the system occupying a particular quantum state $ |n \mathpunct{\rangle}$ with energy $ E_n $ is given by:</p> \[P_n = \frac{e^{- \beta E_n}}{Z}.\] <p>This is the same formula as the classical Boltzmann distribution, but with one crucial difference: it arises in a quantum context, where the states $ \vert n \mathpunct{\rangle} $ are discrete and represent the quantum states of the system. Importantly, the partition function $ Z $ accounts for the normalization of the system over all possible states, ensuring that the total probability sums to 1.</p> <p>In quantum systems, the partition function $ Z $ is not just a mathematical convenience‚Äîit‚Äôs a central thermodynamic quantity. It encapsulates all the thermal information of the system, allowing us to derive quantities such as the average energy, entropy, and even the free energy. The internal energy $ U $, for example, is given by:</p> \[U = \langle \hat{H} \rangle = \frac{1}{Z} \sum_n E_n e^{-\beta E_n}.\] <p>This quantity, $ U $, provides the average energy of the system, weighted by the Boltzmann factor $ e^{-\beta E_n} $. Similarly, the entropy $ S $ of the system can be derived from the density matrix, and is given by:</p> \[S = -k_B \sum_n P_n \ln P_n = k_B \left( \ln Z + \beta \langle E \rangle \right).\] <h3 id="recovering-the-classical-limit">Recovering the Classical Limit</h3> <p>At this point, we‚Äôve arrived at the <strong>quantum Boltzmann distribution</strong>:</p> \[P_n = \frac{e^{- \beta E_n}}{Z}.\] <p>But how do we connect this result to the classical case? The answer lies in the high-temperature limit. As temperature $T$ increases, the quantum system behaves more and more like a classical system. Specifically, in the classical limit (when $ \beta E_n \ll 1 $), the quantum energy levels become very close to each other, and the partition function can be approximated by an integral over continuous energy states rather than a sum over discrete states. In this limit, the quantum Boltzmann distribution approaches the classical result:</p> \[P_n \approx \frac{e^{-\beta E_n}}{Z_{\text{classical}}}.\] <p>This recovery of the classical Boltzmann distribution from the quantum framework illustrates the seamless transition from quantum to classical statistics as the temperature increases and the system becomes large.</p> <h3 id="demo-1">Demo</h3> <p>Now that we‚Äôve got a solid understanding of how the quantum Boltzmann distribution works, let‚Äôs see it in action. We‚Äôll simulate a system of particles that can occupy discrete energy levels, and use Python to calculate and plot the probability of each energy state at various temperatures.</p> <p>In quantum mechanics, things get interesting because energy levels are discrete. This means particles in a system can only occupy specific energy states. The Boltzmann distribution in this context gives us the probability of a system being in a particular state, and it‚Äôs influenced by temperature. The cooler the system, the more likely it is to find particles in the lower energy states. As the temperature rises, particles are more likely to occupy higher energy states.</p> <p>We‚Äôll work with a simple system of five energy levels, ranging from 0 to 4 (in arbitrary units), and observe how the distribution of particles shifts as we change the temperature.</p> <p>In the code, we define the energy levels and compute the Boltzmann distribution for different temperatures. The partition function $ Z $ normalizes the distribution, ensuring that the total probability across all states sums to 1. At each temperature, we calculate the probability of the system being in each energy state and plot it.</p> <d-code block="" language="python"> import numpy as np import matplotlib.pyplot as plt # Constants k_B = 1 # Boltzmann constant (arbitrary units) temperatures = [1, 5, 10, 20] # Temperature values (arbitrary units) energy_levels = np.array([0, 1, 2, 3, 4]) # Energy levels (arbitrary units) # Function to compute the quantum Boltzmann distribution def quantum_boltzmann_distribution(E, T): beta = 1 / (k_B * T) Z = np.sum(np.exp(-beta * E)) # Partition function P = np.exp(-beta * E) / Z # Quantum Boltzmann distribution return P, Z # Create the plot fig, ax = plt.subplots(figsize=(8, 6)) # Plot the probability distributions for different temperatures for T in temperatures: P, Z = quantum_boltzmann_distribution(energy_levels, T) ax.plot(energy_levels, P, label=f'T = {T} units (Z = {Z:.2f})') # Formatting the plot ax.set_xlabel('Energy Levels (E)', fontsize=14) ax.set_ylabel('Probability (P)', fontsize=14) ax.set_title('Quantum Boltzmann Distribution at Different Temperatures', fontsize=16) ax.legend(title="Temperature (T)") # Display the plot plt.grid(True) plt.tight_layout() plt.show() </d-code> <p>At low temperatures, the particles prefer the lower energy states, so you‚Äôll see a sharp peak at $ E = 0 $. As the temperature increases, the distribution flattens out, meaning particles are more evenly spread across the available energy states. At high temperatures, the system behaves almost classically, with probabilities becoming more uniform across the states.</p> <p>If you push the temperature high enough, the quantum effects start to blur out. The system behaves less quantum-mechanically and more classically‚Äîparticles are equally likely to be found in any state. This is where quantum and classical statistics start to converge, and the Boltzmann distribution recovers its classical form.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-11-28/Quantum%20Boltzmann%20Distribution%20at%20Different%20Temperatures'-480.webp 480w,/assets/posts_img/2024-11-28/Quantum%20Boltzmann%20Distribution%20at%20Different%20Temperatures'-800.webp 800w,/assets/posts_img/2024-11-28/Quantum%20Boltzmann%20Distribution%20at%20Different%20Temperatures'-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-11-28/Quantum%20Boltzmann%20Distribution%20at%20Different%20Temperatures'.png" class="img-fluid" width="600" height="400" alt="Quantum Boltzmann Distribution at Different Temperatures" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Along the way, we explored how the seemingly simple concept of entropy can elegantly extend from classical to quantum systems, providing us with the tools to model everything from gas particles to quantum states in thermal equilibrium.</p> <p>Of course, I‚Äôll be the first to admit that this is far from a perfect or complete explanation. Some of the math might feel a bit rushed, and the logic may not always be as sharp as it should be. Frankly, I‚Äôm still wrapping my head around some of these ideas myself. So, if you‚Äôre an expert (or even if you‚Äôre not), feel free to call me out‚ÄîI‚Äôm sure there are plenty of rough edges that need smoothing over. This is a work in progress, not the final word. üåé</p>]]></content><author><name>Shuhong Dai</name></author><category term="Bits and Pieces"/><category term="Quantum"/><category term="Physics"/><category term="Statistics"/><summary type="html"><![CDATA[Specifically, in the classical limit (when Œ≤En << 1), the quantum energy levels become very close to each other, and the partition function can be approximated by an integral over continuous energy states rather than a sum over discrete states. In this limit, the quantum Boltzmann distribution approaches the classical result...]]></summary></entry><entry><title type="html">A Sketch of Proofs for Some Properties of Multivariate Gaussian Distribution</title><link href="https://shuhongdai.github.io/blog/2024/Proofs-for-Some-Properties/" rel="alternate" type="text/html" title="A Sketch of Proofs for Some Properties of Multivariate Gaussian Distribution"/><published>2024-07-09T00:00:00+00:00</published><updated>2024-07-09T00:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Proofs-for-Some-Properties</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Proofs-for-Some-Properties/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In the <a href="https://shuhongdai.github.io/blog/2024/multivariate_Gaussian_distribution/">previous blog post</a>, we discussed the extension from univariate to multivariate Gaussian distributions and examined the transformation in their functional form. Moving from a single variable to an N-dimensional space introduces a rich structure where properties like independence, marginal distributions, and conditional behaviors play a crucial role in understanding the multivariate Gaussian distribution in depth. In this post, we will provide a sketch of proofs for several key properties of multivariate Gaussians. These properties include the independence of Gaussian random vectors, the derivation of marginal and conditional distributions, the invariance of covariance under transformations, and the behavior of Gaussian distributions under linear transformations. Rather than delving into detailed, rigorous proofs, we aim to offer an intuitive and accessible overview, highlighting the underlying mathematical concepts without overwhelming technical detail.</p> <hr/> <h2 id="proving-the-independence-of-gaussian-random-vectors">Proving the Independence of Gaussian Random Vectors</h2> <p>The relationship between the covariance structure of a random vector and its independence properties plays a key role in many machine learning algorithms, signal processing, and other applied fields. It is particularly relevant in Gaussian processes, where we assume that the random vectors representing function values are often independent, under certain conditions. So, let‚Äôs break down the problem mathematically, focusing on proving the independence of the components of a multivariate Gaussian vector. The path to this insight involves linear algebra, specifically understanding the covariance matrix and its diagonalization.</p> <h3 id="covariance-matrix-diagonalization-and-independence">Covariance Matrix Diagonalization and Independence</h3> <p>Consider a random vector \(\mathbf{X} = (X_1, X_2, \dots, X_k)^T\) following a multivariate normal distribution with mean vector \(\mu = (\mu_1, \mu_2, \dots, \mu_k)^T\) and covariance matrix \(\Sigma\).</p> <p>The probability density function (PDF) of this multivariate Gaussian distribution is given by:</p> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right)\] <p>Here, \(\Sigma\) is a \(k \times k\) covariance matrix, where the diagonal elements \(\sigma_i^2\) represent the variances of each component of the vector \(\mathbf{X}\), and the off-diagonal elements \(\sigma_{ij}\) represent the covariances between the components \(X_i\) and \(X_j\).</p> <p>Now, to examine the independence of the components of \(\mathbf{X}\), we need to focus on the structure of the covariance matrix \(\Sigma\). For random variables to be independent, their covariance must be zero. This leads to the crucial fact that if the covariance matrix is diagonal, then the components of \(\mathbf{X}\) must be independent. The question now is: when does this happen?</p> <h3 id="diagonalization-of-covariance-matrix">Diagonalization of Covariance Matrix</h3> <p>We know from linear algebra that any symmetric matrix, including the covariance matrix \(\Sigma\), can be diagonalized by an orthogonal transformation. This means that there exists an orthogonal matrix \(Q\) (i.e., \(Q^T Q = I\)) such that:</p> \[Q^T \Sigma Q = D\] <p>where \(D\) is a diagonal matrix. Each diagonal element of \(D\), say \(\sigma_i^2\), corresponds to the variance of the transformed variables, which are linear combinations of the original components of \(\mathbf{X}\).</p> <p>To unpack this geometrically: the matrix \(Q\) represents a rotation (or possibly reflection) of the coordinate axes in the space of the random vector \(\mathbf{X}\). After the transformation, the random vector \(\mathbf{Y} = Q^T \mathbf{X}\) will have components \(Y_1, Y_2, \dots, Y_k\), each with variance \(\sigma_i^2\), and no covariances between them. In other words, these transformed components are uncorrelated.</p> <p>But here‚Äôs the critical point: uncorrelated components in a multivariate normal distribution are independent. This result holds because the multivariate normal distribution has a special property: if its components are uncorrelated, they must also be independent. This follows from the fact that the joint distribution factorizes when the covariance matrix is diagonal.</p> <h3 id="formal-proof-of-independence">Formal Proof of Independence</h3> <p>Let‚Äôs now formalize this with a rigorous proof. Given that the covariance matrix \(\Sigma\) is diagonal, it can be written as:</p> \[\Sigma = \text{diag}(\sigma_1^2, \sigma_2^2, \dots, \sigma_k^2)\] <p>The off-diagonal elements of \(\Sigma\) are zero, indicating that the components of \(\mathbf{X}\) are uncorrelated.</p> <p>For two random variables \(X_i\) and \(X_j\), their covariance is defined as:</p> \[\text{Cov}(X_i, X_j) = \mathbb{E}[(X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])]\] <p>If the covariance matrix \(\Sigma\) is diagonal, then:</p> \[\text{Cov}(X_i, X_j) = 0 \quad \text{for} \quad i \neq j\] <p>This implies that \(X_i\) and \(X_j\) are uncorrelated. Since we are dealing with a multivariate normal distribution, this uncorrelation implies that \(X_i\) and \(X_j\) are independent.</p> <p>Thus, if the covariance matrix \(\Sigma\) is diagonal, the components of \(\mathbf{X}\) are independent, and we can conclude:</p> \[X_1, X_2, \dots, X_k \quad \text{are independent}.\] <h3 id="geometrical-intuition">Geometrical Intuition</h3> <p>To gain some geometrical intuition, think of a random vector \(\mathbf{X}\) in a high-dimensional space. The covariance matrix \(\Sigma\) captures how the components of \(\mathbf{X}\) vary together. If the covariance matrix is diagonal, this indicates that the components of \(\mathbf{X}\) do not ‚Äúmix‚Äù with one another‚Äîthey vary independently along different directions of the space.</p> <p>When we diagonalize the covariance matrix, we essentially rotate the space such that the axes align with the principal directions of variation. Along each axis, the corresponding component of \(\mathbf{X}\) varies independently of the others. This geometric interpretation aligns with the algebraic fact that the components are independent when the covariance matrix is diagonal.</p> <h3 id="demo">Demo</h3> <p>In this section, we explore how the covariance matrix influences the distribution of multivariate Gaussian random variables. Specifically, we‚Äôll visualize the difference between correlated and independent random variables by examining two different covariance matrices: one that introduces correlation between the components and one that enforces independence.</p> <p>We will work with a bivariate Gaussian distribution, where the random vector \(\mathbf{x} = [X_1, X_2]^T\) has a mean vector \(\mu = [0, 0]^T\). The covariance matrix determines the shape and orientation of the distribution in the feature space.</p> <ol> <li> <p><strong>Non-diagonal Covariance Matrix (Correlated Case)</strong>: A covariance matrix with off-diagonal elements (non-zero values) implies that the two components \(X_1\) and \(X_2\) are correlated. For example, a covariance matrix like:</p> \[\Sigma = \begin{bmatrix} 1 &amp; 0.8 \\ 0.8 &amp; 1 \end{bmatrix}\] <p>indicates a positive correlation between \(X_1\) and \(X_2\). When plotted, this produces an elliptical distribution, where the data points are spread in a specific direction, showing a linear relationship between the two variables.</p> </li> <li> <p><strong>Diagonal Covariance Matrix (Independent Case)</strong>: A diagonal covariance matrix implies that the two variables are independent. For example:</p> \[\Sigma = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\] <p>In this case, there is no correlation between \(X_1\) and \(X_2\), and the distribution is circular. The points are spread equally in all directions, showing no linear dependence between the two variables.</p> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Set plotting style
</span><span class="n">sns</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="sh">"</span><span class="s">white</span><span class="sh">"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="sh">"</span><span class="s">muted</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Seed for reproducibility
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Define mean and covariance matrices
</span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Covariance Matrix 1 (Non-diagonal, correlated)
</span><span class="n">cov_1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>

<span class="c1"># Covariance Matrix 2 (Diagonal, independent)
</span><span class="n">cov_2</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>

<span class="c1"># Generate random samples
</span><span class="n">data_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov_1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">data_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov_2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Plotting the samples
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

<span class="c1"># Plotting the non-diagonal covariance samples
</span><span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Non-diagonal Covariance Matrix (Correlated)</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plotting the diagonal covariance samples
</span><span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data_2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">data_2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Diagonal Covariance Matrix (Independent)</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Visualizing%20the%20Impact%20of%20Covariance%20Matrices%20on%20the%20Distribution%20of%20Bivariate%20Gaussian%20Random%20Variables-480.webp 480w,/assets/posts_img/2024-07-09/Visualizing%20the%20Impact%20of%20Covariance%20Matrices%20on%20the%20Distribution%20of%20Bivariate%20Gaussian%20Random%20Variables-800.webp 800w,/assets/posts_img/2024-07-09/Visualizing%20the%20Impact%20of%20Covariance%20Matrices%20on%20the%20Distribution%20of%20Bivariate%20Gaussian%20Random%20Variables-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-07-09/Visualizing%20the%20Impact%20of%20Covariance%20Matrices%20on%20the%20Distribution%20of%20Bivariate%20Gaussian%20Random%20Variables.png" class="img-fluid" width="700" height="480" alt="Visualizing the Impact of Covariance Matrices on the Distribution of Bivariate Gaussian Random Variables" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li> <p><strong>Non-diagonal Covariance Matrix (Correlated Case)</strong>: In the first plot (left), where the covariance matrix contains off-diagonal values (0.8), the points form an elongated ellipse. This shows that the two variables \(X_1\) and \(X_2\) are positively correlated. The data points are not spread equally in all directions but rather along the principal axes of the ellipse, which reflects the dependency between the variables.</p> </li> <li> <p><strong>Diagonal Covariance Matrix (Independent Case)</strong>: In the second plot (right), the covariance matrix is diagonal, indicating that \(X_1\) and \(X_2\) are independent. The points form a circular distribution, with no discernible directionality. This indicates that the changes in \(X_1\) do not depend on \(X_2\), and vice versa. The variables are independent, which is precisely what the diagonal covariance structure represents.</p> </li> </ol> <hr/> <h2 id="deriving-marginal-and-conditional-distributions-of-multivariate-gaussians">Deriving Marginal and Conditional Distributions of Multivariate Gaussians</h2> <p>The fact that while the marginal distributions of a multivariate Gaussian are always Gaussian, the reverse inference is not true.</p> <h3 id="deriving-the-marginal-distribution">Deriving the Marginal Distribution</h3> <p>In probability theory, the marginal distribution is the distribution of a subset of variables, ignoring the others. Mathematically, for a random vector \(\mathbf{X} = [X_1, X_2, \dots, X_k]^T\) following a multivariate Gaussian distribution with mean vector \(\mu = [\mu_1, \mu_2, \dots, \mu_k]^T\) and covariance matrix \(\Sigma\), the marginal distribution of a subset of the components of \(\mathbf{X}\) is still a Gaussian distribution.</p> <p>Suppose we have a partition of the vector \(\mathbf{X}\) into two disjoint subsets: \(\mathbf{X}_S\) (the subset of interest) and \(\mathbf{X}_C\) (the complement of \(\mathbf{X}_S\)).</p> <p>The full vector \(\mathbf{X}\) follows a multivariate normal distribution:</p> \[\mathbf{X} \sim \mathcal{N}(\mu, \Sigma)\] <p>We want to derive the marginal distribution of \(\mathbf{X}_S\), i.e., the distribution of just the subset \(\mathbf{X}_S\) while marginalizing over the components of \(\mathbf{X}_C\). The marginal distribution is obtained by integrating out the components corresponding to \(\mathbf{X}_C\) from the joint distribution.</p> <p>The joint PDF of \(\mathbf{X}\) is given by:</p> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right)\] <p>Now, let‚Äôs partition the mean vector \(\mu\) and the covariance matrix \(\Sigma\) as follows:</p> \[\mu = \begin{bmatrix} \mu_S \\ \mu_C \end{bmatrix}, \quad \Sigma = \begin{bmatrix} \Sigma_{SS} &amp; \Sigma_{SC} \\ \Sigma_{CS} &amp; \Sigma_{CC} \end{bmatrix}\] <p>Where:</p> <ul> <li>\(\mu_S\) is the mean vector for the subset \(\mathbf{X}_S\),</li> <li>\(\mu_C\) is the mean vector for the complement \(\mathbf{X}_C\),</li> <li>\(\Sigma_{SS}\) is the covariance matrix between the components in \(\mathbf{X}_S\),</li> <li>\(\Sigma_{SC}\) and \(\Sigma_{CS}\) are the cross-covariances between \(\mathbf{X}_S\) and \(\mathbf{X}_C\),</li> <li>\(\Sigma_{CC}\) is the covariance matrix of \(\mathbf{X}_C\).</li> </ul> <p>To find the marginal distribution of \(\mathbf{X}_S\), we integrate out the complement \(\mathbf{X}_C\) from the joint PDF. That is we compute the integral:</p> \[f(\mathbf{x}_S) = \int f(\mathbf{x}) \, d\mathbf{x}_C\] <p>Substituting the partitioned PDF:</p> \[f(\mathbf{x}_S) = \int \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x}_S, \mathbf{x}_C - \begin{bmatrix} \mu_S \\ \mu_C \end{bmatrix})^T \begin{bmatrix} \Sigma_{SS} &amp; \Sigma_{SC} \\ \Sigma_{CS} &amp; \Sigma_{CC} \end{bmatrix}^{-1} (\mathbf{x}_S, \mathbf{x}_C - \begin{bmatrix} \mu_S \\ \mu_C \end{bmatrix}) \right) d\mathbf{x}_C\] <p>This integral can be solved, and the result is another Gaussian distribution for \(\mathbf{x}_S\), with mean \(\mu_S\) and covariance matrix \(\Sigma_{SS}\):</p> \[f(\mathbf{x}_S) = \frac{1}{(2\pi)^{|S|/2} |\Sigma_{SS}|^{1/2}} \exp \left( -\frac{1}{2} \mathbf{x}_S^T \Sigma_{SS}^{-1} \mathbf{x}_S \right)\] <p>Thus, the marginal distribution of \(\mathbf{X}_S\) is still Gaussian, as expected.</p> <h3 id="reverse-inference-marginal-distributions-do-not-imply-multivariate-normality">Reverse Inference: Marginal Distributions Do Not Imply Multivariate Normality</h3> <p>While it is clear from the above derivation that the marginal distribution of any subset of a multivariate Gaussian is also Gaussian, there is an interesting and subtle issue when trying to reverse the argument. That is, just because the marginal distributions of a random vector are Gaussian does not necessarily mean the entire vector follows a multivariate Gaussian distribution.</p> <p>Suppose we have a random vector \(\mathbf{X} = [X_1, X_2]^T\), and we know that each component \(X_1\) and \(X_2\) is distributed according to a Gaussian distribution:</p> \[X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2), \quad X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)\] <p>This implies that the marginal distributions of \(X_1\) and \(X_2\) are Gaussian. However, this does not imply that the joint distribution \((X_1, X_2)\) is necessarily Gaussian. To see this, we need to consider a counterexample.</p> <p><strong>Counterexample:</strong> Suppose that we have two random variables \(X_1\) and \(X_2\) which are both Gaussian marginally, but their joint distribution is not Gaussian. This can occur when their joint distribution involves nonlinear relationships or other higher-order dependencies that are not captured by the marginals alone.</p> <p>For instance, consider the following joint distribution:</p> \[f(X_1, X_2) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}X_1^2\right) \cdot \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}X_2^2\right) \cdot (1 + \sin(X_1X_2))\] <p>Even though the marginals of \(X_1\) and \(X_2\) are Gaussian, the joint distribution clearly isn‚Äôt. This demonstrates that knowing the marginal distributions are Gaussian does not provide sufficient information to conclude that the joint distribution is Gaussian.</p> <p>In simpler terms: marginal Gaussianity is a necessary but not sufficient condition for the joint distribution to be Gaussian.</p> <hr/> <h2 id="invariance-of-the-multivariate-gaussian-distribution-under-linear-transformations">Invariance of the Multivariate Gaussian Distribution under Linear Transformations</h2> <p>In this section, we explore a crucial property of the multivariate Gaussian distribution: <strong>its invariance under linear transformations</strong>.</p> <h3 id="the-setup-a-multivariate-gaussian-vector">The Setup: A Multivariate Gaussian Vector</h3> <p>We begin with a random vector \(\mathbf{x}\) that follows a multivariate Gaussian distribution:</p> \[\mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x)\] <p>Here, \(\mu_x\) is the \(k\)-dimensional mean vector \(\mu_x = [\mu_{x1}, \mu_{x2}, \dots, \mu_{xk}]^T\), \(\Sigma_x\) is the \(k \times k\) covariance matrix \(\Sigma_x = \begin{bmatrix} \sigma_{x1}^2 &amp; \text{Cov}(X_1, X_2) &amp; \dots \\ \end{bmatrix}\).</p> <p>The PDF of \(\mathbf{x}\) is:</p> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2} |\Sigma_x|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x) \right)\] <p>We are interested in how the distribution of \(\mathbf{x}\) transforms when we apply a linear transformation to \(\mathbf{x}\).</p> <h3 id="the-linear-transformation">The Linear Transformation</h3> <p>Consider a linear transformation of the vector \(\mathbf{x}\) defined by:</p> \[\mathbf{y} = A\mathbf{x} + \mathbf{b}\] <p>Where \(A\) is a \(m \times k\) matrix representing the linear transformation, and \(\mathbf{b}\) represents a \(m\)-dimensional translation vector. \(\mathbf{y}\) is a new random vector, which we seek to analyze. Our goal is to show that \(\mathbf{y}\) is also Gaussian and derive the new mean and covariance matrix of \(\mathbf{y}\).</p> <p>To do this, we use the fact that the multivariate Gaussian distribution is closed under linear transformations. This means that if \(\mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x)\), then the transformed vector \(\mathbf{y}\) will follow a multivariate Gaussian distribution as well, albeit with a different mean and covariance matrix.</p> <hr/> <h3 id="deriving-the-mean-and-covariance-of-mathbfy">Deriving the Mean and Covariance of \(\mathbf{y}\)</h3> <p><strong>Step 1: The New Mean Vector</strong></p> <p>The mean of \(\mathbf{y}\), denoted \(\mu_y\), can be derived by taking the expectation of \(\mathbf{y} = A\mathbf{x} + \mathbf{b}\):</p> \[\mathbb{E}[\mathbf{y}] = \mathbb{E}[A\mathbf{x} + \mathbf{b}] = A \mathbb{E}[\mathbf{x}] + \mathbf{b}\] <p>Since the expectation of \(\mathbf{x}\) is \(\mu_x\), we have:</p> \[\mu_y = A \mu_x + \mathbf{b}\] <p>Thus, the mean vector of \(\mathbf{y}\) is simply the linear transformation of the mean vector of \(\mathbf{x}\), plus the translation vector \(\mathbf{b}\).</p> <p><strong>Step 2: The New Covariance Matrix</strong></p> <p>Next, we derive the covariance matrix of \(\mathbf{y}\). The covariance matrix \(\Sigma_y\) of \(\mathbf{y}\) is given by the expected value of the outer product of \(\mathbf{y}\) with itself, minus the outer product of the mean of \(\mathbf{y}\) with itself:</p> \[\Sigma_y = \mathbb{E}[(\mathbf{y} - \mu_y)(\mathbf{y} - \mu_y)^T]\] <p>Substituting \(\mathbf{y} = A\mathbf{x} + \mathbf{b}\) into this equation, and noting that \(\mathbb{E}[\mathbf{y}] = \mu_y\), we get:</p> \[\Sigma_y = \mathbb{E}[(A\mathbf{x} + \mathbf{b} - \mu_y)(A\mathbf{x} + \mathbf{b} - \mu_y)^T]\] <p>Since \(\mathbf{b} - \mu_y = 0\) (by definition of \(\mu_y\)), this simplifies to:</p> \[\Sigma_y = \mathbb{E}[(A\mathbf{x})(A\mathbf{x})^T] = A \mathbb{E}[\mathbf{x} \mathbf{x}^T] A^T\] <p>Recall that \(\mathbb{E}[\mathbf{x} \mathbf{x}^T] = \Sigma_x\), the covariance matrix of \(\mathbf{x}\). Therefore:</p> \[\Sigma_y = A \Sigma_x A^T\] <p>This shows that the covariance matrix of the transformed vector \(\mathbf{y}\) is simply the original covariance matrix \(\Sigma_x\), transformed by the matrix \(A\) and its transpose.</p> <hr/> <h3 id="demo-1">Demo</h3> <p>In this experiment, we explore the invariance of multivariate Gaussian distributions under linear transformations. Specifically, we start with a bivariate Gaussian distribution and apply a simple linear transformation, such as rotation, to the data. The transformation is visualized by comparing the original and transformed distributions using scatter plots.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Comparison%20of%20Covariance%20Ellipses%20for%20Original%20and%20Transformed%20Distributions-480.webp 480w,/assets/posts_img/2024-07-09/Comparison%20of%20Covariance%20Ellipses%20for%20Original%20and%20Transformed%20Distributions-800.webp 800w,/assets/posts_img/2024-07-09/Comparison%20of%20Covariance%20Ellipses%20for%20Original%20and%20Transformed%20Distributions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-07-09/Comparison%20of%20Covariance%20Ellipses%20for%20Original%20and%20Transformed%20Distributions.png" class="img-fluid" width="600" height="400" alt="Comparison of Covariance Ellipses for Original and Transformed Distributions" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Additionally, we delve into how the covariance structure of the data changes under linear transformations. The covariance ellipses are plotted for both the original and transformed distributions, showing how the shape and orientation of the ellipses reflect the underlying correlations between the variables. This experiment demonstrates that, while the distribution remains Gaussian, the transformation alters the data‚Äôs spread and direction,</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Comparison%20of%20Original%20and%20Transformed%20Bivariate%20Gaussian%20Distributions-480.webp 480w,/assets/posts_img/2024-07-09/Comparison%20of%20Original%20and%20Transformed%20Bivariate%20Gaussian%20Distributions-800.webp 800w,/assets/posts_img/2024-07-09/Comparison%20of%20Original%20and%20Transformed%20Bivariate%20Gaussian%20Distributions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-07-09/Comparison%20of%20Original%20and%20Transformed%20Bivariate%20Gaussian%20Distributions.png" class="img-fluid" width="600" height="400" alt="Comparison of Original and Transformed Bivariate Gaussian Distributions" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">matplotlib.patches</span> <span class="k">as</span> <span class="n">patches</span>

<span class="c1"># Set random seed for reproducibility
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Define original mean and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Mean vector
</span><span class="n">sigma</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>  <span class="c1"># Covariance matrix with positive correlation
</span>
<span class="c1"># Generate samples from the bivariate Gaussian distribution
</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mi">5000</span><span class="p">).</span><span class="n">T</span>

<span class="c1"># Define a simple rotation matrix (45 degrees)
</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span>  <span class="c1"># Rotation by 45 degrees
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># No translation
</span>
<span class="c1"># Apply the linear transformation
</span><span class="n">xy_transformed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]))</span>  <span class="c1"># Perform the linear transformation
</span><span class="n">xy_transformed</span> <span class="o">=</span> <span class="n">xy_transformed</span> <span class="o">+</span> <span class="n">b</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>  <span class="c1"># Apply the translation vector separately
</span>
<span class="c1"># Plot the original and transformed distributions on the same axes
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Plot the original distribution with transparency
</span><span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Blues</span><span class="sh">"</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Original Distribution</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plot the transformed distribution with transparency
</span><span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xy_transformed</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">xy_transformed</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Oranges</span><span class="sh">"</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Transformed Distribution</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Add titles and labels
</span><span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Comparison of Original and Transformed Bivariate Gaussian Distributions</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X2</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Display legend to differentiate the distributions
</span><span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Function to plot covariance ellipse
</span><span class="k">def</span> <span class="nf">plot_cov_ellipse</span><span class="p">(</span><span class="n">covariance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1"># Calculate eigenvalues and eigenvectors
</span>    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">covariance</span><span class="p">)</span>
    <span class="n">order</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Sort by eigenvalue size
</span>    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">order</span><span class="p">],</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">order</span><span class="p">]</span>
    
    <span class="c1"># Calculate rotation angle
</span>    <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigenvectors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    
    <span class="c1"># Plot the covariance ellipse
</span>    <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span>
    <span class="n">ell</span> <span class="o">=</span> <span class="n">patches</span><span class="p">.</span><span class="nc">Ellipse</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>

<span class="c1"># Plot covariance ellipses for both distributions on the same axes
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Covariance ellipse for original distribution
</span><span class="nf">plot_cov_ellipse</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">skyblue</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Original Samples</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Covariance ellipse for transformed distribution
</span><span class="n">transformed_sigma</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">sigma</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span>
<span class="nf">plot_cov_ellipse</span><span class="p">(</span><span class="n">transformed_sigma</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">xy_transformed</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy_transformed</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Transformed Samples</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Add titles and labels
</span><span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Comparison of Covariance Ellipses for Original and Transformed Distributions</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X2</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Display legend
</span><span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div> <hr/> <h2 id="chi2-distribution-and-the-ellipsoid-theorem">\(\chi^2\) Distribution and the Ellipsoid Theorem</h2> <h3 id="the-chi2-distribution-derivation-from-multivariate-gaussian">The \(\chi^2\) Distribution: Derivation from Multivariate Gaussian</h3> <p>Let‚Äôs begin with the first concept: the \(\chi^2\) distribution. Suppose that \(\mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x)\) is a \(k\)-dimensional Gaussian vector, and we are interested in the quadratic form \(Q\):</p> \[Q = \mathbf{x}^T \Sigma^{-1} \mathbf{x}\] <p>This expression appears frequently in multivariate statistics, particularly when we are testing hypotheses about the mean vector \(\mu_x\). We‚Äôll show that \(Q\) follows a \(\chi^2\) distribution with \(k\) degrees of freedom.</p> <p><strong>Step 1: The Transformation to Standard Normal</strong></p> <p>To analyze \(Q\), we first standardize the vector \(\mathbf{x}\). Let‚Äôs consider the transformation \(\mathbf{z} = \Sigma^{-1/2} (\mathbf{x} - \mu_x)\), where \(\Sigma^{-1/2}\) is the matrix square root of \(\Sigma^{-1}\). Under this transformation, \(\mathbf{z}\) follows the standard multivariate normal distribution:</p> \[\mathbf{z} \sim \mathcal{N}(0, I)\] <p>Here, \(I\) is the \(k \times k\) identity matrix. In this new coordinate system, the quadratic form becomes:</p> \[Q = (\Sigma^{-1/2} (\mathbf{x} - \mu_x))^T (\Sigma^{-1/2} (\mathbf{x} - \mu_x)) = \mathbf{z}^T \mathbf{z}\] <p><strong>Step 2: The \(\chi^2\) Distribution</strong></p> <p>Now, notice that \(\mathbf{z}^T \mathbf{z}\) is the sum of the squares of \(k\) independent standard normal random variables:</p> \[Q = \sum_{i=1}^k z_i^2\] <p>Each \(z_i \sim \mathcal{N}(0,1)\), so \(z_i^2\) follows a \(\chi^2\) distribution with 1 degree of freedom. Thus, the sum of these independent \(\chi^2\) variables, \(Q\), follows a \(\chi^2\) distribution with \(k\) degrees of freedom:</p> \[Q \sim \chi^2_k\] <p>This result tells us that any quadratic form \(\mathbf{x}^T \Sigma^{-1} \mathbf{x}\) in a multivariate Gaussian vector follows a \(\chi^2\) distribution with degrees of freedom equal to the dimensionality of the vector \(\mathbf{x}\).</p> <h3 id="demo-for--the-chi2-distribution">Demo for the \(\chi^2\) Distribution</h3> <p>To test this, we simulate 1,000 samples from a 3-dimensional multivariate Gaussian distribution with a mean vector of zeros and an identity covariance matrix. For each sample, we compute the quadratic form \(Q\), which essentially measures the ‚Äúdistance‚Äù of each sample from the mean in a scaled manner. By plotting the histogram of these values, we can compare it with the theoretical \(\chi^2\) distribution with 3 degrees of freedom.</p> <p>The resulting plot includes the simulated distribution of \(Q\), along with the theoretical \(\chi^2\) distribution curve, cumulative distribution function (CDF), the 95% critical value, and markers for the mean and standard deviation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Chi-Squared%20Distribution%20with%203%20Degrees%20of%20Freedom-480.webp 480w,/assets/posts_img/2024-07-09/Chi-Squared%20Distribution%20with%203%20Degrees%20of%20Freedom-800.webp 800w,/assets/posts_img/2024-07-09/Chi-Squared%20Distribution%20with%203%20Degrees%20of%20Freedom-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-07-09/Chi-Squared%20Distribution%20with%203%20Degrees%20of%20Freedom.png" class="img-fluid" width="700" height="487" alt="Chi-Squared Distribution with 3 Degrees of Freedom" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2</span>

<span class="c1"># Set experiment parameters
</span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Degrees of freedom, as we are using a 3-dimensional Gaussian
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># Mean vector (all zeros)
</span><span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># Covariance matrix (identity matrix for simplicity)
</span>
<span class="c1"># Generate samples from a multivariate normal distribution
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># Set seed for reproducibility
</span><span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">Sigma</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># 1,000 samples
</span>
<span class="c1"># Compute the quadratic form Q = x^T * Sigma_inv * x for each sample
</span><span class="n">Sigma_inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>  <span class="c1"># Inverse of the covariance matrix
</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">samples</span> <span class="o">@</span> <span class="n">Sigma_inv</span><span class="p">)</span> <span class="o">*</span> <span class="n">samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Quadratic form values
</span>
<span class="c1"># Plot the histogram of Q values with kernel density estimation (KDE)
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="sh">"</span><span class="s">density</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">skyblue</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Empirical Distribution</span><span class="sh">"</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1"># Plot the theoretical Chi-Squared PDF
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">Q</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># Range for theoretical curve
</span><span class="n">y</span> <span class="o">=</span> <span class="n">chi2</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># Chi-Squared  PDF with k degrees of freedom
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Chi-squared Distribution (df=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot the Chi-Squared  CDF for additional reference
</span><span class="n">y_cdf</span> <span class="o">=</span> <span class="n">chi2</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_cdf</span><span class="p">,</span> <span class="sh">'</span><span class="s">g--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Chi-squared CDF (df=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Mark the 95% critical value for the Chi-Squared distribution
</span><span class="n">critical_value_95</span> <span class="o">=</span> <span class="n">chi2</span><span class="p">.</span><span class="nf">ppf</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">critical_value_95</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">95% Critical Value (df=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Add text annotation for the 95% critical value
</span><span class="n">plt</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">critical_value_95</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Critical Value = </span><span class="si">{</span><span class="n">critical_value_95</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># Mark the mean and standard deviation lines
</span><span class="n">mean</span> <span class="o">=</span> <span class="n">k</span>  <span class="c1"># Mean of Chi-Squared  with k degrees of freedom
</span><span class="n">std_dev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">k</span><span class="p">)</span>  <span class="c1"># Standard deviation of Chi-Squared  with k degrees of freedom
</span><span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">purple</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean = </span><span class="si">{</span><span class="n">mean</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">mean</span> <span class="o">+</span> <span class="n">std_dev</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">purple</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean + 1 SD</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">mean</span> <span class="o">-</span> <span class="n">std_dev</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">purple</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean - 1 SD</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Beautify and finalize the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Chi-Squared Distribution with </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s"> Degrees of Freedom</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Q Value</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Display the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <hr/> <h3 id="the-ellipsoid-theorem-geometrical-interpretation-of-gaussian-contours">The Ellipsoid Theorem: Geometrical Interpretation of Gaussian Contours</h3> <p>The second concept we explore is the ellipsoid theorem, which describes the shape of the level sets (contours) of a multivariate Gaussian distribution. Specifically, we will prove that the contour lines of a multivariate Gaussian distribution are ellipsoids, and we will derive their geometric properties.</p> <p>Let‚Äôs consider again a random vector \(\mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x)\). The PDF for \(\mathbf{x}\) is given by:</p> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2} |\Sigma_x|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x) \right)\] <p>The contour lines of the Gaussian distribution correspond to the set of points \(\mathbf{x}\) for which the PDF is constant. That is, we want to solve:</p> \[(\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x) = c\] <p>where \(c\) is a constant. This equation represents a set of points that lie on a surface with a constant value of the quadratic form \((\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x)\), which we will show is an ellipsoid.</p> <p><strong>Step 1: Eigenvalue Decomposition of the Covariance Matrix</strong></p> <p>To understand the geometry of this surface, we perform the eigenvalue decomposition of the covariance matrix \(\Sigma_x\):</p> \[\Sigma_x = Q \Lambda Q^T\] <p>where \(Q\) is the orthogonal matrix of eigenvectors of \(\Sigma_x\), and \(\Lambda\) is the diagonal matrix of eigenvalues of \(\Sigma_x\):</p> \[\Lambda = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_k)\] <p>Thus, the quadratic form can be rewritten as:</p> \[(\mathbf{x} - \mu_x)^T \Sigma_x^{-1} (\mathbf{x} - \mu_x) = (\mathbf{z})^T \Lambda^{-1} \mathbf{z} = \sum_{i=1}^k \frac{z_i^2}{\lambda_i}\] <p>where \(\mathbf{z} = Q^T (\mathbf{x} - \mu_x)\) is the transformed vector in the eigenbasis of \(\Sigma_x\), and \(z_i\) are the components of \(\mathbf{z}\). This shows that the level sets of the multivariate Gaussian distribution are ellipsoids, with axes scaled by the square roots of the eigenvalues \(\lambda_i\) of \(\Sigma_x\).</p> <p><strong>Step 2: Geometry of the Ellipsoid</strong></p> <p>In the transformed space, the equation describing the level set becomes:</p> \[\sum_{i=1}^k \frac{z_i^2}{\lambda_i} = c\] <p>This is the equation of an ellipsoid in the \(z\)-coordinates. The shape of this ellipsoid depends on the eigenvalues \(\lambda_i\) of the covariance matrix \(\Sigma_x\). The axes of the ellipsoid are aligned with the eigenvectors of \(\Sigma_x\), and the lengths of the axes are proportional to the square roots of the corresponding eigenvalues.</p> <p>Thus, the geometry of the level sets (or contours) of a multivariate Gaussian distribution is determined by the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors determine the direction of the axes of the ellipsoid, and the eigenvalues determine their lengths (the standard deviations along those axes).</p> <h3 id="demo-for-the-ellipsoid-theorem">Demo for the Ellipsoid Theorem</h3> <p>To illustrate this, we generate samples from three different 2-dimensional Gaussian distributions, each with a unique covariance matrix. Each covariance matrix introduces a different level of correlation between the dimensions, which changes the orientation and shape of the ellipsoidal contours. We visualize these contours using concentric ellipses representing one, two, and three standard deviations from the mean. These ellipses are derived from the eigenvalues and eigenvectors of the covariance matrices, where the eigenvalues define the axis lengths, and the eigenvectors determine the rotation of each ellipse. The final plot overlays scatter plots of each Gaussian sample set to show sample density, and the ellipsoid contours at multiple standard deviations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-07-09/Ellipsoid%20Contours%20for%20Different%20Covariance%20Matrices-480.webp 480w,/assets/posts_img/2024-07-09/Ellipsoid%20Contours%20for%20Different%20Covariance%20Matrices-800.webp 800w,/assets/posts_img/2024-07-09/Ellipsoid%20Contours%20for%20Different%20Covariance%20Matrices-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-07-09/Ellipsoid%20Contours%20for%20Different%20Covariance%20Matrices.png" class="img-fluid" width="700" height="450" alt="Ellipsoid Contours for Different Covariance Matrices" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">from</span> <span class="n">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>

<span class="c1"># Set experiment parameters
</span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Mean vector
</span><span class="n">covariances</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>  <span class="c1"># Covariance matrix 1
</span>    <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>  <span class="c1"># Covariance matrix 2
</span>    <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>  <span class="c1"># Covariance matrix 3
</span><span class="p">]</span>

<span class="c1"># Generate samples for each covariance matrix
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span> <span class="k">for</span> <span class="n">cov</span> <span class="ow">in</span> <span class="n">covariances</span><span class="p">]</span>  <span class="c1"># 500 samples per covariance matrix
</span>
<span class="c1"># Set up plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Define colormaps and colors for each covariance matrix
</span><span class="n">colormaps</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Blues</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Greens</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Reds</span><span class="sh">"</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Plot sample distributions and ellipsoid contours
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">covariances</span><span class="p">)):</span>
    <span class="c1"># Density plot for samples with KDE
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">colormaps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="c1"># Scatter plot for samples with explicit color
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Samples with Covariance </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Draw ellipsoids for 1, 2, and 3 standard deviations
</span>    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>  <span class="c1"># Eigenvalues and eigenvectors for covariance matrix
</span>    <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigenvectors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>  <span class="c1"># Rotation angle in degrees
</span>    <span class="k">for</span> <span class="n">n_std</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># 1, 2, 3 standard deviations
</span>        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span>  <span class="c1"># Ellipse width and height
</span>        <span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span>
                          <span class="n">edgecolor</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>

<span class="c1"># Add title and axis labels
</span><span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Ellipsoid Contours for Different Covariance Matrices</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X1</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X2</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">"</span><span class="s">upper right</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Beautify the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">"</span><span class="s">both</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>In this blog, we introduced the key properties of the multivariate Gaussian distribution, including the independence of random variables, the derivation of marginal and conditional distributions, closure under linear transformations, and its geometric characteristics. These properties form the foundation of probabilistic modeling and serve as building blocks for more complex models.</p> <p>As a natural extension of the multivariate Gaussian distribution, Gaussian Mixture Models (GMMs) combine multiple Gaussian components to flexibly capture multimodal characteristics in data. In the next blog, we will explore the mathematical principles behind GMMs and their applications as generative models.</p>]]></content><author><name></name></author><category term="A Column on Gaussian Processes"/><category term="Statistics"/><category term="Stochastic Processes"/><summary type="html"><![CDATA[When we diagonalize the covariance matrix, we essentially rotate the space such that the axes align with the principal directions of variation...]]></summary></entry><entry><title type="html">Is the Transition from Univariate to Multivariate Gaussian Distribution Linear?</title><link href="https://shuhongdai.github.io/blog/2024/multivariate_Gaussian_distribution/" rel="alternate" type="text/html" title="Is the Transition from Univariate to Multivariate Gaussian Distribution Linear?"/><published>2024-06-24T00:32:10+00:00</published><updated>2024-06-24T00:32:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/multivariate_Gaussian_distribution</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/multivariate_Gaussian_distribution/"><![CDATA[<h2 id="background">Background</h2> <p>In the previous <a href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">Chapter 0</a> and <a href="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/">Chapter 1</a> of <a href="https://shuhongdai.github.io/blog/category/a-column-on-gaussian-processes/">this column</a>, we explored foundational concepts like the covariance matrix and the correlation matrix, which are key building blocks in understanding relationships between multiple variables. Building on that, this chapter introduces the multivariate Gaussian distribution, a central concept that plays a pivotal role in many areas of statistical modeling and machine learning. While this distribution has broad applications, from image processing to finance, it is particularly important in the context of Gaussian processes, which we will explore further in upcoming chapters.</p> <p>The multivariate Gaussian distribution is a natural extension of the univariate normal distribution to higher dimensions, providing a simple yet powerful way to model the relationships between multiple variables. In the context of Gaussian processes, it is used to model the underlying distributions of functions, helping us make predictions about unknown values based on observed data.</p> <hr/> <h2 id="the-multivariate-gaussian-distribution">The Multivariate Gaussian Distribution</h2> <h3 id="univariate-gaussian-distribution-revisiting-the-basics">Univariate Gaussian Distribution: Revisiting the Basics</h3> <p>To set the stage, let‚Äôs recall the form of the one-dimensional Gaussian distribution. The probability density function (PDF) for a random variable \(x\) that follows a normal distribution with mean \(\mu\) and variance \(\sigma^2\) is given by:</p> \[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2\sigma^2} \right)\] <p>At its core, this function describes the probability that a random variable \(x\) will take a particular value, given that it is normally distributed around a mean \(\mu\) with variance \(\sigma^2\). The bell-shaped curve of the Gaussian distribution is symmetric around \(\mu\), with the spread determined by \(\sigma\). The exponential term \(\exp \left( -\frac{(x - \mu)^2}{2 \sigma^2} \right)\) captures how the probability decreases as we move further away from the mean.</p> <p>The factor \(\frac{1}{\sqrt{2\pi \sigma^2}}\) is the normalization constant. To ensure that the total probability across all values of \(x\) is equal to 1, we integrate the PDF across the entire real line:</p> \[\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2 \sigma^2} \right) dx = 1\] <p>This is a standard result, but the key point is that the normalization factor ensures that the area under the curve sums to one, thereby giving us a valid probability distribution.</p> <h3 id="extending-to-multivariate-gaussian-distribution">Extending to Multivariate Gaussian Distribution</h3> <p>Now that we have the foundation in place, let‚Äôs shift gears and consider the generalization of the univariate Gaussian to higher dimensions. In the multivariate case, we are no longer dealing with a single random variable, but rather a vector of random variables, say \(\mathbf{x} = [x_1, x_2, \dots, x_k]^T\). This vector \(\mathbf{x}\) can represent a collection of correlated random variables, and we want to model the joint distribution of these variables.</p> <p>The multivariate Gaussian distribution generalizes the concept of the univariate normal distribution to \(k\)-dimensional space. The PDF for a multivariate Gaussian distribution is expressed as:</p> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right)\] <p>Here, \(\mathbf{x}\) is a \(k\)-dimensional vector representing the random variables, \(\mu\) is a \(k\)-dimensional mean vector, and \(\Sigma\) is a \(k \times k\) covariance matrix that encodes the relationships (correlations and variances) between the variables in \(\mathbf{x}\). The term \((\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu)\) is a quadratic form, measuring the ‚Äúdistance‚Äù of the vector \(\mathbf{x}\) from the mean vector \(\mu\). This distance is weighted by the inverse of the covariance matrix, \(\Sigma^{-1}\), which accounts for the fact that the random variables may not be independent, i.e., they can have correlations with each other. The factor \(\frac{1}{(2\pi)^{k/2} \mid \Sigma \mid^{1/2}}\) is the normalization constant, and it ensures that the total probability across the entire \(k\)-dimensional space integrates to one.</p> <p>The mean vector \(\mu = [\mu_1, \mu_2, \dots, \mu_k]^T\) represents the central location of the distribution in the \(k\)-dimensional space. This vector shifts the distribution from the origin, telling us where the ‚Äúcenter‚Äù of the distribution lies. If all variables \(x_1, x_2, \dots, x_k\) were independent and identically distributed (i.i.d.), then each \(\mu_i\) would be the mean of the respective variable.</p> <p>The covariance matrix \(\Sigma\) is the heart of the multivariate Gaussian. The diagonal elements of \(\Sigma\) represent the variances of the individual variables (i.e., how spread out each variable is), and the off-diagonal elements represent the covariances between pairs of variables. For example, \(\sigma_{ij}\) represents the covariance between \(x_i\) and \(x_j\). If this value is nonzero, it means that \(x_i\) and \(x_j\) are correlated. A covariance of zero indicates that the variables are independent.</p> <p>One important feature of the multivariate Gaussian is that the covariance matrix must be positive semi-definite. This ensures that the quadratic form \((\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu)\) is always non-negative and that the distribution behaves well.</p> <p>The above three paragraphs also serve as a review of <a href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">Chapter 0</a> of this column and will not be repeated here.</p> <h3 id="demo">Demo</h3> <p>We‚Äôll start by plotting the univariate Gaussian distribution (a bell curve), then move to the bivariate Gaussian (a 2D contour plot), and finally extend to the trivariate Gaussian (3D surface plot). By doing so, we can observe how the distribution evolves with increasing dimensionality.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-24/Visualization%20of%20Univariate,%20Bivariate,%20and%20Trivariate%20Gaussian%20Distributions-480.webp 480w,/assets/posts_img/2024-06-24/Visualization%20of%20Univariate,%20Bivariate,%20and%20Trivariate%20Gaussian%20Distributions-800.webp 800w,/assets/posts_img/2024-06-24/Visualization%20of%20Univariate,%20Bivariate,%20and%20Trivariate%20Gaussian%20Distributions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-24/Visualization%20of%20Univariate,%20Bivariate,%20and%20Trivariate%20Gaussian%20Distributions.png" class="img-fluid" width="700" height="500" alt="Visualization of Univariate, Bivariate, and Trivariate Gaussian Distributions" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">from</span> <span class="n">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>

<span class="c1"># --- Univariate Gaussian (1D) ---
</span><span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span>      <span class="c1"># Mean
</span><span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span>   <span class="c1"># Standard deviation
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># Generate x values
</span><span class="n">pdf_1d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># PDF
</span>
<span class="c1"># --- Bivariate Gaussian (2D) ---
</span><span class="n">mu_2d</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Mean vector
</span><span class="n">Sigma_2d</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>  <span class="c1"># Covariance matrix with correlation
</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="p">:.</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="p">:.</span><span class="mi">01</span><span class="p">]</span>  <span class="c1"># Create grid for 2D plot
</span><span class="n">pos_2d</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dstack</span><span class="p">((</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">))</span>  <span class="c1"># Combine x2, y2 for the grid points
</span><span class="n">rv_2d</span> <span class="o">=</span> <span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu_2d</span><span class="p">,</span> <span class="n">Sigma_2d</span><span class="p">)</span>  <span class="c1"># Multivariate normal distribution
</span><span class="n">pdf_2d</span> <span class="o">=</span> <span class="n">rv_2d</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">pos_2d</span><span class="p">)</span>  <span class="c1"># Compute the PDF
</span>
<span class="c1"># --- Trivariate Gaussian (3D) ---
</span><span class="n">mu_3d</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Mean vector for 3D
</span><span class="n">Sigma_3d</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Covariance matrix (diagonal, no correlation)
</span><span class="n">x3</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="n">z3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="p">:.</span><span class="mi">05</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="p">:.</span><span class="mi">05</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="p">:.</span><span class="mi">05</span><span class="p">]</span>  <span class="c1"># Create 3D grid
</span><span class="n">pos_3d</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">x3</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y3</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">z3</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()]).</span><span class="n">T</span>  <span class="c1"># Flatten grid for 3D plotting
</span><span class="n">rv_3d</span> <span class="o">=</span> <span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu_3d</span><span class="p">,</span> <span class="n">Sigma_3d</span><span class="p">)</span>  <span class="c1"># Multivariate normal distribution
</span><span class="n">pdf_3d</span> <span class="o">=</span> <span class="n">rv_3d</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">pos_3d</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">x3</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Compute the PDF for 3D
</span>
<span class="c1"># --- Plotting ---
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># 1D plot
</span><span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf_1d</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="sh">'</span><span class="s">$\mathcal{N}(0, 1)$</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Univariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Probability Density</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># 2D plot
</span><span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">contour</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">pdf_2d</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">Blues</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Bivariate Gaussian Distribution (Correlation)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">x1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">x2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># 3D plot
</span><span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="sh">'</span><span class="s">3d</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">plot_surface</span><span class="p">(</span><span class="n">x3</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y3</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pdf_3d</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">viridis</span><span class="sh">'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Trivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">x1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">x2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_zlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Probability Density</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Show all plots
</span><span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <hr/> <h2 id="deriving-the-normalization-constant">Deriving the Normalization Constant</h2> <p>The most mathematically involved part of deriving the multivariate Gaussian distribution is ensuring that the PDF is normalized correctly. That is, we need to confirm that:</p> \[\int_{\mathbb{R}^k} \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right) d\mathbf{x} = 1\] <p>This normalization can be tricky due to the presence of the covariance matrix \(\Sigma\). The solution lies in diagonalizing the covariance matrix using an eigenvalue decomposition. The covariance matrix \(\Sigma\) can be written as:</p> \[\Sigma = V \Lambda V^T\] <p>where \(V\) is the matrix of eigenvectors and \(\Lambda\) is the diagonal matrix of eigenvalues. By changing variables to the eigenbasis of \(\Sigma\), we transform the quadratic form into a simpler form, and we can compute the necessary integrals.</p> <p>In the new coordinates, the quadratic form becomes:</p> \[(\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) = \sum_{i=1}^k \frac{y_i^2}{\lambda_i}\] <p>where \(\lambda_i\) are the eigenvalues of \(\Sigma\) and \(y_i\) are the transformed variables. Each integral of the form:</p> \[\int_{-\infty}^{\infty} \exp \left( -\frac{y_i^2}{2 \lambda_i} \right) dy_i\] <p>is a standard Gaussian integral, which evaluates to \(\sqrt{2\pi \lambda_i}\). The product of these integrals gives the normalization constant, and when combined with the volume scaling factor \(\mid\Sigma\mid^{1/2}\), we arrive at the final form of the PDF.</p> <p>Thus, the normalization constant becomes:</p> \[\frac{1}{(2\pi)^{k/2} \mid\Sigma\mid^{1/2}}\] <p>This ensures that the total probability is indeed 1, completing the derivation of the multivariate Gaussian distribution.</p>]]></content><author><name></name></author><category term="A Column on Gaussian Processes"/><category term="Statistics"/><category term="Stochastic Processes"/><summary type="html"><![CDATA[Now that we have the foundation in place, let‚Äôs shift gears and consider the generalization of the univariate Gaussian to higher dimensions. In the multivariate case, we are no longer dealing with a single random variable, but rather a vector of random variables...]]></summary></entry><entry><title type="html">A Supplementary Discussion on Correlation Coefficients</title><link href="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/" rel="alternate" type="text/html" title="A Supplementary Discussion on Correlation Coefficients"/><published>2024-06-22T00:32:10+00:00</published><updated>2024-06-22T00:32:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Correlation_Coefficients</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In my <a href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">previous blog post</a>, we examined covariance matrices, looking into their derivation, properties, and the geometric insights they offer for understanding multidimensional data. Covariance matrices provide a solid foundation for understanding how variables interact, yet they have certain limitations‚Äîespecially when we want a clearer measure of the strength of these relationships.</p> <p>This brings us to correlation and the correlation matrix, essential concepts for interpreting the strength of relationships between variables independently of their original units or scales. Given their importance, this follow-up post serves as a brief guide to correlation coefficients and the correlation matrix. Here, we‚Äôll cover the basics of correlation, the structure and derivation of the correlation matrix, and some of its most useful properties and applications.</p> <p>In the previous discussion on covariance, we established a foundational understanding of how variables co-vary, yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales. Here, the correlation coefficient, \(\rho_{X,Y}\), refines this measure by standardizing the relationship between two variables, allowing a comparison of their linear association independent of units.</p> <hr/> <h2 id="the-correlation-coefficient">The Correlation Coefficient</h2> <p>In the previous discussion on covariance, we established a foundational understanding of how variables co-vary, yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales. Here, the correlation coefficient, \(\rho_{X,Y}\), refines this measure by standardizing the relationship between two variables, allowing a comparison of their linear association independent of units.</p> <h3 id="definition">Definition</h3> <p>The correlation coefficient \(\rho_{X,Y}\) between two random variables \(X\) and \(Y\) is formally defined as:</p> \[\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\] <p>where \(\text{Cov}(X, Y)\) represents the covariance between \(X\) and \(Y\), \(\sigma_X\) and \(\sigma_Y\) are the standard deviations of \(X\) and \(Y\), respectively.</p> <p>This formula can be viewed as a ‚Äúnormalized‚Äù covariance, essentially adjusting the relationship between \(X\) and \(Y\) by their individual dispersions, or spreads. This normalization is the key: it removes the influence of scale, making \(\rho_{X,Y}\) a dimensionless quantity that ranges from -1 to 1.</p> <h3 id="derivation-from-covariance">Derivation from Covariance</h3> <p>To delve deeper, consider the covariance definition between two random variables \(X\) and \(Y\):</p> \[\text{Cov}(X, Y) = E[(X - \mu_X)(Y - \mu_Y)]\] <p>where \(\mu_X = E[X]\) and \(\mu_Y = E[Y]\) are the expected values (means) of \(X\) and \(Y\). Covariance, while useful, retains the units of \(X\) and \(Y\), which complicates comparisons across variables with differing units or scales.</p> <p>The correlation coefficient refines this by dividing covariance by the product of the standard deviations of \(X\) and \(Y\):</p> \[\sigma_X = \sqrt{E[(X - \mu_X)^2]}, \quad \sigma_Y = \sqrt{E[(Y - \mu_Y)^2]}\] <p>Thus, correlation can be expressed as:</p> \[\rho_{X,Y} = \frac{E[(X - \mu_X)(Y - \mu_Y)]}{\sigma_X \sigma_Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\] <p>This reformulation gives us a measure of association on a standardized scale. Specifically, if \(\rho_{X,Y} = 1\), there exists a perfect positive linear relationship between \(X\) and \(Y\): as \(X\) increases, \(Y\) increases proportionally. If \(\rho_{X,Y} = -1\), the variables exhibit perfect negative linear correlation, where \(Y\) decreases as \(X\) increases. If \(\rho_{X,Y} = 0\), there is no linear relationship between \(X\) and \(Y\).</p> <h3 id="other-properties">Other Properties</h3> <p>One of the most useful features of \(\rho_{X,Y}\) is its unit invariance. Unlike covariance, which scales with the units of \(X\) and \(Y\), correlation removes these effects by normalizing with standard deviations, making \(\rho_{X,Y}\) dimensionless. This quality enables comparisons across variables with different units or scales, ensuring that the strength of relationships is evaluated consistently, regardless of measurement. Furthermore, the symmetry of correlation is another noteworthy property: \(\rho_{X,Y} = \rho_{Y,X}\). This symmetry arises naturally from the covariance term and reinforces that correlation measures a mutual relationship between variables, independent of which one is considered first. These properties, together, establish the correlation coefficient as a powerful, standardized metric for interpreting linear dependencies across diverse contexts</p> <hr/> <h2 id="the-correlation-matrix">The Correlation Matrix</h2> <p>When moving from a single pair of variables to multidimensional data, we naturally extend the concept of correlation to capture all pairwise relationships at once. This is where the correlation matrix \(R\) comes in, giving us a compact summary of the linear associations across an entire dataset. For a random vector \(X = [X_1, X_2, \dots, X_n]^T\), the correlation matrix \(R\) is an \(n \times n\) matrix, where each element \(R_{ij}\) represents the correlation coefficient between \(X_i\) and \(X_j\):</p> \[R_{ij} = \rho_{X_i, X_j} = \frac{\text{Cov}(X_i, X_j)}{\sigma_{X_i} \sigma_{X_j}}\] <h3 id="definition-and-structure">Definition and Structure</h3> <p>In other words, if \(X\) is a random vector with a covariance matrix \(\Sigma\), then \(R\) is constructed element-by-element as:</p> \[R = \begin{bmatrix} \rho_{X_1, X_1} &amp; \rho_{X_1, X_2} &amp; \dots &amp; \rho_{X_1, X_n} \\ \rho_{X_2, X_1} &amp; \rho_{X_2, X_2} &amp; \dots &amp; \rho_{X_2, X_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \rho_{X_n, X_1} &amp; \rho_{X_n, X_2} &amp; \dots &amp; \rho_{X_n, X_n} \end{bmatrix}.\] <p>Each diagonal entry \(R_{ii}\) equals 1, since \(\rho_{X_i, X_i} = \frac{\text{Cov}(X_i, X_i)}{\sigma_{X_i}^2} = 1\). This makes sense because a variable is perfectly correlated with itself. The off-diagonal elements \(R_{ij}\) (for \(i \neq j\)) give us the correlation between different variables, capturing their linear relationships in a standardized form that‚Äôs easy to interpret across the entire matrix.</p> <h3 id="relationship-to-the-covariance-matrix">Relationship to the Covariance Matrix</h3> <p>The correlation matrix \(R\) is directly derived from the covariance matrix \(\Sigma\) by a process of standardization. The idea here is to convert the units and scale-dependent covariances into unit-free, comparable correlation values. To achieve this, we divide each covariance by the product of the standard deviations of the relevant variables, so we arrive at the expression:</p> \[R = D^{-1} \Sigma D^{-1}\] <p>where \(D\) is the diagonal matrix of standard deviations, given by:</p> \[D = \text{diag}(\sigma_{X_1}, \sigma_{X_2}, \dots, \sigma_{X_n}).\] <p>Expanding this calculation, let‚Äôs see how each entry in \(R\) is computed in terms of \(\Sigma\) and \(D\):</p> <ol> <li>Start with the covariance matrix element \(\Sigma_{ij} = \text{Cov}(X_i, X_j)\).</li> <li>Divide \(\Sigma_{ij}\) by \(\sigma_{X_i} \sigma_{X_j}\), where \(\sigma_{X_i} = \sqrt{\Sigma_{ii}}\) and \(\sigma_{X_j} = \sqrt{\Sigma_{jj}}\).</li> <li> <p>This yields each element in \(R\) as:</p> \[R_{ij} = \frac{\Sigma_{ij}}{\sigma_{X_i} \sigma_{X_j}}.\] </li> </ol> <p>In matrix form, we achieve this by pre-multiplying and post-multiplying \(\Sigma\) with \(D^{-1}\), transforming the raw covariance entries into standardized, unitless correlation coefficients. This transformation is crucial when comparing variables measured on different scales, as it removes any units, letting us focus purely on the strength of the relationships.</p> <h3 id="some-properties">Some Properties</h3> <p>The correlation matrix \(R\) possesses a few key properties that make it an elegant and powerful tool for analyzing multidimensional data. First, it is symmetric by nature, since \(\rho_{X_i, X_j} = \rho_{X_j, X_i}\) for any pair of variables \(X_i\) and \(X_j\). This symmetry ensures that each pairwise correlation is mutual and gives the matrix a balanced, mirror-like structure around its diagonal. This diagonal, in turn, consists entirely of ones, as each variable is perfectly correlated with itself‚Äîa subtle reminder that correlation is inherently a self-consistent measure.</p> <p>In addition to its symmetry, \(R\) is positive semi-definite, meaning that for any vector \(z\), the quadratic form \(z^T R z\) is non-negative. This positive semi-definiteness implies that all eigenvalues of \(R\) are non-negative, which is significant because it confirms that \(R\) has a stable variance structure. This property becomes particularly valuable in applications like principal component analysis (PCA), where the correlation matrix‚Äôs eigenvalues reflect the spread of the data along different directions.</p> <p>Furthermore, every element in \(R\) falls within the interval \([-1, 1]\), a direct result of the correlation coefficient‚Äôs own bounded nature. This bounded range ensures that each entry in \(R\) is a pure, unitless indicator of linear association strength. Regardless of the scale or units of the original variables, the values in \(R\) give a consistent, standardized view of how variables align with each other.</p> <hr/> <h2 id="the-geometric-meaning">The Geometric Meaning</h2> <h3 id="geometric-interpretation-angles-and-alignments">Geometric Interpretation: Angles and Alignments</h3> <p>Consider two random variables \(X_i\) and \(X_j\), each represented as vectors in an \(n\)-dimensional data space. The correlation coefficient \(\rho_{X_i, X_j}\) between \(X_i\) and \(X_j\) can be understood as the cosine of the angle \(\theta\) between these two vectors. Formally, this relationship is given by:</p> \[\rho_{X_i, X_j} = \cos \theta_{ij}\] <p>where \(\theta_{ij}\) is the angle between the vectors corresponding to \(X_i\) and \(X_j\). When \(\rho_{X_i, X_j} = 1\), the vectors point in the same direction (\(\theta = 0^\circ\)), indicating a perfect positive linear relationship. Conversely, if \(\rho_{X_i, X_j} = -1\), the vectors point in opposite directions (\(\theta = 180^\circ\)), representing a perfect negative linear relationship. A correlation of zero corresponds to \(\theta = 90^\circ\), suggesting that the vectors are orthogonal and thus linearly uncorrelated.</p> <p>This geometric interpretation gives a clear, visual sense of the relationships encoded in \(R\): the closer the angle between two variables‚Äô vectors is to zero, the stronger and more positive their correlation; the closer the angle is to \(180^\circ\), the stronger and more negative the correlation. And when the vectors are perpendicular, they are uncorrelated in a linear sense, even though nonlinear relationships might still exist.</p> <h3 id="eigenvalues-and-eigenvectors-the-structure-of-r">Eigenvalues and Eigenvectors: The Structure of \(R\)</h3> <p>The geometric story of \(R\) deepens when we consider its eigenvalues and eigenvectors. By performing an eigen-decomposition on \(R\), we can break it down as follows:</p> \[R = Q \Lambda Q^T\] <p>where \(Q\) is an orthogonal matrix whose columns are the eigenvectors of \(R\), and \(\Lambda\) is a diagonal matrix containing the eigenvalues of \(R\). Each eigenvalue \(\lambda_i\) in \(\Lambda\) represents the variance explained along the direction specified by the corresponding eigenvector \(q_i\) in \(Q\).</p> <p>Geometrically, the eigenvectors of \(R\) indicate the principal directions of variance in the data. The eigenvalues, on the other hand, tell us the ‚Äúlengths‚Äù or ‚Äústrengths‚Äù of these directions. A large eigenvalue associated with an eigenvector indicates that the data has significant variance along that direction, meaning the variables exhibit strong alignment with each other in this space. Conversely, smaller eigenvalues correspond to directions with less variance, suggesting that the data is more tightly clustered or linearly dependent in those directions.</p> <p>Since \(R\) is positive semi-definite, all eigenvalues are non-negative, and the sum of the eigenvalues equals the dimensionality of the space. Each eigenvalue-eigenvector pair offers a glimpse into the ‚Äúshape‚Äù of the data cloud in terms of its stretch and orientation in different directions.</p> <h3 id="pca-extracting-linear-patterns">PCA: Extracting Linear Patterns</h3> <p>The correlation matrix \(R\) is a natural tool for conducting PCA, a technique used to identify the main linear patterns in data by transforming it into a new coordinate system based on the eigenvectors of \(R\). In PCA, the data is projected onto the eigenvectors of \(R\), with each projection representing a principal component. The eigenvalue associated with each eigenvector measures the variance along that principal component, so ordering the eigenvalues from largest to smallest provides a ranking of the directions of greatest variability.</p> <p>To perform PCA, we start by obtaining the eigen-decomposition of \(R\):</p> \[R = Q \Lambda Q^T\] <p>The columns of \(Q\), the eigenvectors, form the new basis in which the data is represented. Each eigenvector corresponds to a principal component, and the associated eigenvalue indicates the amount of variance captured by that component. For instance, the first principal component (corresponding to the largest eigenvalue) captures the direction of maximum variance in the data, providing the best one-dimensional summary of the data‚Äôs spread. Adding successive principal components gives progressively refined approximations of the data, capturing most of the structure with far fewer dimensions than the original dataset.</p> <p>By using \(R\) in PCA, we effectively perform dimensionality reduction based on the strength of the linear relationships between variables, preserving the most informative aspects of the data while discarding redundancy.</p> <h3 id="demo">Demo</h3> <p>To illustrate how PCA leverages the correlation matrix \(R\) to extract meaningful linear patterns, let‚Äôs walk through a concrete example. Suppose we have a dataset with three variables‚Äîsay, height, weight, and age‚Äîmeasured across a sample of individuals. Let‚Äôs assume that after standardizing the data, we calculate the correlation matrix \(R\) as follows:</p> \[R = \begin{bmatrix} 1 &amp; 0.8 &amp; 0.5 \\ 0.8 &amp; 1 &amp; 0.4 \\ 0.5 &amp; 0.4 &amp; 1 \end{bmatrix}\] <p>Each entry \(R_{ij}\) represents the correlation between pairs of variables. For example, the correlation between height and weight is 0.8, indicating a strong positive linear relationship, while the correlation between height and age is 0.5, a moderate positive association.</p> <p><strong>Step 1: Eigen-Decomposition of the Correlation Matrix</strong></p> <p>The first step in PCA is to perform an eigen-decomposition of \(R\) to identify the principal directions of variance. By finding the eigenvalues and eigenvectors of \(R\), we can understand the main directions in which the data varies most.</p> <p>For our example, suppose the eigenvalues and their corresponding eigenvectors for \(R\) are as follows:</p> <ul> <li>Eigenvalue \(\lambda_1 = 1.8\), with eigenvector \(q_1 = \begin{bmatrix} 0.7 \\ 0.6 \\ 0.4 \end{bmatrix}\)</li> <li>Eigenvalue \(\lambda_2 = 0.9\), with eigenvector \(q_2 = \begin{bmatrix} -0.5 \\ 0.7 \\ 0.5 \end{bmatrix}\)</li> <li>Eigenvalue \(\lambda_3 = 0.3\), with eigenvector \(q_3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}\)</li> </ul> <p>Each eigenvalue represents the variance explained by its corresponding eigenvector direction. The largest eigenvalue, \(\lambda_1 = 1.8\), tells us that the first principal component explains the most variance in the data‚Äîroughly \(\frac{1.8}{3} = 60\%\) of the total variance (since the sum of the eigenvalues is 3 in a three-variable system). The second eigenvalue \(\lambda_2 = 0.9\) accounts for \(30\%\) of the variance, and the third eigenvalue \(\lambda_3 = 0.3\) contributes \(10\%\).</p> <p><strong>Step 2: Interpreting the Principal Components</strong></p> <p>Each eigenvector represents a direction in the original variable space (height, weight, age) along which there is a certain level of variability in the data. Let‚Äôs break down what each principal component reveals:</p> <ol> <li> <p><strong>First Principal Component (PC1)</strong>: The first eigenvector \(q_1 = \begin{bmatrix} 0.7 \\ 0.6 \\ 0.4 \end{bmatrix}\) suggests that PC1 is a weighted combination of all three variables, with the largest weights on height and weight. This component captures the common variability between height and weight, reflecting a general ‚Äúsize‚Äù factor‚Äîindividuals with larger heights tend to have larger weights. Since this component explains 60% of the variance, it‚Äôs the most informative single direction for summarizing the data.</p> </li> <li> <p><strong>Second Principal Component (PC2)</strong>: The second eigenvector \(q_2 = \begin{bmatrix} -0.5 \\ 0.7 \\ 0.5 \end{bmatrix}\) has significant contributions from all three variables, but with a negative sign for height and positive signs for weight and age. This suggests that PC2 captures a contrast between height and a combined weight-age factor, which might reflect a tendency where, for a given age, people with smaller heights have relatively higher weights.</p> </li> <li> <p><strong>Third Principal Component (PC3)</strong>: The third eigenvector \(q_3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}\) explains only 10% of the total variance. This component primarily reflects variability in age with some influence from height and a negative contribution from weight, capturing a pattern that is less pronounced in the data.</p> </li> </ol> <p><strong>Step 3: Projecting the Data onto the Principal Components</strong></p> <p>With the principal components identified, we can project our original data onto these components to transform it into a new coordinate system defined by \(q_1\), \(q_2\), and \(q_3\). This projection can be computed as:</p> \[\text{Projected Data} = Q^T X\] <p>where \(Q\) is the matrix of eigenvectors:</p> \[Q = \begin{bmatrix} 0.7 &amp; -0.5 &amp; 0.5 \\ 0.6 &amp; 0.7 &amp; -0.2 \\ 0.4 &amp; 0.5 &amp; 0.8 \end{bmatrix}\] <p>For each individual in the dataset, their original measurements in terms of height, weight, and age are transformed into scores along PC1, PC2, and PC3. These scores reflect the new, simplified representation of each individual in terms of the most significant patterns in the data. For example, projecting the original data onto PC1 (the direction of greatest variance) gives a one-dimensional summary of ‚Äúsize‚Äù variation across individuals, effectively condensing the information from three variables into a single informative score.</p> <p><strong>Step 4: Reducing Dimensionality</strong></p> <p>Since PC1 and PC2 together account for 90% of the total variance, we might decide to approximate our data using only these two components, discarding PC3, which contributes relatively little. By retaining only PC1 and PC2, we reduce the dimensionality of the dataset from three to two while preserving the bulk of the information.</p> <p>This reduced representation is particularly useful for visualization: each individual can now be represented by a point in a two-dimensional plane defined by PC1 and PC2. The positions of these points reflect the primary structure and relationships in the original data, without the ‚Äúnoise‚Äù from minor variations that PC3 captures. Moreover, patterns and clusters within the data often become more apparent in this reduced-dimensional space, revealing insights that might not be obvious in the original three-dimensional view.</p> <script type="text/tikz">
\begin{tikzpicture}

% Darker blue fill for the front Weight-Height square plane
\fill[blue!40, opacity=0.6] (0,0,0) -- (2.8,0,0) -- (2.8,2.8,0) -- (0,2.8,0) -- cycle;

% Lighter blue fill for the Weight-Age side parallelogram
\fill[blue!30, opacity=0.4] (0,0,0) -- (0,0,2.8) -- (0,2.8,2.8) -- (0,2.8,0) -- cycle;

% Lighter blue fill for the Height-Age top parallelogram
\fill[blue!30, opacity=0.4] (0,0,2.8) -- (2.8,0,2.8) -- (2.8,2.8,2.8) -- (0,2.8,2.8) -- cycle;

% Original 3D coordinate system (Height, Weight, Age)
\draw[->, thick] (0,0,0) -- (2.8,0,0) node[anchor=north east] {Height};
\draw[->, thick] (0,0,0) -- (0,2.8,0) node[anchor=north west] {Weight};
\draw[->, thick] (0,0,0) -- (0,0,2.8) node[anchor=south] {Age};

% Original data points in the 3D coordinate system (blue)
\fill[blue] (1,2,1.5) circle (2pt);
\fill[blue] (1.8,1.2,1.8) circle (2pt);
\fill[blue] (1.3,1.8,1.3) circle (2pt);
\fill[blue] (1.6,1.5,1) circle (2pt);

% Arrow indicating projection to the PC1-PC2 plane, with reduced distance
\draw[->, thick, dashed] (3, 1.8, 1.5) -- (5.8, 1.8, 0) node[midway, above, sloped] {Projection onto PC1-PC2 plane};

% Darker orange fill for the 2D PC1-PC2 plane, with slight shadow effect
\fill[orange!40, opacity=0.5] (6.5,0,0) -- (9,0,0) -- (8.7,2.3,0) -- (6.2,2.3,0) -- cycle;

% 2D PC1-PC2 plane axes
\draw[->, color=orange, thick] (6.5,0,0) -- (9,0,0) node[anchor=north east] {PC1(60\%)};
\draw[->, color=orange, thick] (6.5,0,0) -- (6.2,2.3,0) node[anchor=north west] {PC2(30\%)};

% Projected data points on the PC1-PC2 plane (red)
\fill[red] (7.2,1.8,0) circle (2pt);
\fill[red] (7.9,1,0) circle (2pt);
\fill[red] (7.4,1.5,0) circle (2pt);
\fill[red] (7.8,1.2,0) circle (2pt);

% Dashed lines connecting original points to their projections
\draw[dashed, gray] (1,2,1.5) -- (7.2,1.8,0);
\draw[dashed, gray] (1.8,1.2,1.8) -- (7.9,1,0);
\draw[dashed, gray] (1.3,1.8,1.3) -- (7.4,1.5,0);
\draw[dashed, gray] (1.6,1.5,1) -- (7.8,1.2,0);

\end{tikzpicture}
</script> <p>¬†</p> <p><strong>Summary</strong></p> <p>In this example, PCA has taken a dataset with three interrelated variables and transformed it into a new set of uncorrelated components that reveal the primary patterns of variability. By examining the eigenvalues and eigenvectors of the correlation matrix \(R\), we extracted principal components that provided both a compact and interpretable representation of the data.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>In conclusion, this post serves as a natural extension to <a href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">our previous discussion on covariance</a>, diving into the aspects of correlation that were left unexplored. While the covariance matrix lays the groundwork for understanding how variables interact, it lacks the standardization needed for clear, direct comparisons. The correlation matrix fills this gap, distilling complex relationships into a standardized, unit-free format that provides immediate insights into linear dependencies across variables. Each entry in the matrix reflects a pairwise relationship as an angle or alignment.</p>]]></content><author><name></name></author><category term="A Column on Gaussian Processes"/><category term="Statistics"/><category term="Stochastic Processes"/><summary type="html"><![CDATA[Yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales...]]></summary></entry><entry><title type="html">An Introductory Look at Covariance and the Mean Vector</title><link href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/" rel="alternate" type="text/html" title="An Introductory Look at Covariance and the Mean Vector"/><published>2024-06-11T00:32:10+00:00</published><updated>2024-06-11T00:32:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>When you first encounter the world of multivariate Gaussian distributions, it‚Äôs easy to feel like you‚Äôve entered a labyrinth of equations, variables, and matrices. But beneath the mathematical machinery lies a beautifully structured framework that helps us understand complex data, and in many ways, it‚Äôs as elegant as it is powerful.</p> <p>In statistics, the Gaussian, or ‚Äúnormal‚Äù distribution, is often our first stop when we dive into data analysis. We‚Äôve all seen its familiar bell-shaped curve, neatly centered around a mean, showing us the most probable values a single variable might take. But in reality, data rarely exists in a vacuum. Many variables are interconnected, forming a ‚Äúmultivariate‚Äù data landscape where each variable influences and interacts with others. Here, the multivariate Gaussian distribution steps in as a natural extension of the single-variable Gaussian, providing us a way to model these multidimensional relationships.</p> <p>At the heart of this distribution are two key players: the mean vector and the covariance matrix. The mean vector is the multivariate equivalent of the single-variable mean, summarizing the central tendencies of all variables in one go. It tells us where the ‚Äúcenter‚Äù of our data cloud lies, capturing the average or typical values across all dimensions.</p> <blockquote> <p>Updated on June 22, 2024: Additionally, <a href="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/">a new blog post</a> has been published that expands on this topic with content covering correlation coefficients and the correlation coefficient matrix.</p> </blockquote> <p>The covariance matrix, on the other hand, is a bit like a backstage operator. It describes how variables interact with each other, revealing not just their individual spreads but also how they move in tandem. Each entry in this matrix provides insights into the relationship between pairs of variables, showing whether they rise and fall together or behave independently. Together, the mean vector and covariance matrix form a powerful duo, shaping the geometry of the distribution and giving us a complete picture of how our data points are scattered and related.</p> <p>In this post, we‚Äôll explore the roles of the mean vector and covariance matrix within the multivariate Gaussian distribution, diving into how they help us grasp and model complex data structures.</p> <hr/> <h2 id="the-mean-vector--definition-and-properties">The Mean Vector ‚Äì Definition and Properties</h2> <p>Now that we‚Äôve opened the door to the multivariate Gaussian, let‚Äôs take a closer look at one of its core components: the mean vector. You can think of the mean vector as the ‚Äúcentroid‚Äù or the ‚Äúanchor‚Äù point of the distribution‚Äîa snapshot of where the average of each variable in a dataset tends to lie. In a multivariate world, we don‚Äôt just care about one mean; we want to know the average value in each dimension, and that‚Äôs where the mean vector steps in.</p> <h3 id="definition">Definition</h3> <p>For a multivariate random variable \(X\) with \(n\) dimensions, the mean vector \(\mu\) is defined as:</p> \[\mu = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix},\] <p>where each \(\mu_i = E[X_i]\) represents the expected value of the \(i\)-th variable. Essentially, the mean vector \(\mu\) gives us a one-stop summary of the ‚Äúaverage‚Äù position of all dimensions, capturing the expected value along each axis of the multidimensional data space.</p> <h3 id="derivation-expectation-of-the-mean-vector">Derivation: Expectation of the Mean Vector</h3> <p>To fully appreciate the mean vector, let‚Äôs delve into its calculation using the expectation operator. Suppose \(X = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix}\) is our multivariate random variable, where each \(X_i\) is a random variable in itself. The mean vector \(\mu\) is simply the expected value of \(X\):</p> \[\mu = E[X].\] <p>Breaking this down, <strong>the expectation of \(X\) is computed component-wise</strong>. That is,</p> \[\mu = E[X] = \begin{bmatrix} E[X_1] \\ E[X_2] \\ \vdots \\ E[X_n] \end{bmatrix} = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix}.\] <p>This form allows us to treat each dimension independently when calculating the mean, <strong>even though they may be interdependent in terms of their distributions.</strong></p> <h3 id="properties-of-the-mean-vector">Properties of the Mean Vector</h3> <p>The mean vector isn‚Äôt just a passive summary of averages; it‚Äôs highly responsive to transformations, particularly linear ones. Let‚Äôs explore one of its key properties: how it behaves under a linear transformation. Consider a linear transformation where we define a new random vector \(Y\) based on \(X\) as:</p> \[Y = AX + b,\] <p>where \(A\) is a constant matrix of dimensions \(m \times n\) and \(b\) is a constant vector of dimension \(m \times 1\). This setup is common in multivariate analysis, where we often transform data to new coordinate systems or scales. Here, we want to understand how this transformation impacts the mean vector.</p> <p>To derive the expected value of \(Y\), we use the linearity of expectation:</p> \[E[Y] = E[AX + b].\] <p>Since \(A\) and \(b\) are constants, we can simplify:</p> \[E[Y] = AE[X] + b = A\mu + b.\] <p>So, the transformed mean vector of \(Y\) is given by \(A\mu + b\). This tells us that linear transformations shift and stretch the mean vector in predictable ways: multiplying by \(A\) scales or rotates \(\mu\), while adding \(b\) translates it. Let‚Äôs formalize this with a quick proof. Suppose \(X\) is a multivariate random variable with mean vector \(\mu = E[X]\), and we define \(Y = AX + b\). By the definition of expectation, we have:</p> \[E[Y] = E[AX + b] = E[AX] + E[b].\] <p>Since \(b\) is constant, \(E[b] = b\). Additionally, because expectation is a linear operator, we get \(E[AX] = A E[X] = A \mu\). Thus,</p> \[E[Y] = A \mu + b.\] <p>This property is not only elegant but incredibly useful. It implies that, regardless of the transformation (as long as it‚Äôs linear), we can predict how the mean vector shifts without recalculating everything from scratch. This ‚Äútransformation invariance‚Äù simplifies a lot of practical work in data analysis, letting us predict and manipulate mean vectors in transformed spaces.</p> <p>In sum, the mean vector \(\mu\) isn‚Äôt just a set of averages. It‚Äôs a fundamental descriptor that tells us where our data is centered, and its behavior under transformations is both consistent and computationally friendly. <strong>Whether you‚Äôre scaling, rotating, or translating your data, the mean vector adjusts accordingly, maintaining its role as the central anchor of your multivariate Gaussian distribution.</strong></p> <h3 id="demo">Demo</h3> <p>To understand how the mean vector \(\mu\) behaves, particularly under transformations, let‚Äôs start by generating some multivariate data. We‚Äôll create a simple 2D Gaussian distribution, calculate its mean vector, and then apply a linear transformation to see how \(\mu\) changes in response.</p> <p>Here‚Äôs a Python script that does exactly this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Set a random seed for reproducibility
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Step 1: Define the original mean vector and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>  <span class="c1"># Positive correlation between X1 and X2
</span>
<span class="c1"># Generate a sample of 500 points from the 2D Gaussian distribution
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Plot the original distribution
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Original Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector (Original)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Original Multivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, we initialize a mean vector \(\mu = [2, 3]\) and a covariance matrix with a positive correlation between the two variables. We then generate 500 points to visually represent the data cloud centered around \(\mu\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution-480.webp 480w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution-800.webp 800w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution.png" class="img-fluid" width="600" height="400" alt="Original Multivariate Gaussian Distribution" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now, let‚Äôs apply a linear transformation to this data. For demonstration, we‚Äôll use a transformation matrix \(A = \begin{bmatrix} 1.5 &amp; 0 \\ 0.5 &amp; 1 \end{bmatrix}\) and a translation vector \(b = \begin{bmatrix} -1 \\ 2 \end{bmatrix}\). According to our derivation, we expect the mean vector to change to \(A\mu + b\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 2: Define the transformation matrix A and translation vector b
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Apply the transformation to each data point
</span><span class="n">transformed_data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Calculate the transformed mean vector
</span><span class="n">transformed_mu</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Plot the transformed distribution
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Transformed Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">transformed_mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">transformed_mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector (Transformed)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Transformed Multivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>This second snippet applies our transformation and then calculates the new mean vector using \(A\mu + b\), aligning with our theoretical result. Here‚Äôs what we observe from the plot:</p> <ol> <li><strong>Data Shift and Rotation</strong>: The data cloud shifts according to the translation vector \(b\) and stretches based on the transformation matrix \(A\).</li> <li><strong>Mean Vector Update</strong>: The new mean vector \(\mu\) moves precisely to \(A\mu + b\), as predicted.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution-480.webp 480w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution-800.webp 800w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution.png" class="img-fluid" width="600" height="400" alt="Transformed Multivariate Gaussian Distribution" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="covariance-matrix--definition-and-computation">Covariance Matrix ‚Äì Definition and Computation</h2> <p>If the mean vector \(\mu\) gives us a sense of location, <strong>then the covariance matrix \(\Sigma\) gives us a sense of shape.</strong> It describes how variables are spread and how they relate to each other, capturing both individual variances and pairwise covariances.</p> <h3 id="definition-1">Definition</h3> <p>The covariance matrix \(\Sigma\) of a multivariate random variable \(X = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix}\) is defined as:</p> \[\Sigma = \begin{bmatrix} \text{Cov}(X_1, X_1) &amp; \text{Cov}(X_1, X_2) &amp; \cdots &amp; \text{Cov}(X_1, X_n) \\ \text{Cov}(X_2, X_1) &amp; \text{Cov}(X_2, X_2) &amp; \cdots &amp; \text{Cov}(X_2, X_n) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \text{Cov}(X_n, X_1) &amp; \text{Cov}(X_n, X_2) &amp; \cdots &amp; \text{Cov}(X_n, X_n) \end{bmatrix},\] <p>where each element \(\Sigma\_{ij} = \text{Cov}(X_i, X_j)\) is the covariance between \(X_i\) and \(X_j\).</p> <p>Covariance, in essence, measures the degree to which two variables vary together. When \(i = j\), \(\Sigma\_{ii} = \text{Var}(X_i)\), representing the variance of \(X_i\) itself.</p> <h3 id="derivation-the-formula-for-covariance">Derivation: The Formula for Covariance</h3> <p>To formally compute \(\Sigma\), let‚Äôs start with the mean vector \(\mu\) of \(X\), defined as:</p> \[\mu = E[X] = \begin{bmatrix} E[X_1] \\ E[X_2] \\ \vdots \\ E[X_n] \end{bmatrix}.\] <p>Now, the covariance matrix is calculated as the expectation of the outer product of the deviations of \(X\) from its mean:</p> \[\Sigma = E[(X - \mu)(X - \mu)^T].\] <p>This formula may look abstract, but it‚Äôs grounded in a straightforward concept: by centering \(X\) around its mean (i.e., subtracting \(\mu\)) and then taking the outer product, we capture the spread and co-spread of each variable pair.</p> <h4 id="step-by-step-derivation-of-sigma">Step-by-Step Derivation of \(\Sigma\)</h4> <p>Let‚Äôs expand the formula a bit to understand the inner workings. We have:</p> \[\Sigma = E \left[ \begin{bmatrix} X_1 - \mu_1 \\ X_2 - \mu_2 \\ \vdots \\ X_n - \mu_n \end{bmatrix} \begin{bmatrix} X_1 - \mu_1 &amp; X_2 - \mu_2 &amp; \cdots &amp; X_n - \mu_n \end{bmatrix} \right].\] <p>When we take the expectation, each element \(\Sigma\_{ij}\) becomes:</p> \[\Sigma_{ij} = E[(X_i - \mu_i)(X_j - \mu_j)].\] <p>This is simply the definition of covariance between \(X_i\) and \(X_j\). As such, \(\Sigma\) encodes all pairwise relationships in one matrix, giving us a complete picture of how our variables are interconnected.</p> <h3 id="properties-of-the-covariance-matrix">Properties of the Covariance Matrix</h3> <p>The covariance matrix isn‚Äôt just a convenient summary; it has some fascinating mathematical properties that make it a powerful tool in multivariate analysis.</p> <h4 id="symmetry">Symmetry</h4> <p>One of the most fundamental properties of \(\Sigma\) is that it is symmetric. To see why, consider the definition of covariance:</p> \[\Sigma_{ij} = \text{Cov}(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)].\] <p>By the commutative property of multiplication, \((X_i - \mu_i)(X_j - \mu_j) = (X_j - \mu_j)(X_i - \mu_i)\). Thus:</p> \[\Sigma_{ij} = E[(X_i - \mu_i)(X_j - \mu_j)] = E[(X_j - \mu_j)(X_i - \mu_i)] = \Sigma_{ji}.\] <p>This symmetry property tells us that \(\Sigma\) is equal to its own transpose, or \(\Sigma = \Sigma^T\). This is crucial because it ensures that the eigenvalues of \(\Sigma\) are real, a feature that will come in handy when interpreting the distribution‚Äôs geometry.</p> <h4 id="positive-semi-definiteness">Positive Semi-Definiteness</h4> <p>Another key property of the covariance matrix is that it is positive semi-definite. Mathematically, this means that for any vector \(z\), we have:</p> \[z^T \Sigma z \geq 0.\] <p><strong>Intuitively, this property tells us that the ‚Äúspread‚Äù of the data is never negative</strong>‚Äîa fundamental requirement for any meaningful measure of variance. Let‚Äôs quickly prove this property.</p> <p>For any vector \(z \in \mathbb{R}^n\), we have:</p> \[z^T \Sigma z = z^T E[(X - \mu)(X - \mu)^T] z = E[z^T (X - \mu)(X - \mu)^T z] = E[(z^T(X - \mu))^2].\] <p>Since \((z^T(X - \mu))^2\) is a square, it is always non-negative, which implies that \(z^T \Sigma z \geq 0\). Thus, \(\Sigma\) is positive semi-definite, meaning it has non-negative eigenvalues, another crucial feature for understanding data spread.</p> <h4 id="covariance-under-linear-transformation">Covariance under Linear Transformation</h4> <p>One of the most powerful aspects of the covariance matrix is how it transforms under linear operations. Suppose we apply a linear transformation \(Y = AX + b\), where \(A\) is a constant matrix and \(b\) is a constant vector. Just as we saw with the mean vector, we want to understand how the covariance matrix \(\Sigma\) changes under this transformation.</p> <p>The covariance of \(Y\), denoted \(\text{Cov}(Y)\), is given by:</p> \[\text{Cov}(Y) = E[(Y - E[Y])(Y - E[Y])^T].\] <p>Since \(Y = AX + b\), we can substitute and simplify:</p> \[Y - E[Y] = AX + b - (AE[X] + b) = A(X - E[X]).\] <p>Thus:</p> \[\text{Cov}(Y) = E[A(X - E[X])(X - E[X])^T A^T] = A E[(X - E[X])(X - E[X])^T] A^T = A \Sigma A^T.\] <p>This result, \(\text{Cov}(Y) = A \Sigma A^T\), tells us that under a linear transformation, the covariance matrix \(\Sigma\) transforms in a predictable manner. This property is critical in fields like machine learning and statistics, where data is often scaled or rotated to enhance interpretability or improve model performance.</p> <h3 id="demo-1">Demo</h3> <p>To illustrate the covariance matrix in action, let‚Äôs revisit our earlier 2D Gaussian example, but now focus on how the covariance affects the spread and orientation of the data cloud. We‚Äôll also examine how the covariance matrix changes under a linear transformation, connecting this back to our theoretical results.</p> <h4 id="step-1-visualize-the-original-covariance-structure">Step 1: Visualize the Original Covariance Structure</h4> <p>In this example, we‚Äôll use the same mean vector \(\mu = \begin{bmatrix} 2 \\ 3 \end{bmatrix}\) and a covariance matrix \(\Sigma = \begin{bmatrix} 1 &amp; 0.8 \\ 0.8 &amp; 1 \end{bmatrix}\). The positive off-diagonal values indicate a positive correlation between the two variables, meaning they tend to vary together.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Define the mean vector and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Generate data points from a 2D Gaussian distribution
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Plot the data cloud and the covariance structure
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Original Multivariate Gaussian Distribution with Covariance Structure</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="c1"># Visualize the covariance as an ellipse
</span><span class="kn">from</span> <span class="n">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>

<span class="c1"># Eigenvalues and eigenvectors of the covariance matrix
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># Width and height of the ellipse based on the eigenvalues (scaled for visibility)
</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>
<span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">'</span><span class="s">None</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Covariance Ellipse</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, we:</p> <ol> <li>Generate a data cloud based on \(\Sigma\), capturing the positive correlation between \(X_1\) and \(X_2\).</li> <li>Plot the data points along with the mean vector.</li> <li>Use the eigenvalues and eigenvectors of \(\Sigma\) to plot an ellipse representing the covariance structure. The orientation and size of this ellipse reflect the spread and correlation encoded in \(\Sigma\), with the longer axis aligned along the direction of greatest variance.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure-480.webp 480w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure-800.webp 800w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure.png" class="img-fluid" width="600" height="400" alt="Original Multivariate Gaussian Distribution with Covariance Structure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="step-2-apply-a-linear-transformation-and-observe-the-covariance-matrix-change">Step 2: Apply a Linear Transformation and Observe the Covariance Matrix Change</h4> <p>Next, let‚Äôs apply a linear transformation to our data. We‚Äôll use a transformation matrix \(A = \begin{bmatrix} 1.2 &amp; 0.5 \\ 0.3 &amp; 0.8 \end{bmatrix}\), which will stretch and rotate the data. According to our earlier derivation, the new covariance matrix should be \(A \Sigma A^T\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the transformation matrix A
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>

<span class="c1"># Transform the data
</span><span class="n">transformed_data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span>

<span class="c1"># Calculate the transformed covariance matrix
</span><span class="n">transformed_cov</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">cov</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span>

<span class="c1"># Plot the transformed data and new covariance structure
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Transformed Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Transformed Multivariate Gaussian Distribution with New Covariance Structure</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Calculate and plot the new covariance ellipse
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">transformed_cov</span><span class="p">)</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>
<span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">A</span> <span class="o">@</span> <span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">'</span><span class="s">None</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Transformed Covariance Ellipse</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this visualization:</p> <ol> <li>We transform the data by applying \(A\), which stretches and rotates the original distribution.</li> <li>We compute the transformed covariance matrix as \(A \Sigma A^T\) and plot the new covariance ellipse to represent its structure.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure-480.webp 480w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure-800.webp 800w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure.png" class="img-fluid" width="600" height="400" alt="Transformed Multivariate Gaussian Distribution with New Covariance Structure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="observations">Observations</h3> <p>With this code, we see the impact of a linear transformation on the covariance matrix:</p> <ul> <li><strong>Spread and Orientation</strong>: The transformed covariance ellipse is reshaped and reoriented. The principal axes of the ellipse align with the directions of greatest and least variance in the transformed data, which reflect the new covariance structure encoded by \(A \Sigma A^T\).</li> <li><strong>Predicted Transformation</strong>: The calculation \(A \Sigma A^T\) matches the new spread, showing that even though the data has shifted in space, we can predict exactly how its variability changes.</li> </ul> <hr/> <h2 id="the-geometric-meaning-of-the-covariance-matrix">The Geometric Meaning of the Covariance Matrix</h2> <p>At this point, we know the covariance matrix \(\Sigma\) defines the structure of a multivariate Gaussian distribution in terms of variance and covariance. But what does \(\Sigma\) really <em>look like</em>? In a two-dimensional space, the covariance matrix paints an elegant picture of geometry, describing a distribution‚Äôs shape as an ellipse. This section is a journey into the geometric implications of \(\Sigma\), where each feature of the covariance matrix corresponds to a unique aspect of the data‚Äôs spatial spread.</p> <h3 id="elliptical-contours-the-shape-of-data-in-2d">Elliptical Contours: The Shape of Data in 2D</h3> <p>In the case of a two-dimensional Gaussian distribution, the contours of equal density‚Äîthink of these as the ‚Äúoutlines‚Äù or ‚Äúborders‚Äù where data points tend to cluster‚Äîform concentric ellipses. These ellipses reveal the spread of data around the mean vector \(\mu\) and are directly determined by the covariance matrix \(\Sigma\).</p> <p>Each ellipse‚Äôs geometry‚Äîthe length and direction of its axes‚Äîoffers a visual interpretation of \(\Sigma\):</p> <ol> <li><strong>Principal Axes (Direction)</strong>: The directions of the ellipse‚Äôs axes correspond to the eigenvectors of \(\Sigma\). These eigenvectors are vectors in space that point along the directions where the data varies the most (the ‚Äúprincipal directions‚Äù).</li> <li><strong>Axis Lengths (Spread)</strong>: The lengths of these axes are proportional to the square roots of the eigenvalues of \(\Sigma\). The larger an eigenvalue, the longer the axis, meaning that data stretches more along this direction. Smaller eigenvalues correspond to shorter axes, indicating less variability in that direction.</li> </ol> <h3 id="eigenvalue-decomposition-of-the-covariance-matrix">Eigenvalue Decomposition of the Covariance Matrix</h3> <p>To understand the ellipse‚Äôs structure more deeply, let‚Äôs look at the eigenvalue decomposition of \(\Sigma\). The covariance matrix \(\Sigma\) is symmetric, meaning it can be decomposed as:</p> \[\Sigma = Q \Lambda Q^T,\] <p>where:</p> <ul> <li>\(Q\) is a matrix of eigenvectors of \(\Sigma\), and</li> <li>\(\Lambda\) is a diagonal matrix of eigenvalues of \(\Sigma\), with each eigenvalue corresponding to the variance along a principal direction.</li> </ul> <p>This decomposition provides a clean geometric interpretation. If we rewrite our multivariate random variable \(X\) (centered at the origin for simplicity) as:</p> \[X = Q D Z,\] <p>where \(D = \sqrt{\Lambda}\) is the matrix of square roots of the eigenvalues and \(Z\) is a vector of standard normal variables, we can see that \(Q\) rotates the data along the principal directions and \(D\) scales it according to the variances.</p> <p>Let‚Äôs illustrate this in two steps.</p> <h4 id="step-1-visualize-the-original-data-with-eigenvectors">Step 1: Visualize the Original Data with Eigenvectors</h4> <p>We can start by plotting the original data and overlaying the eigenvectors of \(\Sigma\) to visualize the principal directions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Define mean and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>

<span class="c1"># Generate data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Plot the data
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Multivariate Gaussian with Principal Directions</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Compute eigenvalues and eigenvectors
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

<span class="c1"># Plot eigenvectors as principal directions
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">eigvecs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]],</span>
             <span class="p">[</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">eigvecs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]],</span>
             <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Principal Direction </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, we:</p> <ol> <li>Generate data according to the mean \(\mu\) and covariance matrix \(\Sigma\).</li> <li>Plot the data along with the mean vector.</li> <li>Compute the eigenvalues and eigenvectors of \(\Sigma\), and overlay the eigenvectors on the data, scaled by the square root of their corresponding eigenvalues to represent the primary directions and spread.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions-480.webp 480w,/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions-800.webp 800w,/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions.png" class="img-fluid" width="600" height="400" alt="Multivariate Gaussian with Principal Directions" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The eigenvectors point along the primary axes of the data, while their lengths, scaled by the square roots of the eigenvalues, indicate how far the data stretches along these axes. This visualization immediately tells us the directions in which our data is most (or least) spread out.</p> <h4 id="step-2-visualizing-the-covariance-matrix-as-an-ellipse">Step 2: Visualizing the Covariance Matrix as an Ellipse</h4> <p>To highlight the covariance structure even further, we can draw the ellipse representing a particular level set of our Gaussian density. Here‚Äôs the code to do so, using the eigenvalues and eigenvectors to construct an ellipse around the data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot data with covariance ellipse
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Covariance Ellipse of the Multivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Draw the ellipse
</span><span class="kn">from</span> <span class="n">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>  <span class="c1"># Ellipse axes scaled to represent variance
</span><span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">'</span><span class="s">None</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Covariance Ellipse</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>This plot gives us a clear visual representation of the covariance matrix‚Äôs ‚Äúfootprint‚Äù on the data:</p> <ul> <li><strong>Direction</strong>: The ellipse‚Äôs major and minor axes correspond to the principal directions (eigenvectors) of the data spread.</li> <li><strong>Length of Axes</strong>: The lengths of these axes are proportional to the square roots of the eigenvalues of \(\Sigma\), indicating the variance along each direction.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution-480.webp 480w,/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution-800.webp 800w,/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution.png" class="img-fluid" width="600" height="400" alt="Covariance Ellipse of the Multivariate Gaussian Distribution" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="geometric-takeaways">Geometric Takeaways</h3> <p>So, what does this ellipse tell us about the data‚Äôs geometry?</p> <ol> <li><strong>Primary Spread</strong>: The major axis shows where the data is most dispersed, aligned along the eigenvector with the largest eigenvalue. This direction represents the direction of greatest variance.</li> <li><strong>Secondary Spread</strong>: The minor axis, perpendicular to the major axis, aligns with the eigenvector of the smaller eigenvalue, showing where the data is more tightly clustered.</li> </ol> <p>This elliptical geometry offers a powerful intuition: <strong>the covariance matrix defines an oriented and scaled ellipse that encapsulates the data‚Äôs spread and directionality. By examining this shape, we can quickly grasp the underlying structure of the distribution‚Äîits direction of spread, symmetry (or lack thereof), and how tightly data points are packed.</strong></p> <hr/> <h2 id="a-practical-example-of-deriving-the-covariance-matrix">A Practical Example of Deriving the Covariance Matrix</h2> <p>To ground our understanding of the covariance matrix in something concrete, let‚Äôs work through a hands-on example. We‚Äôll take a simple two-dimensional random variable, calculate its mean vector and covariance matrix from a small dataset, and observe how these values summarize our data.</p> <p>Imagine we have a dataset representing the measurements of two variables, say \(X_1\) and \(X_2\), for simplicity. This could be anything‚Äîa set of financial returns, height and weight pairs, or temperatures at different locations. Let‚Äôs define our random variable \(X\) as:</p> \[X = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix}.\] <p>We‚Äôll calculate the sample mean vector \(\hat{\mu}\) and the sample covariance matrix \(\hat{\Sigma}\) based on observed data.</p> <h3 id="step-1-calculate-the-sample-mean-vector">Step 1: Calculate the Sample Mean Vector</h3> <p>Let‚Äôs start with the sample mean vector, which captures the ‚Äúcentral location‚Äù of the data. Given \(N\) observations, we define the sample mean \(\hat{\mu}\) as:</p> \[\hat{\mu} = \frac{1}{N} \sum_{i=1}^N X_i,\] <p>where \(X_i\) represents the \(i\)-th observation of our random variable \(X\).</p> <p>For instance, suppose we have the following five observations:</p> \[X_1 = \begin{bmatrix} 2 \\ 3 \end{bmatrix}, \quad X_2 = \begin{bmatrix} 3 \\ 5 \end{bmatrix}, \quad X_3 = \begin{bmatrix} 5 \\ 7 \end{bmatrix}, \quad X_4 = \begin{bmatrix} 6 \\ 8 \end{bmatrix}, \quad X_5 = \begin{bmatrix} 8 \\ 10 \end{bmatrix}.\] <p>The sample mean vector \(\hat{\mu}\) is then calculated as:</p> \[\hat{\mu} = \frac{1}{5} \left( \begin{bmatrix} 2 \\ 3 \end{bmatrix} + \begin{bmatrix} 3 \\ 5 \end{bmatrix} + \begin{bmatrix} 5 \\ 7 \end{bmatrix} + \begin{bmatrix} 6 \\ 8 \end{bmatrix} + \begin{bmatrix} 8 \\ 10 \end{bmatrix} \right).\] <p>Breaking it down component-wise:</p> \[\hat{\mu}_1 = \frac{1}{5} (2 + 3 + 5 + 6 + 8) = 4.8, \quad \hat{\mu}_2 = \frac{1}{5} (3 + 5 + 7 + 8 + 10) = 6.6.\] <p>Thus, our mean vector is:</p> \[\hat{\mu} = \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix}.\] <h3 id="step-2-calculate-the-sample-covariance-matrix">Step 2: Calculate the Sample Covariance Matrix</h3> <p>Next, let‚Äôs calculate the sample covariance matrix, which tells us not only the variability of each variable but also how \(X_1\) and \(X_2\) move in relation to each other. The sample covariance matrix \(\hat{\Sigma}\) is defined as:</p> \[\hat{\Sigma} = \frac{1}{N-1} \sum_{i=1}^N (X_i - \hat{\mu})(X_i - \hat{\mu})^T.\] <p>For our five observations, \(N = 5\), so we‚Äôll divide by \(4\) (that‚Äôs \(N - 1\)).</p> <p>Let‚Äôs compute each term \((X_i - \hat{\mu})(X_i - \hat{\mu})^T\) for each observation:</p> <ol> <li> <p><strong>For \(X_1 = \begin{bmatrix} 2 \\ 3 \end{bmatrix}\):</strong></p> \[X_1 - \hat{\mu} = \begin{bmatrix} 2 \\ 3 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} -2.8 \\ -3.6 \end{bmatrix},\] <p>and</p> \[(X_1 - \hat{\mu})(X_1 - \hat{\mu})^T = \begin{bmatrix} -2.8 \\ -3.6 \end{bmatrix} \begin{bmatrix} -2.8 &amp; -3.6 \end{bmatrix} = \begin{bmatrix} 7.84 &amp; 10.08 \\ 10.08 &amp; 12.96 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_2 = \begin{bmatrix} 3 \\ 5 \end{bmatrix}\):</strong></p> \[X_2 - \hat{\mu} = \begin{bmatrix} 3 \\ 5 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} -1.8 \\ -1.6 \end{bmatrix},\] <p>and</p> \[(X_2 - \hat{\mu})(X_2 - \hat{\mu})^T = \begin{bmatrix} -1.8 \\ -1.6 \end{bmatrix} \begin{bmatrix} -1.8 &amp; -1.6 \end{bmatrix} = \begin{bmatrix} 3.24 &amp; 2.88 \\ 2.88 &amp; 2.56 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_3 = \begin{bmatrix} 5 \\ 7 \end{bmatrix}\):</strong></p> \[X_3 - \hat{\mu} = \begin{bmatrix} 5 \\ 7 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.4 \end{bmatrix},\] <p>and</p> \[(X_3 - \hat{\mu})(X_3 - \hat{\mu})^T = \begin{bmatrix} 0.2 \\ 0.4 \end{bmatrix} \begin{bmatrix} 0.2 &amp; 0.4 \end{bmatrix} = \begin{bmatrix} 0.04 &amp; 0.08 \\ 0.08 &amp; 0.16 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_4 = \begin{bmatrix} 6 \\ 8 \end{bmatrix}\):</strong></p> \[X_4 - \hat{\mu} = \begin{bmatrix} 6 \\ 8 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 1.4 \end{bmatrix},\] <p>and</p> \[(X_4 - \hat{\mu})(X_4 - \hat{\mu})^T = \begin{bmatrix} 1.2 \\ 1.4 \end{bmatrix} \begin{bmatrix} 1.2 &amp; 1.4 \end{bmatrix} = \begin{bmatrix} 1.44 &amp; 1.68 \\ 1.68 &amp; 1.96 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_5 = \begin{bmatrix} 8 \\ 10 \end{bmatrix}\):</strong> \(X_5 - \hat{\mu} = \begin{bmatrix} 8 \\ 10 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} 3.2 \\ 3.4 \end{bmatrix},\) and \((X_5 - \hat{\mu})(X_5 - \hat{\mu})^T = \begin{bmatrix} 3.2 \\ 3.4 \end{bmatrix} \begin{bmatrix} 3.2 &amp; 3.4 \end{bmatrix} = \begin{bmatrix} 10.24 &amp; 10.88 \\ 10.88 &amp; 11.56 \end{bmatrix}.\)</p> </li> </ol> <p>Now, we add these matrices and divide by \(N - 1 = 4\) to get the sample covariance matrix:</p> \[\hat{\Sigma} = \frac{1}{4} \left( \begin{bmatrix} 7.84 &amp; 10.08 \\ 10.08 &amp; 12.96 \end{bmatrix} + \begin{bmatrix} 3.24 &amp; 2.88 \\ 2.88 &amp; 2.56 \end{bmatrix} + \begin{bmatrix} 0.04 &amp; 0.08 \\ 0.08 &amp; 0.16 \end{bmatrix} + \begin{bmatrix} 1.44 &amp; 1.68 \\ 1.68 &amp; 1.96 \end{bmatrix} + \begin{bmatrix} 10.24 &amp; 10.88 \\ 10.88 &amp; 11.56 \end{bmatrix} \right).\] <p>After summing the matrices:</p> \[\hat{\Sigma} = \frac{1}{4} \begin{bmatrix} 22.8 &amp; 25.6 \\ 25.6 &amp; 29.2 \end{bmatrix} = \begin{bmatrix} 5.7 &amp; 6.4 \\ 6.4 &amp; 7.3 \end{bmatrix}.\] <h3 id="summary">Summary</h3> <p>In this example, we‚Äôve calculated both the sample mean vector and the sample covariance matrix. Our sample covariance matrix, \(\hat{\Sigma} = \begin{bmatrix} 5.7 &amp; 6.4 \\ 6.4 &amp; 7.3 \end{bmatrix}\), encapsulates the spread and relationship between \(X_1\) and \(X_2\). The off-diagonal terms (6.4) represent the covariance between \(X_1\) and \(X_2\), indicating a positive correlation, while the diagonal terms represent the variances of \(X_1\) and \(X_2\) individually.</p> <p>This simple calculation reinforces the power of the covariance matrix‚Äîit captures both the ‚Äúspread‚Äù of each variable and their interaction, providing a complete picture of our data‚Äôs geometric and statistical structure.</p> <hr/> <h2 id="covariance-matrix-and-independence">Covariance Matrix and Independence</h2> <p>As we wrap up our journey into the world of covariance matrices, it‚Äôs fitting to address one of the most common misconceptions: the relationship between <strong>independence</strong> and <strong>uncorrelatedness</strong>. In the world of multivariate distributions, understanding whether two variables are independent or merely uncorrelated is crucial. The covariance matrix can give us valuable insights, but it has its limitations‚Äîespecially when it comes to verifying true independence.</p> <h3 id="defining-independence">Defining Independence</h3> <p>In probability theory, two random variables \(X_i\) and \(X_j\) are defined to be <strong>independent</strong> if the occurrence of one has no influence on the probability distribution of the other. Mathematically, this means:</p> \[P(X_i \leq x, X_j \leq y) = P(X_i \leq x) \cdot P(X_j \leq y),\] <p>for all \(x\) and \(y\). In simpler terms, knowing the value of \(X_i\) provides no information about \(X_j\) and vice versa.</p> <h3 id="covariance-and-independence-a-subtle-distinction">Covariance and Independence: A Subtle Distinction</h3> <p>The covariance matrix tells us about the <strong>linear relationships</strong> between variables, but not necessarily about their independence. For two variables \(X_i\) and \(X_j\), the covariance \(\text{Cov}(X_i, X_j)\) is defined as:</p> \[\text{Cov}(X_i, X_j) = E[(X_i - E[X_i])(X_j - E[X_j])].\] <p>If \(X_i\) and \(X_j\) are independent, then \(\text{Cov}(X_i, X_j) = 0\). Independence implies that there is no relationship between the variables at all, which includes a lack of linear correlation. However, the reverse is not true: <strong>zero covariance does not imply independence</strong>.</p> <p>To see why, let‚Äôs explore this distinction mathematically and with an example.</p> <h3 id="proof-that-zero-covariance-does-not-imply-independence">Proof that Zero Covariance Does Not Imply Independence</h3> <p>To understand why zero covariance does not necessarily mean independence, consider two random variables \(X\) and \(Y\) that are uncorrelated (i.e., \(\text{Cov}(X, Y) = 0\)) but not independent.</p> <p>Let \(X\) be a standard normal random variable: \(X \sim N(0, 1)\). Now define \(Y = X^2\). Clearly, \(Y\) depends on \(X\); in fact, \(Y\) is entirely determined by \(X\), so \(X\) and \(Y\) are not independent.</p> <ol> <li> <p><strong>Calculating the Covariance</strong>: We‚Äôll calculate \(\text{Cov}(X, Y)\) to see if they‚Äôre uncorrelated.</p> \[\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])].\] <p>Since \(X \sim N(0, 1)\), we have \(E[X] = 0\) and \(E[Y] = E[X^2] = \text{Var}(X) = 1\). So,</p> \[\text{Cov}(X, Y) = E[X(Y - 1)] = E[X(X^2 - 1)] = E[X^3 - X].\] </li> <li> <p><strong>Expectation of Odd Moments</strong>: Given that \(X\) is normally distributed with mean 0, all odd moments of \(X\) (such as \(E[X]\) and \(E[X^3]\)) are zero. Therefore,</p> \[\text{Cov}(X, Y) = E[X^3] - E[X] = 0 - 0 = 0.\] <p>Thus, \(\text{Cov}(X, Y) = 0\), indicating that \(X\) and \(Y\) are uncorrelated. But as we know, \(Y = X^2\), which is entirely determined by \(X\). Therefore, they are not independent, even though they are uncorrelated.</p> </li> </ol> <p>This example highlights a key takeaway: <strong>uncorrelated variables are not necessarily independent</strong>. Covariance only captures linear relationships, so two variables could have a non-linear dependency and still exhibit zero covariance.</p> <h3 id="role-of-the-covariance-matrix-in-independence-analysis">Role of the Covariance Matrix in Independence Analysis</h3> <p>The covariance matrix \(\Sigma\) provides us with a way to examine linear dependencies between variables. Each off-diagonal element \(\Sigma\_{ij} = \text{Cov}(X_i, X_j)\) indicates the extent to which two variables vary together. If all off-diagonal entries of \(\Sigma\) are zero, we know that each pair of variables is <strong>uncorrelated</strong>. However, this does not guarantee independence.</p> <p>For multivariate normal distributions, however, the story is simpler. In a multivariate normal distribution, uncorrelated variables are indeed independent. Specifically, if \(X \sim N(\mu, \Sigma)\), then zero off-diagonal entries in \(\Sigma\) imply that the components of \(X\) are independent. This is a special property of the Gaussian distribution and does not hold in general.</p> <h3 id="practical-implications">Practical Implications</h3> <p>When analyzing real-world data, it‚Äôs essential to remember that zero covariance should not be mistaken for independence unless you‚Äôre specifically working with a multivariate Gaussian. In most other cases, zero covariance merely indicates a lack of linear relationship. Non-linear dependencies, which are common in fields like finance, biology, and machine learning, can often ‚Äúhide‚Äù behind zero covariance.</p> <p>In summary, the covariance matrix is a powerful tool for understanding linear relationships but is limited in detecting true independence. When we need a full assessment of independence, especially in non-Gaussian contexts, we have to look beyond \(\Sigma\) and consider additional techniques, such as analyzing joint distributions or using tests for independence.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>The mean vector serves as the ‚Äúcenter of gravity‚Äù for the data, shifting predictably under transformations and providing a concise summary of central tendencies across dimensions. Meanwhile, the covariance matrix is a lens into the data‚Äôs geometry. It encodes not only the individual variances of each variable but also the pairwise interactions that reveal whether variables rise and fall together or act independently. This matrix‚Äôs symmetry and positive semi-definiteness give it a unique role in shaping data contours and helping analysts visualize distribution shape, orientation, and spread.</p> <p>But as we‚Äôve seen, the covariance matrix also has its limits. While it efficiently captures linear relationships, it cannot capture the entire complexity of data dependencies. As our exploration of independence versus uncorrelatedness shows, true independence requires more than just a zero covariance‚Äîespecially in non-Gaussian settings where nonlinear dependencies can lie hidden. Understanding these nuances is crucial for effective data analysis. In real-world applications, whether in finance, biology, or machine learning, knowing when the covariance matrix can (and cannot) tell the full story allows us to apply these tools more accurately and creatively.</p>]]></content><author><name></name></author><category term="A Column on Gaussian Processes"/><category term="Statistics"/><category term="Stochastic Processes"/><summary type="html"><![CDATA[If the mean vector gives us a sense of location, then the covariance matrix gives us a sense of shape...]]></summary></entry><entry><title type="html">Sampling Smarter: Unlocking the Power of Latin Hypercube Sampling</title><link href="https://shuhongdai.github.io/blog/2024/Latin_Hypercube_Sampling/" rel="alternate" type="text/html" title="Sampling Smarter: Unlocking the Power of Latin Hypercube Sampling"/><published>2024-05-03T00:32:10+00:00</published><updated>2024-05-03T00:32:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Latin_Hypercube_Sampling</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Latin_Hypercube_Sampling/"><![CDATA[<hr/> <h2 id="what-is-latin-hypercube-sampling">What is Latin Hypercube Sampling?</h2> <p>Imagine you‚Äôre standing over a large chessboard, each square representing a potential outcome of some experiment. The squares are spread out, spanning the entire board, and the goal is to gather a sample of outcomes that gives you the best possible understanding of the whole chessboard‚Äînot just a corner, not just a few scattered patches, but everywhere. Latin Hypercube Sampling (LHS) is a smart way to do this, ensuring that each row and column of the board has exactly one selected square. It‚Äôs like a carefully orchestrated game where you end up with one piece in every row and every column, giving you a complete sense of the landscape. In the world of data science, this chessboard metaphor expands into multidimensional space. Instead of just two dimensions‚Äîlike the chessboard‚Äôs rows and columns‚Äîthink of an entire universe of variables, each one adding a new dimension. If you‚Äôre trying to model something complex, like how different factors affect climate or how various inputs influence the outcome of an engineering system, you need a way to efficiently sample from all these different dimensions.</p> <p>That‚Äôs where LHS shines. Unlike random sampling, which might leave some regions underrepresented while others get chosen repeatedly, LHS spreads the samples evenly across each dimension. Think of it as ensuring every corner of the universe of possibilities gets its fair share of attention. Each sample is like a probe that‚Äôs perfectly positioned to gather information from every aspect of the system, without clustering too much in any one place.</p> <p>The key idea behind LHS is balance‚Äîthe same kind of balance you see when picking one square from each row and column on the chessboard. In a higher-dimensional space, this means that every slice, or segment, of each variable range is covered, guaranteeing that the entire spectrum of potential values is represented. Whether you‚Äôre dealing with three variables or thirty, LHS keeps the sampling efficient and comprehensive, giving you a representative cross-section of all possible outcomes without redundancy or waste. This balanced sampling can be crucial in many practical applications. Imagine trying to predict the success of a new product launch by varying factors like price, marketing budget, and target audience. With LHS, you‚Äôre not just randomly throwing darts; you‚Äôre making sure that every aspect‚Äîfrom low budgets to high, from niche audiences to broad appeal‚Äîis represented in a structured way. The end result? A clearer picture of how each factor interacts, and ultimately, better insights and more informed decisions.</p> <p>LHS isn‚Äôt just about getting data‚Äîit‚Äôs about getting the right data, the kind that captures complexity without unnecessary repetition. In the chapters ahead, we‚Äôll dive deeper into how LHS works in practice, explore its benefits, and compare it to other sampling methods, illustrating why this approach has become a favorite among statisticians and engineers alike.</p> <hr/> <h2 id="why-do-we-need-latin-hypercube-sampling">Why Do We Need Latin Hypercube Sampling?</h2> <p>LHS is a powerful tool for efficiently understanding complex systems when resources are limited. In the real world, whether we are studying natural phenomena, building financial models, or designing new technology, we often face too many possible combinations of factors to analyze exhaustively. LHS provides a smart way to sample the most relevant data without having to examine every possibility.</p> <p>Think of tasting a big pot of vegetable soup. If you randomly scoop just once, you might get mostly broth or only one type of vegetable, missing the full flavor. LHS ensures that each spoonful represents every major ingredient, giving a balanced understanding of the whole pot. Similarly, LHS ensures that every part of a complex system is represented, giving us a clearer, more complete picture. This method is particularly useful in high-dimensional problems, like understanding energy consumption in a building, which depends on factors such as temperature, occupancy, lighting, and insulation. Random sampling might leave some factor combinations out, leading to gaps in understanding. LHS ensures that all relevant combinations are covered, making it easier to see the relationships without redundant or overlooked data. The efficiency of LHS is one of its biggest advantages. For instance, in modeling the effects of a new drug, with many variables like dosage and genetic markers, LHS allows for fewer, but strategically chosen, trials that still provide a comprehensive view. This means we can extract meaningful insights without the need for an overwhelming number of experiments.</p> <p>In practical applications, whether constrained by budget, time, or logistics, LHS proves invaluable. Engineers can use it to test a manageable number of car engine configurations, while scientists can employ it to study pollutant spread efficiently. LHS provides a strategic way to cover the landscape, ensuring no critical area is neglected.</p> <h2 id="how-does-latin-hypercube-sampling-work">How Does Latin Hypercube Sampling Work?</h2> <p>To truly understand LHS and appreciate its unique capabilities, we‚Äôll delve into the mathematics underpinning it. Through a sequence of structured steps, we‚Äôll see how LHS ensures efficient, balanced sampling in multidimensional space. This method systematically constructs sample points to provide a representative cross-section of complex systems, thereby capturing the essence of multidimensional data without unnecessary redundancy.</p> <h3 id="1-problem-setup">1. Problem Setup</h3> <p>Consider a \(d\)-dimensional parameter space where each parameter \(x_i\) can take values within a defined interval \([a_i, b_i]\). Our objective is to generate \(N\) distinct sample points \(x^{(1)}, x^{(2)}, \dots, x^{(N)}\), where each \(x^{(k)} = (x_1^{(k)}, x_2^{(k)}, \dots, x_d^{(k)})\) lies within this \(d\)-dimensional space. The challenge is to arrange these sample points such that they collectively cover the entire parameter space in a balanced and representative manner, avoiding clusters or sparse areas.</p> <h3 id="2-dividing-each-dimension-into-intervals">2. Dividing Each Dimension into Intervals</h3> <p>For each dimension \(x_i\), we begin by dividing the interval \([a_i, b_i]\) into \(N\) non-overlapping subintervals. Formally, we can define these intervals as:</p> \[[a_i, b_i] = \bigcup_{j=1}^N \left[ a_i + \frac{j-1}{N} (b_i - a_i), a_i + \frac{j}{N} (b_i - a_i) \right]\] <p>where each subinterval \(I\_{i,j}\) for dimension \(x_i\) is given by:</p> \[I_{i,j} = \left[ a_i + \frac{j-1}{N} (b_i - a_i), a_i + \frac{j}{N} (b_i - a_i) \right]\] <p>The length of each interval, \(\Delta x_i\), is consistent across \(x_i\):</p> \[\Delta x_i = \frac{b_i - a_i}{N}\] <p>This systematic partitioning ensures that each dimension \(x_i\) is split into \(N\) equal sections, which lays the groundwork for comprehensive sampling.</p> <h3 id="3-random-sampling-within-each-interval">3. Random Sampling within Each Interval</h3> <p>Within each subinterval \(I*{i,j}\) of dimension \(x_i\), we randomly select a point \(x*{i,j}\). This point can be represented mathematically as:</p> \[x_{i, j} = a_i + \frac{j-1}{N} (b_i - a_i) + U_{i,j} \cdot \Delta x_i\] <p>where \(U*{i,j} \sim \text{Uniform}(0,1)\) represents a uniformly distributed random variable within \([0,1]\). This formulation ensures that \(x*{i,j}\) falls randomly within the subinterval \(I\_{i,j}\), providing a sample point that respects the interval boundaries.</p> <h3 id="4-constructing-multidimensional-sample-points">4. Constructing Multidimensional Sample Points</h3> <p>To generate the complete set of \(N\) sample points in \(d\)-dimensional space, we must combine these dimension-specific sample points. This is done by assigning each sample from \(x_i\) to a unique subinterval in each dimension using a random permutation function \(\pi_i\). The permutation \(\pi_i\) for each dimension \(x_i\) randomly orders the indices \(\{1, 2, \dots, N\}\) such that each subinterval is represented exactly once.</p> <p>The \(k\)-th sample point in \(d\)-dimensional space is thus constructed as:</p> \[x^{(k)} = \left( x_{1, \pi_1(k)}, x_{2, \pi_2(k)}, \dots, x_{d, \pi_d(k)} \right)\] <p>This arrangement guarantees that each sample point spans a unique combination of subintervals across all dimensions, achieving an even spread and complete coverage of the parameter space.</p> <h3 id="5-proof-of-marginal-uniformity">5. Proof of Marginal Uniformity</h3> <p>One of the core strengths of LHS lies in its marginal uniformity. This property ensures that the samples are uniformly distributed along each individual dimension \(x_i\), even as they span multiple dimensions. Let‚Äôs delve into a formal explanation:</p> <ol> <li>Each dimension \(x*i\) is divided into \(N\) intervals \(I*{i,1}, I*{i,2}, \dots, I*{i,N}\), with a single sample \(x\_{i,j}\) taken from each interval.</li> <li>Each sample \(x\_{i,j}\) is drawn uniformly from within its interval, meaning that the probability of sampling any particular region within \([a_i, b_i]\) is evenly distributed.</li> <li>Consequently, for each dimension \(x_i\), the probability distribution of the sample points across intervals is uniform, with each subinterval receiving exactly one sample point.</li> </ol> <p>This uniformity across intervals ensures that the samples are well-distributed along each dimension, resulting in comprehensive and unbiased coverage of the space.</p> <h3 id="6-variance-reduction-in-latin-hypercube-sampling">6. Variance Reduction in Latin Hypercube Sampling</h3> <p>An essential advantage of LHS is its ability to reduce variance in the estimated outcomes, particularly when compared to simple random sampling (SRS). This reduction in variance leads to more accurate estimations with fewer samples. To illustrate this, we consider the variance of the sample mean \(\hat{f}\) when estimating the expectation \(\mathbb{E}[f(x)]\) for a function \(f(x)\) over our \(d\)-dimensional space.</p> <p>Given \(f(x)\), the sample mean \(\hat{f}\) based on \(N\) samples is:</p> \[\hat{f} = \frac{1}{N} \sum_{k=1}^N f(x^{(k)})\] <p>We can express the variance of \(\hat{f}\) as:</p> \[\text{Var}(\hat{f}) = \frac{1}{N^2} \sum_{k=1}^N \sum_{l=1}^N \text{Cov}(f(x^{(k)}), f(x^{(l)}))\] <p>In LHS, because each sample point \(f(x^{(k)})\) is drawn independently across distinct subintervals, the covariance \(\text{Cov}(f(x^{(k)}), f(x^{(l)}))\) for \(k \neq l\) is zero. This simplifies the variance expression to:</p> \[\text{Var}(\hat{f}) = \frac{1}{N^2} \sum_{k=1}^N \text{Var}(f(x^{(k)}))\] <h3 id="result-variance-reduction-in-lhs">Result: Variance Reduction in LHS</h3> <p>Since each sample \(f(x^{(k)})\) is uniformly distributed across the intervals, the variance in LHS is inherently reduced compared to simple random sampling. In SRS, the variance is:</p> \[\text{Var}_{\text{SRS}}(\hat{f}) = \frac{\sigma^2}{N}\] <p>where \(\sigma^2\) represents the population variance of \(f(x)\). In contrast, LHS ensures that each dimension‚Äôs intervals are well-covered, which results in:</p> \[\text{Var}_{\text{LHS}}(\hat{f}) \leq \frac{\sigma^2}{N}\] <p>This inequality demonstrates that LHS consistently achieves lower variance than simple random sampling, making it a more efficient and precise sampling method.</p> <hr/> <h2 id="example-optimizing-a-drug-dosage-experiment-with-lhs">Example: Optimizing a Drug Dosage Experiment with LHS</h2> <p>To truly appreciate how LHS works, let‚Äôs dive into a practical example in drug dosage research. Suppose we‚Äôre conducting a study to understand how different drug doses, patient ages, and body weights affect blood concentration levels. Instead of relying on simple random sampling, which might overlook certain important combinations, LHS allows us to achieve balanced sampling across multiple dimensions, making the experiment both efficient and insightful.</p> <h3 id="experiment-setup-and-goals">Experiment Setup and Goals</h3> <p>In this study, we have three key variables:</p> <ol> <li><strong>Dosage (\(x_1\))</strong>: Ranges from \([50, 200]\) mg.</li> <li><strong>Age (\(x_2\))</strong>: Ranges from \([20, 80]\) years.</li> <li><strong>Weight (\(x_3\))</strong>: Ranges from \([50, 100]\) kg.</li> </ol> <p>Our goal is to generate \(N = 5\) sample points that represent this multi-dimensional space well, ensuring each factor‚Äôs range is adequately covered. This approach will allow us to identify patterns without conducting an exhaustive set of tests.</p> <h3 id="step-1-dividing-each-dimension-into-intervals">Step 1: Dividing Each Dimension into Intervals</h3> <p>With \(N = 5\) samples, we divide each variable‚Äôs range into five non-overlapping intervals:</p> <ul> <li> <p><strong>Dosage (\(x_1\))</strong>: The range \([50, 200]\) mg is divided into five intervals of length \(\Delta x_1 = \frac{200 - 50}{5} = 30\) mg:</p> \[[50, 80], [80, 110], [110, 140], [140, 170], [170, 200]\] </li> <li> <p><strong>Age (\(x_2\))</strong>: The range \([20, 80]\) years is divided into five intervals of length \(\Delta x_2 = \frac{80 - 20}{5} = 12\) years:</p> \[[20, 32], [32, 44], [44, 56], [56, 68], [68, 80]\] </li> <li> <p><strong>Weight (\(x_3\))</strong>: The range \([50, 100]\) kg is divided into five intervals of length \(\Delta x_3 = \frac{100 - 50}{5} = 10\) kg:</p> \[[50, 60], [60, 70], [70, 80], [80, 90], [90, 100]\] </li> </ul> <p>This structured partitioning provides a foundation for balanced sampling within each dimension.</p> <h3 id="step-2-random-sampling-within-each-interval">Step 2: Random Sampling within Each Interval</h3> <p>Next, we randomly select a sample point within each subinterval. For example, within the first dosage interval \([50, 80]\), we select a random point \(x\_{1,1}\). We do the same for other intervals, ensuring a point is chosen within each range. This process guarantees that every segment of each variable‚Äôs range contributes to our sample set.</p> <p>Let‚Äôs say we get the following randomly chosen points for each dimension:</p> <ul> <li><strong>Dosage (\(x_1\))</strong>: 63, 94, 125, 157, 189</li> <li><strong>Age (\(x_2\))</strong>: 24, 39, 50, 63, 76</li> <li><strong>Weight (\(x_3\))</strong>: 52, 66, 78, 85, 93</li> </ul> <h3 id="step-3-constructing-multi-dimensional-sample-points">Step 3: Constructing Multi-Dimensional Sample Points</h3> <p>To create our final five sample points in three-dimensional space, we combine these points by applying random permutations to each dimension. For example, we can randomly shuffle each dimension‚Äôs values, resulting in the following combinations:</p> <ul> <li><strong>Dosage permutation</strong>: 125, 63, 189, 94, 157</li> <li><strong>Age permutation</strong>: 63, 24, 76, 39, 50</li> <li><strong>Weight permutation</strong>: 66, 78, 93, 52, 85</li> </ul> <p>Thus, our final sample points are:</p> \[(125, 63, 66)\] \[(63, 24, 78)\] \[(189, 76, 93)\] \[(94, 39, 52)\] \[(157, 50, 85)\] <h3 id="step-4-evaluating-the-balance-of-our-sample-set">Step 4: Evaluating the Balance of Our Sample Set</h3> <p>This arrangement ensures that each variable is evenly represented across its range. Every sample point combines different segments of each dimension, avoiding excessive clustering in any one area. Compared to simple random sampling, LHS guarantees a well-rounded representation of our parameter space, which is critical for accurately studying relationships between variables.</p> <h3 id="analyzing-the-results">Analyzing the Results</h3> <p>In this example, LHS allows us to effectively sample a three-dimensional space, covering a comprehensive range of dosage, age, and weight combinations with only five samples. This balanced approach gives us a holistic view of how these variables interact, helping researchers understand dosage effects across diverse patient profiles while minimizing experimental overhead.</p> <hr/> <h2 id="real-world-applications-of-latin-hypercube-sampling">Real-World Applications of Latin Hypercube Sampling</h2> <p>LHS may sound like a tool for abstract math, but its practical impact is very real. Across engineering, environmental science, finance, healthcare, and energy modeling, LHS is a quietly transformative technique, allowing researchers to gain clear insights without drowning in data. Imagine trying to paint a detailed landscape but having only a handful of colors and brushstrokes to work with‚ÄîLHS is like using those few strokes in exactly the right places to capture the whole scene with remarkable accuracy. Here‚Äôs how LHS works its magic across different fields.</p> <p>In engineering, for example, LHS helps streamline design and testing. Think of a car manufacturer simulating thousands of design combinations to improve performance under diverse conditions like temperature, load, and speed. Instead of running endless trials, LHS allows engineers to test a fraction of those designs in a structured way, covering the full range of conditions without redundancy. The result? They get the insights needed to enhance performance with far fewer tests, saving time and resources without sacrificing precision.</p> <p>Environmental science is another area where LHS is indispensable. Imagine trying to model how pollutants might spread across a city. Factors like wind speed, direction, and geographic features all interact to shape pollution patterns. With LHS, researchers can simulate these combinations in a balanced way, ensuring that different scenarios are well represented without oversampling any particular case. This approach not only sharpens predictions but also provides critical insights for health policies and urban planning‚Äîessential when resources are tight, but accuracy is crucial.</p> <p>Finance is yet another domain where LHS proves its worth, especially in risk analysis. Financial analysts need to understand how a portfolio might behave across a range of market conditions‚Äîinterest rates, currency fluctuations, inflation, and so on. Rather than randomly selecting scenarios, LHS strategically samples across these factors, capturing both typical and extreme market conditions. This balanced approach provides a clearer risk profile and enables better-informed investment decisions, all while keeping data requirements manageable.</p> <p>In healthcare, LHS is a valuable ally in clinical trials and biomedical research. Imagine testing a new drug across diverse patient characteristics like age, genetic profile, and lifestyle. Testing every combination is impossible, so LHS helps researchers select a representative set of patient scenarios. By covering each variable without redundancy, LHS ensures that no group is overlooked, giving a comprehensive view of the drug‚Äôs impact across different demographics, often revealing critical insights early on.</p> <p>Even energy modeling for buildings benefits from LHS. When designing sustainable buildings, architects need to know how energy usage changes based on insulation, occupancy, weather, and other factors. LHS allows for sampling across these variables in a way that efficiently covers the range of possible conditions. By using LHS to model these combinations, analysts can optimize energy efficiency without testing every single scenario, resulting in smarter, greener design choices.</p> <p>In each of these fields, LHS is the secret ingredient that transforms limited samples into broad, balanced insights, ensuring that complex systems are thoroughly represented without overwhelming resources. It‚Äôs a perfect example of sampling smarter, not harder, proving that sometimes, a carefully chosen subset can be just as powerful as an exhaustive dataset. As data-driven decisions become more central to progress, LHS is one tool that ensures those decisions are both efficient and well-informed.</p> <hr/> <h2 id="limitations-of-latin-hypercube-sampling">Limitations of Latin Hypercube Sampling</h2> <p>While LHS is a remarkably efficient method for capturing multidimensional data, it has limitations, particularly when dealing with high-dimensional spaces and dynamic systems. These challenges underscore that even advanced sampling techniques like LHS must sometimes be augmented or adapted to maintain efficiency and accuracy.</p> <h3 id="computational-demands-in-high-dimensions">Computational Demands in High Dimensions</h3> <p>One of the primary challenges with LHS arises in high-dimensional spaces. As the number of dimensions \(d\) grows, the complexity of generating \(N\) samples with balanced coverage across each dimension increases significantly. In lower-dimensional spaces, LHS achieves a clear advantage by ensuring that every interval in each dimension is represented. However, as \(d\) rises, this method begins to experience what‚Äôs known as the ‚Äúcurse of dimensionality,‚Äù where the sample space becomes exponentially large.</p> <p>To understand this more formally, consider that LHS divides each dimension‚Äôs interval into \(N\) equal parts, requiring \(N^d\) unique combinations to ensure complete coverage in the \(d\)-dimensional space. However, practical constraints often limit the total number of samples \(N\), leading to fewer possible combinations in the high-dimensional setting, which means that not all regions of the space are sampled as evenly. When the sampling coverage is incomplete, the variance reduction properties of LHS also diminish. The variance of a sample mean \(\hat{f}\) from LHS in high-dimensional spaces approximates as:</p> \[\text{Var}_{\text{LHS}}(\hat{f}) \approx \frac{\sigma^2}{N} \cdot \left( 1 + \frac{d - 1}{N} \right)\] <p>where \(\sigma^2\) is the population variance. This variance formula highlights that as \(d\) approaches \(N\), variance grows, and LHS‚Äôs advantage over simple random sampling (SRS) begins to diminish. In high-dimensional scenarios, achieving a representative sample with LHS can require exponentially more points to maintain the same precision, potentially making it less efficient than anticipated.</p> <h3 id="challenges-with-dynamic-systems">Challenges with Dynamic Systems</h3> <p>Another notable limitation of LHS lies in its static nature, which can be less effective for systems where variables have time-dependent relationships or feedback loops. LHS operates under the assumption that each variable can be sampled independently within its interval, which is reasonable for many static or quasi-static systems. However, in dynamic systems‚Äîsuch as those seen in financial markets, climate models, or real-time simulations‚Äîdependencies between variables often evolve over time, meaning the state of one variable may directly influence the others.</p> <p>For example, in a climate model where temperature, humidity, and wind speed are interdependent and change over time, simply sampling each dimension independently may miss critical interdependencies. Mathematically, if we denote a system state at time \(t\) as \(\mathbf{x}(t) = (x_1(t), x_2(t), \dots, x_d(t))\), then dynamic relationships between variables \(x_i(t)\) might require joint distribution sampling, something that LHS in its classical form doesn‚Äôt inherently accommodate.</p> <p>For dynamic models, we would ideally sample from the conditional distributions \(P(x*i(t) \vert x*{-i}(t))\), where \(x\_{-i}(t)\) represents the set of all other variables at time \(t\). However, traditional LHS treats each dimension independently, lacking the ability to conditionally update samples based on evolving states of other variables. As a result, alternative sampling techniques‚Äîsuch as sequential Monte Carlo (SMC) or particle filtering, which adapt to these dependencies‚Äîare often more appropriate for dynamic systems.</p> <h3 id="addressing-dependencies-and-dimensionality-constraints">Addressing Dependencies and Dimensionality Constraints</h3> <p>One way to address these limitations is by hybridizing LHS with other sampling techniques. For high-dimensional spaces, combining LHS with stratified sampling or Sobol sequences can mitigate the curse of dimensionality, ensuring better coverage in each dimension without requiring an impractical number of samples. In dynamic systems, integrating LHS with adaptive sampling techniques, where the sample distribution updates based on real-time data, may offer a way to retain the efficiency of LHS while accommodating evolving dependencies.</p> <hr/> <h2 id="demo">Demo</h2> <p>Here‚Äôs a demonstration of LHS across different dimensions. In 2D, we see LHS distributing sample points evenly across the grid, ensuring each part of the space is represented. In 3D, this principle extends gracefully, capturing a well-balanced spread across all three dimensions. Finally, in 4D, we employ dimensionality reduction to visualize the sampling density, revealing how LHS continues to provide comprehensive coverage even in complex, multi-dimensional settings.</p> <blockquote> <p>Updated on June 22, 2024: Additionally, the PCA method used for dimensionality reduction in 4D sampling is detailed with examples in <a href="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/#pca-extracting-linear-patterns">our latest blog post</a>.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-05-03/2d-480.webp 480w,/assets/posts_img/2024-05-03/2d-800.webp 800w,/assets/posts_img/2024-05-03/2d-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-05-03/2d.png" class="img-fluid" width="600" height="400" alt="2d" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-05-03/3d-480.webp 480w,/assets/posts_img/2024-05-03/3d-800.webp 800w,/assets/posts_img/2024-05-03/3d-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-05-03/3d.png" class="img-fluid" width="600" height="400" alt="3d" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-05-03/4d-480.webp 480w,/assets/posts_img/2024-05-03/4d-800.webp 800w,/assets/posts_img/2024-05-03/4d-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-05-03/4d.png" class="img-fluid" width="600" height="400" alt="4d" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">qmc</span>

<span class="c1"># Set global Seaborn style for consistent visualization
</span><span class="n">sns</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="sh">"</span><span class="s">whitegrid</span><span class="sh">"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="sh">"</span><span class="s">muted</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Number of samples and different dimensions to illustrate LHS sampling
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">dimensions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="c1"># Function to generate Latin Hypercube Sampling points
</span><span class="k">def</span> <span class="nf">generate_lhs_samples</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">qmc</span><span class="p">.</span><span class="nc">LatinHypercube</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">.</span><span class="nf">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sample</span>

<span class="c1"># Function to reduce dimensions using PCA (for high-dimensional data)
</span><span class="k">def</span> <span class="nf">reduce_dimension</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Plotting 2D LHS samples
</span><span class="k">def</span> <span class="nf">plot_2d_lhs</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">2D LHS Sampling</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 1</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Plotting 3D LHS samples with a 3D perspective
</span><span class="k">def</span> <span class="nf">plot_3d_lhs</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">3D LHS Sampling</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="sh">'</span><span class="s">3d</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># Create 3D projection
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 1</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 3</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Plotting 4D LHS samples reduced to 2D with density map
</span><span class="k">def</span> <span class="nf">plot_4d_lhs_with_density</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">4D LHS Sampling (PCA Reduced)</span><span class="sh">"</span><span class="p">):</span>
    <span class="c1"># Apply PCA to reduce the 4D data to 2D
</span>    <span class="n">reduced_samples</span> <span class="o">=</span> <span class="nf">reduce_dimension</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">reduced_samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="c1"># Plot density heatmap using kdeplot
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Blues</span><span class="sh">"</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="c1"># Overlay the scatter plot for sampled points
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">PCA Dimension 1</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">PCA Dimension 2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Loop through the specified dimensions and generate the corresponding plots
</span><span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">dimensions</span><span class="p">:</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="nf">generate_lhs_samples</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Plot for 2D LHS sampling
</span>        <span class="nf">plot_2d_lhs</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">2D LHS Sampling</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Plot for 3D LHS sampling with 3D view
</span>        <span class="nf">plot_3d_lhs</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">3D LHS Sampling</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="c1"># Plot for 4D LHS sampling after PCA reduction with density visualization
</span>        <span class="nf">plot_4d_lhs_with_density</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">4D LHS Sampling (PCA Reduced)</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="wrapping-it-all-up">Wrapping It All Up</h2> <p>LHS is, without a doubt, an impressive technique. It‚Äôs one of those methods that shows just how much smarter sampling can be than a simple ‚Äúgrab a few random points and hope for the best‚Äù approach. By strategically covering each dimension and ensuring no corner of our data space is left unexplored, LHS brings precision and balance to the chaotic world of complex systems. Whether you‚Äôre testing car engines, predicting financial risk, modeling climate change, or designing clinical trials, LHS has a unique knack for extracting the most insight with the least effort. It‚Äôs efficiency and elegance wrapped into one neat package.</p> <p>That said, LHS is not a cure-all. As we explored, it starts to stumble in high dimensions, where the dreaded curse of dimensionality can make even the most elegant sampling methods feel a bit sluggish. And when variables are in flux, like in dynamic systems, LHS‚Äôs simple structure can‚Äôt quite capture the dance of interdependencies over time. In those cases, it‚Äôs like trying to catch a shadow‚Äîby the time you sample one point, the system has already changed. But here‚Äôs the fun part about a method like LHS: it has this air of adaptability. While it might not suit every situation perfectly, it can be hybridized, adjusted, and even reinvented to suit new needs. I think that‚Äôs why it‚Äôs so appealing to both engineers and data scientists alike.</p>]]></content><author><name></name></author><category term="Bits and Pieces"/><category term="Statistics"/><category term="Probability Theory"/><summary type="html"><![CDATA[Unlike random sampling, which might leave some regions underrepresented while others get chosen repeatedly...]]></summary></entry><entry><title type="html">Problem ‚Ö¢: Group Theory</title><link href="https://shuhongdai.github.io/blog/2024/EC525_3/" rel="alternate" type="text/html" title="Problem ‚Ö¢: Group Theory"/><published>2024-03-22T00:00:00+00:00</published><updated>2024-03-22T00:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/EC525_3</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/EC525_3/"><![CDATA[<h2 id="given-conditions">Given Conditions</h2> <p>A group is a set $G$ together with a binary operation $\circ$. The binary operation $\circ$ takes two elements of $G$, and returns an element of $G$. We write $x \circ y$ to indicate the returned value of $\circ$ on input $x$ and $y$. Note that the ordering is important: $x \circ y$ might not be equal to $y \circ x$. If you like, you can instead think of a function $b : G \times G \rightarrow G$ where $x \circ y$ is just shorthand for $b(x,y)$. The binary operation must satisfy:</p> <ul> <li>$(x \circ y) \circ z = x \circ (y \circ z)$ for all $x, y, z \in G$.</li> <li>There must be some element $i \in G$ such that $x \circ i = i \circ x = x$ for all $x \in G$.</li> <li>For each $x \in G$, there is an element $x^{-1}$ called the inverse of $x$ such that $x \circ x^{-1} = x^{-1} \circ x = i$.</li> </ul> <p><strong>Quick Navigation</strong> ‚¨áÔ∏è</p> <ul> <li> <p><a href="https://shuhongdai.github.io/blog/2023/EC525_0/">Preface: Motivation and Overview</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2023/EC525_1/">Problem ‚Ö†: Vector Spaces</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_2/">Problem ‚Ö°: Linear Transformations</a></p> </li> <li> <p><strong>Problem ‚Ö¢: Group Theory</strong> (You are currently browsing this post)</p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_4/">Problem ‚Ö£: Dual Spaces and Functional Analysis</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_5/">Problem ‚Ö§: Infinite Sequences and Combinatorial Principles</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_6/">Problem ‚Ö•: Set Theory and Cardinality</a></p> </li> <li> <p>Conclusion</p> </li> </ul> <hr/> <h2 id="a-consider-a-set-g-with-two-elements-a-and-b-provide-a-binary-operation-circ-that-turns-ginto-a-group">(a) Consider a set $G$ with two elements $a$ and $b$. Provide a binary operation $\circ$ that turns $G$into a group.</h2> <p><strong>Proposition 3.a.1.</strong> <em>Let $G = {a, b}$. Define a binary operation $\circ : G \times G \to G$ by prescribing the following values:</em></p> \[a \circ a = a, \quad a \circ b = b, \quad b \circ a = b, \quad b \circ b = a.\] <p><em>Then the algebraic structure $(G, \circ)$ is a group.</em></p> <p><em>Proof.</em> Consider the set $G = {a, b}$ and the operation $\circ : G \times G \to G$ defined by</p> \[\forall x,y \in G: x \circ y = \begin{cases} a, &amp; \text{if } (x,y) = (a,a) \text{ or } (b,b), \\[6pt] b, &amp; \text{if } (x,y) = (a,b) \text{ or } (b,a). \end{cases}\] <p>Closure: By construction of the operation $\circ$, for each ordered pair $(x,y) \in G \times G$, the element $ x \circ y $ is defined to be either $ a $ or $ b $, both of which lie in $ G $. Hence:</p> \[\forall x,y \in G: x \circ y \in G.\] <p>Associativity: To establish that $\circ$ is associative, it suffices to verify that for all $ x,y,z \in G $, the equality</p> \[(x \circ y) \circ z = x \circ (y \circ z)\] <p>holds. Since $\vert G \vert =2$, one must verify all possible combinations of $(x,y,z)$. The tuples in $G \times G \times G$ are $(a,a,a), (a,a,b), (a,b,a), (a,b,b), (b,a,a), (b,a,b), (b,b,a), (b,b,b)$.</p> <p>Consider each case in turn:</p> <ul> <li> <p>For $(x,y,z) = (a,a,a)$:</p> \[(a \circ a) \circ a = a \circ a = a, \quad a \circ (a \circ a) = a \circ a = a.\] </li> <li> <p>For $(x,y,z) = (a,a,b)$:</p> \[(a \circ a) \circ b = a \circ b = b, \quad a \circ (a \circ b) = a \circ b = b.\] </li> <li> <p>For $(x,y,z) = (a,b,a)$:</p> \[(a \circ b) \circ a = b \circ a = b, \quad a \circ (b \circ a) = a \circ b = b.\] </li> <li> <p>For $(x,y,z) = (a,b,b)$:</p> \[(a \circ b) \circ b = b \circ b = a, \quad a \circ (b \circ b) = a \circ a = a.\] </li> <li> <p>For $(x,y,z) = (b,a,a)$:</p> \[(b \circ a) \circ a = b \circ a = b, \quad b \circ (a \circ a) = b \circ a = b.\] </li> <li> <p>For $(x,y,z) = (b,a,b)$:</p> \[(b \circ a) \circ b = b \circ b = a, \quad b \circ (a \circ b) = b \circ b = a.\] </li> <li> <p>For $(x,y,z) = (b,b,a)$:</p> \[(b \circ b) \circ a = a \circ a = a, \quad b \circ (b \circ a) = b \circ b = a.\] </li> <li> <p>For $(x,y,z) = (b,b,b)$:</p> \[(b \circ b) \circ b = a \circ b = b, \quad b \circ (b \circ b) = b \circ a = b.\] </li> </ul> <p>In each of these eight cases, the left-hand side $(x \circ y) \circ z$ equals the right-hand side $x \circ (y \circ z)$. Therefore, associativity holds:</p> \[\forall x,y,z \in G: (x \circ y) \circ z = x \circ (y \circ z).\] <p>Existence of an Identity Element: An identity element $ i \in G $ must satisfy</p> \[\forall x \in G: x \circ i = x \quad \text{and} \quad i \circ x = x.\] <p>By inspection of the defining relations of $\circ$, the element $ a $ behaves as an identity:</p> \[a \circ a = a, \quad a \circ b = b, \quad b \circ a = b.\] <p>Hence:</p> \[\forall x \in G: x \circ a = x \quad \text{and} \quad a \circ x = x.\] <p>Thus, $ a $ serves as the identity element $ i $ of $(G, \circ)$.</p> <p>Existence of Inverses: For each $ x \in G $, an inverse $ x^{-1} \in G $ must satisfy</p> \[x \circ x^{-1} = i \quad \text{and} \quad x^{-1} \circ x = i.\] <p>Since the identity element $ i = a $ has been identified, one must determine inverses with respect to $ a $:</p> <ul> <li> <p>For $ x = a $:</p> \[a \circ a = a = i, \quad a \circ a = i.\] <p>Thus, $ a^{-1} = a $.</p> </li> <li> <p>For $ x = b $:</p> \[b \circ b = a = i, \quad b \circ b = i.\] <p>Thus, $ b^{-1} = b $.</p> </li> </ul> <p>Therefore:</p> \[a^{-1} = a, \quad b^{-1} = b.\] <p>Since all four group axioms‚Äîclosure, associativity, existence of an identity, and existence of inverses‚Äîhave been rigorously verified, it follows that $(G, \circ)$ is indeed a group. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentary</strong></p> <p>The constructed group $(G,\circ)$ is isomorphic to the cyclic group of order 2, often denoted by $\mathbb{Z}/2\mathbb{Z}$, under addition modulo 2.</p> </blockquote> <hr/> <hr/>]]></content><author><name></name></author><category term="A Commentary of the Pre-Practice for Boston University&apos;s EC525 Course"/><category term="Mathematics"/><summary type="html"><![CDATA[Given Conditions]]></summary></entry><entry><title type="html">Problem ‚Ö°: Linear Transformations</title><link href="https://shuhongdai.github.io/blog/2024/EC525_2/" rel="alternate" type="text/html" title="Problem ‚Ö°: Linear Transformations"/><published>2024-02-26T00:00:00+00:00</published><updated>2024-02-26T00:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/EC525_2</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/EC525_2/"><![CDATA[<h2 id="given-conditions">Given Conditions</h2> <p>If $V$ and $W$ are two different vector spaces, a \textit{linear map} $L$ from $V$ to $W$ is a function $L:V\to W$ such that</p> <ul> <li>$L(v+v^{\prime})=L(v)+L(v^{\prime})$ for all $v,v^{\prime}\in V$.</li> <li>$L(s\cdot v)=s\cdot L(v)$ for all $s\in\mathbb{R}$, $v\in V$.</li> </ul> <p><strong>Quick Navigation</strong> ‚¨áÔ∏è</p> <ul> <li> <p><a href="https://shuhongdai.github.io/blog/2023/EC525_0/">Preface: Motivation and Overview</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2023/EC525_1/">Problem ‚Ö†: Vector Spaces</a></p> </li> <li> <p><strong>Problem ‚Ö°: Linear Transformations</strong> (You are currently browsing this post)</p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_3/">Problem ‚Ö¢: Group Theory</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_4/">Problem ‚Ö£: Dual Spaces and Functional Analysis</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_5/">Problem ‚Ö§: Infinite Sequences and Combinatorial Principles</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_6/">Problem ‚Ö•: Set Theory and Cardinality</a></p> </li> <li> <p>Conclusion</p> </li> </ul> <hr/> <h2 id="a-show-that-if-lvto-w-and-hwto-x-are-both-linear-maps-then-the-composition-hcirc-lvto-x-is-a-linear-map">Ôºàa) Show that if $L:V\to W$ and $H:W\to X$ are both linear maps, then the composition $H\circ L:V\to X$ is a linear map.</h2> <p><strong>Proposition 2.a.1.</strong> <em>Let $V, W, X$ be vector spaces over a field $\mathbb{R}$, and let $L: V \to W$ and $H: W \to X$ be linear maps. Then the composition $H \circ L: V \to X$ defined by $(H \circ L)(v) = H(L(v))$ for all $v \in V$ is linear.</em></p> <p><em>Proof.</em> By hypothesis, $L: V \to W$ is linear, hence for all $v_{1}, v_{2} \in V$ and all scalars $s \in \mathbb{R}$, the following identities hold:</p> \[L(v_{1} + v_{2}) = L(v_{1}) + L(v_{2}) \quad\text{and}\quad L(s \cdot v_{1}) = s \cdot L(v_{1}).\] <p>Similarly, since $H: W \to X$ is linear, for all $w_{1}, w_{2} \in W$ and all $t \in \mathbb{R}$, one has:</p> \[H(w_{1} + w_{2}) = H(w_{1}) + H(w_{2}) \quad\text{and}\quad H(t \cdot w_{1}) = t \cdot H(w_{1}).\] <p>To establish linearity of the composition $H \circ L: V \to X$, one must verify that for all $v_{1}, v_{2} \in V$ and all $r \in \mathbb{R}$, the following equalities hold:</p> \[(H \circ L)(v_{1} + v_{2}) = (H \circ L)(v_{1}) + (H \circ L)(v_{2}) \quad\text{and}\quad\] \[(H \circ L)(r \cdot v_{1}) = r \cdot (H \circ L)(v_{1}).\] <p>Consider arbitrary vectors $v_{1}, v_{2} \in V$. Since $H \circ L$ is defined by composition, one has</p> \[(H \circ L)(v_{1} + v_{2}) = H(L(v_{1} + v_{2})).\] <p>By linearity of $L$,</p> \[L(v_{1} + v_{2}) = L(v_{1}) + L(v_{2}),\] <p>thus</p> \[(H \circ L)(v_{1} + v_{2}) = H(L(v_{1}) + L(v_{2})).\] <p>Since $H$ is linear, it preserves addition, so</p> \[H(L(v_{1}) + L(v_{2})) = H(L(v_{1})) + H(L(v_{2})).\] <p>Substitute back the definition of composition:</p> \[H(L(v_{1})) + H(L(v_{2})) = (H \circ L)(v_{1}) + (H \circ L)(v_{2}).\] <p>Hence we have shown that for all $v_{1}, v_{2} \in V$,</p> \[(H \circ L)(v_{1} + v_{2}) = (H \circ L)(v_{1}) + (H \circ L)(v_{2}).\] <p>Next, consider an arbitrary vector $v \in V$ and an arbitrary scalar $r \in \mathbb{R}$. By the definition of composition,</p> \[(H \circ L)(r \cdot v) = H(L(r \cdot v)).\] <p>Since $L$ is linear, it respects scalar multiplication, yielding</p> \[L(r \cdot v) = r \cdot L(v).\] <p>Therefore,</p> \[(H \circ L)(r \cdot v) = H(r \cdot L(v)).\] <p>Because $H$ is linear, it also respects scalar multiplication, implying</p> \[H(r \cdot L(v)) = r \cdot H(L(v)).\] <p>Rewriting in terms of composition, this becomes</p> \[r \cdot H(L(v)) = r \cdot (H \circ L)(v).\] <p>Hence, for all $r \in \mathbb{R}$ and $v \in V$,</p> \[(H \circ L)(r \cdot v) = r \cdot (H \circ L)(v).\] <p>Both of the defining properties of linear maps, namely the preservation of vector addition and scalar multiplication, have been verified for the map $H \circ L$. Consequently, $H \circ L$ is linear. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentary</strong></p> <p>The essential elements of the proof rest on the axioms of linearity, which are stable under composition. Thus, the space of linear maps between vector spaces is closed under composition.</p> </blockquote> <hr/> <h2 id="b-show-vec0_vthat-if-l-v-to-w-is-a-linear-map-then-lvec0_v--vec0_w-where-vec0_v-indicates-the-additive-identity-in-v-and-vec0_w-indicates-the-additive-identity-in-w">(b) Show $\vec{0}_v$that if $L: V \to W$ is a linear map, then $L(\vec{0}_{V}) = \vec{0}_{W}$, where $\vec{0}_{V}$ indicates the additive identity in $V$ and $\vec{0}_{W}$ indicates the additive identity in $W$.</h2> <p><strong>Proposition 2.b.1.</strong> <em>Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$, and let $L: V \to W$ be a linear map. Then $L(\vec{0}_V) = \vec{0}_W$, where $\vec{0}_V$ and $\vec{0}_W$ denote the additive identities in $V$ and $W$, respectively.</em></p> <p><em>Proof.</em> By the definition of a linear map, $L$ satisfies the following properties for all vectors $v, v‚Äô \in V$ and all scalars $s \in \mathbb{R}$:</p> \[L(v + v') = L(v) + L(v') \quad \text{and} \quad L(s \cdot v) = s \cdot L(v).\] <p>In particular, consider the vector $\vec{0}_V$, the additive identity in $V$. By the property of additive identities in vector spaces, for any vector $v \in V$, we have:</p> \[v + \vec{0}_V = v.\] <p>Applying the linear map $L$ to both sides of this equation yields: \(L(v + \vec{0}_V) = L(v).\) Using the additivity of $L$, the left-hand side can be expressed as:</p> \[L(v + \vec{0}_V) = L(v) + L(\vec{0}_V).\] <p>Thus, we have:</p> \[L(v) + L(\vec{0}_V) = L(v).\] <p>To isolate $L(\vec{0}_V)$, subtract $L(v)$ from both sides:</p> \[L(v) + L(\vec{0}_V) - L(v) = L(v) - L(v).\] <p>Simplifying both sides, we obtain:</p> \[L(\vec{0}_V) = \vec{0}_W,\] <p>where $\vec{0}_W$ is the additive identity in $W$, since the only element in $W$ that satisfies $w + \vec{0}_W = w$ for all $w \in W$ is $\vec{0}_W$ itself.</p> <p>Alternatively, one may employ the homogeneity property of linear maps. Consider the scalar multiplication by zero, which yields:</p> \[L(0 \cdot v) = 0 \cdot L(v).\] <p>Since $0 \cdot v = \vec{0}_V$ for any $v \in V$, and $0 \cdot L(v) = \vec{0}_W$ in $W$, it follows that:</p> \[L(\vec{0}_V) = \vec{0}_W.\] <p>$\textbf{Q.E.D.}$</p> <hr/> <h2 id="c-the-kernel-of-a-linear-map-lvto-w-is-the-set-of-points-vin-v-such-that-lvvec0_w-show-that-a-linear-map-lvto-w-is-1-1-if-and-only-if-the-kernel-of-l-consists-of-only-the-identity-element-vec0_v">(c) The kernel of a linear map $L:V\to W$ is the set of points $v\in V$ such that $L(v)=\vec{0}_{W}$. Show that a linear map $L:V\to W$ is 1-1 if and only if the kernel of $L$ consists of only the identity element $\vec{0}_{V}$.</h2> <p><strong>Proposition 2.c.1.</strong> <em>Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$, and let $L: V \to W$ be a linear transformation. Then $L$ is injective (one-to-one) if and only if the kernel of $L$, denoted by $\ker(L)$, consists solely of the zero vector in $V$, that is, $\ker(L) = { \vec{0}_V }$.</em></p> <p><em>Proof.</em> We proceed by establishing both implications of the equivalence:</p> <ol> <li>Necessity: If $L$ is injective, then $\ker(L) = { \vec{0}_V }$.</li> <li>Sufficiency: If $\ker(L) = { \vec{0}_V }$, then $L$ is injective.</li> </ol> <p>Necessity: Assume that $L: V \to W$ is injective. We aim to show that $\ker(L) = { \vec{0}_V }$.</p> <p>By definition, the kernel of $L$ is the set:</p> \[\ker(L) = \{ v \in V \vert L(v) = \vec{0}_W \}.\] <p>To demonstrate that $\ker(L) = { \vec{0}_V }$, we must show two inclusions:</p> \[\ker(L) \subseteq \{ \vec{0}_V \} \quad \text{and} \quad \{ \vec{0}_V \} \subseteq \ker(L).\] <p>The second inclusion is trivial since $L(\vec{0}_V) = \vec{0}_W$ by the linearity of $L$ (as established in Proposition 2.b.1).</p> <p>For the first inclusion, suppose $v \in \ker(L)$. Then, by definition:</p> \[L(v) = \vec{0}_W.\] <p>Given that $L$ is injective, the only solution to $L(v) = \vec{0}_W$ is $v = \vec{0}_V$. To see this, consider the injectivity of $L$, which implies that if $L(v) = L(v‚Äô)$, then $v = v‚Äô$. Specifically, taking $v‚Äô = \vec{0}_V$, we have:</p> \[L(v) = \vec{0}_W = L(\vec{0}_V) \implies v = \vec{0}_V.\] <p>Thus, $v$ must be the zero vector in $V$, and therefore:</p> \[\ker(L) \subseteq \{ \vec{0}_V \}.\] <p>Combining both inclusions, we conclude:</p> \[\ker(L) = \{ \vec{0}_V \}.\] <p>This establishes the necessity part of the proposition.</p> <p>Sufficiency: Now, assume that $\ker(L) = { \vec{0}_V }$. We aim to demonstrate that $L$ is injective.</p> <p>To prove that $L$ is injective, we must show that for any $v_1, v_2 \in V$, if $L(v_1) = L(v_2)$, then $v_1 = v_2$.</p> <p>Consider arbitrary vectors $v_1, v_2 \in V$ such that:</p> \[L(v_1) = L(v_2).\] <p>Subtracting $L(v_2)$ from both sides yields:</p> \[L(v_1) - L(v_2) = \vec{0}_W.\] <p>Utilizing the linearity of $L$, this can be rewritten as:</p> \[L(v_1 - v_2) = \vec{0}_W.\] <p>By the definition of the kernel, this implies:</p> \[v_1 - v_2 \in \ker(L).\] <p>Given that $\ker(L) = { \vec{0}_V }$, it follows that:</p> \[v_1 - v_2 = \vec{0}_V.\] <p>Thus, we conclude:</p> \[v_1 = v_2.\] <p>This establishes that $L$ is injective, as required. $\textbf{Q.E.D.}$</p> <hr/> <h2 id="d-a-subspace-of-a-vector-space-v-is-a-subset-s-subset-v-such-that-for-all-v-w-in-s-v--w-in-s-and-for-all-v-in-s-and-c-in-mathbbr-c-cdot-v-in-s-that-is-the-subspace-is-closed-under-the-addition-and-scalar-multiplication-operations-show-that-the-kernel-of-a-linear-map-l-v-to-w-forms-a-subspace-of-the-space-v">(d) A <em>subspace</em> of a vector space $V$ is a subset $S \subset V$ such that for all $v, w \in S$, $v + w \in S$ and for all $v \in S$ and $c \in \mathbb{R}$, $c \cdot v \in S$. That is, the subspace is <em>closed</em> under the addition and scalar multiplication operations. Show that the kernel of a linear map $L: V \to W$ forms a subspace of the space $V$.</h2> <p><strong>Proposition 2.d.1</strong> <em>Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$, and let $L: V \to W$ be a linear transformation. Then the kernel of $L$, denoted by $\ker(L)$, defined as</em></p> \[\ker(L) = \{ v \in V \vert L(v) = \vec{0}_W \},\] <p><em>is a subspace of $V$.</em></p> <p><em>Proof.</em> To establish that $\ker(L)$ is a subspace of $V$, we must verify that $\ker(L)$ satisfies the three axioms defining a subspace within a vector space. Specifically, we need to confirm that:</p> <ol> <li>$\ker(L)$ is non-empty.</li> <li>$\ker(L)$ is closed under vector addition.</li> <li>$\ker(L)$ is closed under scalar multiplication.</li> </ol> <p>We proceed by verifying each of these properties in turn.</p> <p>Non-emptiness of $\ker(L)$: By definition, a subspace must contain the zero vector of the ambient vector space. Consider the additive identity $\vec{0}_V \in V$. Applying the linear transformation $L$ to $\vec{0}_V$, we obtain:</p> \[L(\vec{0}_V) = \vec{0}_W,\] <p>as established in Proposition 2.b.1. Therefore, $\vec{0}_V \in \ker(L)$, which implies that $\ker(L)$ is non-empty.</p> <p>Closure under Vector Addition: Let $u, v \in \ker(L)$. By the definition of the kernel, this means:</p> \[L(u) = \vec{0}_W \quad \text{and} \quad L(v) = \vec{0}_W.\] <p>We must show that $u + v \in \ker(L)$, i.e., $L(u + v) = \vec{0}_W$.</p> <p>Applying the linearity of $L$, we have:</p> \[L(u + v) = L(u) + L(v).\] <p>Substituting the known values from the kernel, this becomes:</p> \[L(u + v) = \vec{0}_W + \vec{0}_W = \vec{0}_W.\] <p>Thus, $u + v \in \ker(L)$, establishing closure under vector addition.</p> <p>Closure under Scalar Multiplication: Let $v \in \ker(L)$ and let $c \in \mathbb{R}$ be an arbitrary scalar. We must demonstrate that $c \cdot v \in \ker(L)$, i.e., $L(c \cdot v) = \vec{0}_W$.</p> <p>Applying the linearity of $L$ with respect to scalar multiplication, we obtain:</p> \[L(c \cdot v) = c \cdot L(v).\] <p>Since $v \in \ker(L)$, it follows that $L(v) = \vec{0}_W$. Substituting this into the equation above yields:</p> \[L(c \cdot v) = c \cdot \vec{0}_W = \vec{0}_W.\] <p>Therefore, $c \cdot v \in \ker(L)$, establishing closure under scalar multiplication. $\textbf{Q.E.D.}$</p> <hr/> <h2 id="e-show-that-the-image-of-a-linear-map-lvto-w-is-a-subspace-of-w">(e) Show that the image of a linear map $L:V\to W$ is a subspace of $W$.</h2> <p><strong>Proposition 2.e.1.</strong> <em>Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$, and let $L: V \to W$ be a linear map. Then the image of $L$, defined by \(\text{Im}(L) = \{ L(v) \mid v \in V \},\) is a subspace of $W$.</em></p> <p><em>Proof.</em> To establish that $\text{Im}(L)$ is a subspace of $W$, it is necessary to verify that it satisfies the three axioms defining a subspace. Specifically, we must demonstrate that:</p> <ol> <li>The zero vector $\vec{0}_W$ of $W$ is an element of $\text{Im}(L)$.</li> <li>$\text{Im}(L)$ is closed under vector addition.</li> <li>$\text{Im}(L)$ is closed under scalar multiplication.</li> </ol> <p>Containment of the Zero Vector: By the linearity of $L$, for any vector space $V$, the map $L$ satisfies</p> \[L(\vec{0}_V) = \vec{0}_W.\] <p>Here, $\vec{0}_V$ denotes the additive identity in $V$, and $\vec{0}_W$ denotes the additive identity in $W$. Since $\vec{0}_V \in V$, it follows directly from the definition of the image that</p> \[\vec{0}_W = L(\vec{0}_V) \in \text{Im}(L).\] <p>Thus, the zero vector of $W$ is contained within $\text{Im}(L)$.</p> <p>Closure Under Vector Addition: Let $u, v \in \text{Im}(L)$. By the definition of the image, there exist vectors $u‚Äô, v‚Äô \in V$ such that</p> \[u = L(u') \quad \text{and} \quad v = L(v').\] <p>Consider the sum $u + v$ in $W$. Applying the linearity of $L$, we have</p> \[u + v = L(u') + L(v') = L(u' + v').\] <p>Since $u‚Äô + v‚Äô \in V$ (as $V$ is a vector space and thus closed under addition), it follows that</p> \[u + v = L(u' + v') \in \text{Im}(L).\] <p>Therefore, $\text{Im}(L)$ is closed under vector addition.</p> <p>Closure Under Scalar Multiplication: Let $v \in \text{Im}(L)$ and let $c \in \mathbb{R}$ be an arbitrary scalar. By the definition of the image, there exists a vector $v‚Äô \in V$ such that</p> \[v = L(v').\] <p>Consider the scalar multiple $c \cdot v$ in $W$. Utilizing the linearity of $L$, we obtain</p> \[c \cdot v = c \cdot L(v') = L(c \cdot v').\] <p>Since $c \cdot v‚Äô \in V$ (as $V$ is a vector space and thus closed under scalar multiplication), it follows that</p> \[c \cdot v = L(c \cdot v') \in \text{Im}(L).\] <p>Hence, $\text{Im}(L)$ is closed under scalar multiplication. $\textbf{Q.E.D.}$</p> <hr/> <h2 id="f-the-set-of-linear-maps-from-v-to-w-is-typically-denoted-mathcallvw-define-an-addition-operation-on-linear-maps-as-follows-given-fginmathcallvw-set-kfg-by-kvfvgv-where-the-addition-on-the-rhs-here-is-the-addition-operation-in-w-similarly-given-cinmathbbr-we-define-a-scalar-multiplication-kccdot-f-by-kvccdot-fv-where-again-the-scalar-multiplication-is-the-operation-in-w-show-that-with-these-operations-mathcallvw-is-a-vector-space">(f) The set of linear maps from $V$ to $W$ is typically denoted $\mathcal{L}(V,W)$. Define an addition operation on linear maps as follows: given $F,G\in\mathcal{L}(V,W)$, set $K=F+G$ by $K(v)=F(v)+G(v)$, where the addition on the RHS here is the addition operation in $W$. Similarly, given $c\in\mathbb{R}$, we define a scalar multiplication $K=c\cdot F$ by $K(v)=c\cdot F(v)$, where again the scalar multiplication is the operation in $W$. Show that with these operations, $\mathcal{L}(V,W)$ is a vector space.</h2> <p><strong>Proposition 2.f.1.</strong> <em>Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$. Define the set $\mathcal{L}(V, W)$ to consist of all linear maps from $V$ to $W$. Equip $\mathcal{L}(V, W)$ with the operations of addition and scalar multiplication as defined below:</em></p> \[(F + G)(v) = F(v) + G(v) \quad \text{for all } F, G \in \mathcal{L}(V, W) \text{ and } v \in V,\] \[(c \cdot F)(v) = c \cdot F(v) \quad \text{for all } F \in \mathcal{L}(V, W), \, c \in \mathbb{R}, \text{ and } v \in V.\] <p><em>Then $\mathcal{L}(V, W)$ endowed with these operations constitutes a vector space over $\mathbb{R}$.</em></p> <p><em>Proof.</em> To establish that $\mathcal{L}(V, W)$ constitutes a vector space under the defined operations of addition and scalar multiplication, it is requisite to verify that it adheres to all the fundamental axioms that characterize a vector space. Specifically, we will verify the following properties:</p> <ol> <li> <p>Closure under Addition: For all $F, G \in \mathcal{L}(V, W)$, the map $F + G$ defined by $(F + G)(v) = F(v) + G(v)$ for all $v \in V$ is also an element of $\mathcal{L}(V, W)$.</p> </li> <li> <p>Closure under Scalar Multiplication: For all $F \in \mathcal{L}(V, W)$ and $c \in \mathbb{R}$, the map $c \cdot F$ defined by $(c \cdot F)(v) = c \cdot F(v)$ for all $v \in V$ is also an element of $\mathcal{L}(V, W)$.</p> </li> <li> <p>Associativity of Addition: For all $F, G, H \in \mathcal{L}(V, W)$, $(F + G) + H = F + (G + H)$.</p> </li> <li> <p>Commutativity of Addition: For all $F, G \in \mathcal{L}(V, W)$, $F + G = G + F$.</p> </li> <li> <p>Existence of Additive Identity: There exists a linear map $\Theta \in \mathcal{L}(V, W)$ such that for all $F \in \mathcal{L}(V, W)$, $F + \Theta = F$.</p> </li> <li> <p>Existence of Additive Inverses: For each $F \in \mathcal{L}(V, W)$, there exists a linear map $-F \in \mathcal{L}(V, W)$ such that $F + (-F) = \Theta$.</p> </li> <li> <p>Distributivity of Scalar Multiplication with Respect to Vector Addition: For all $c \in \mathbb{R}$ and $F, G \in \mathcal{L}(V, W)$, $c \cdot (F + G) = c \cdot F + c \cdot G$.</p> </li> <li> <p>Distributivity of Scalar Multiplication with Respect to Field Addition: For all $c, d \in \mathbb{R}$ and $F \in \mathcal{L}(V, W)$, $(c + d) \cdot F = c \cdot F + d \cdot F$.</p> </li> <li> <p>Compatibility of Scalar Multiplication with Field Multiplication: For all $c, d \in \mathbb{R}$ and $F \in \mathcal{L}(V, W)$, $c \cdot (d \cdot F) = (c d) \cdot F$.</p> </li> <li> <p>Identity Element of Scalar Multiplication: For the scalar $1 \in \mathbb{R}$ and for all $F \in \mathcal{L}(V, W)$, $1 \cdot F = F$.</p> </li> </ol> <p>Closure under Addition: Let $F, G \in \mathcal{L}(V, W)$. We must show that $F + G$ is a linear map from $V$ to $W$, i.e., $F + G \in \mathcal{L}(V, W)$.</p> <p>Additivity: For all $v_1, v_2 \in V$,</p> \[(F + G)(v_1 + v_2) = F(v_1 + v_2) + G(v_1 + v_2).\] <p>Since $F$ and $G$ are linear maps,</p> \[F(v_1 + v_2) = F(v_1) + F(v_2),\] \[G(v_1 + v_2) = G(v_1) + G(v_2).\] <p>Thus,</p> \[(F + G)(v_1 + v_2) = [F(v_1) + F(v_2)] + [G(v_1) + G(v_2)]\] \[= [F(v_1) + G(v_1)] + [F(v_2) + G(v_2)] = (F + G)(v_1) + (F + G)(v_2).\] <p>Homogeneity: For all $c \in \mathbb{R}$ and $v \in V$,</p> \[(F + G)(c \cdot v) = F(c \cdot v) + G(c \cdot v).\] <p>Again, since $F$ and $G$ are linear maps,</p> \[F(c \cdot v) = c \cdot F(v),\] \[G(c \cdot v) = c \cdot G(v).\] <p>Thus,</p> \[(F + G)(c \cdot v) = c \cdot F(v) + c \cdot G(v) = c \cdot [F(v) + G(v)] = c \cdot (F + G)(v).\] <p>Therefore, $F + G$ preserves both vector addition and scalar multiplication, and hence $F + G$ is linear. Consequently, $F + G \in \mathcal{L}(V, W)$, establishing closure under addition.</p> <p>Closure under Scalar Multiplication: Let $F \in \mathcal{L}(V, W)$ and $c \in \mathbb{R}$. We must show that $c \cdot F$ is a linear map from $V$ to $W$, i.e., $c \cdot F \in \mathcal{L}(V, W)$.</p> <p>Additivity: For all $v_1, v_2 \in V$,</p> \[(c \cdot F)(v_1 + v_2) = c \cdot F(v_1 + v_2).\] <p>Since $F$ is linear,</p> \[F(v_1 + v_2) = F(v_1) + F(v_2).\] <p>Thus,</p> \[(c \cdot F)(v_1 + v_2) = c \cdot [F(v_1) + F(v_2)] = c \cdot F(v_1) + c \cdot F(v_2) = (c \cdot F)(v_1) + (c \cdot F)(v_2).\] <p>Homogeneity: For all $d \in \mathbb{R}$ and $v \in V$,</p> \[(c \cdot F)(d \cdot v) = c \cdot F(d \cdot v).\] <p>Since $F$ is linear,</p> \[F(d \cdot v) = d \cdot F(v).\] <p>Thus,</p> \[(c \cdot F)(d \cdot v) = c \cdot [d \cdot F(v)] = (c d) \cdot F(v) = d \cdot (c \cdot F)(v).\] <p>Therefore, $c \cdot F$ preserves both vector addition and scalar multiplication, and hence $c \cdot F$ is linear. Consequently, $c \cdot F \in \mathcal{L}(V, W)$, establishing closure under scalar multiplication.</p> <p>Associativity of Addition: Let $F, G, H \in \mathcal{L}(V, W)$. We must show that $(F + G) + H = F + (G + H)$.</p> <p>For all $v \in V$,</p> \[[(F + G) + H](v) = (F + G)(v) + H(v) = [F(v) + G(v)] + H(v) =\] \[F(v) + [G(v) + H(v)] = F(v) + (G + H)(v) = [F + (G + H)](v).\] <p>Since this holds for all $v \in V$, we conclude that $(F + G) + H = F + (G + H)$.</p> <p>Commutativity of Addition: Let $F, G \in \mathcal{L}(V, W)$. We must show that $F + G = G + F$.</p> <p>For all $v \in V$,</p> \[(F + G)(v) = F(v) + G(v) = G(v) + F(v) = (G + F)(v).\] <p>Since this holds for all $v \in V$, we conclude that $F + G = G + F$.</p> <p>Existence of Additive Identity: We must exhibit an element $\Theta \in \mathcal{L}(V, W)$ such that for all $F \in \mathcal{L}(V, W)$, $F + \Theta = F$.</p> <p>Define $\Theta: V \to W$ by $\Theta(v) = \vec{0}_W$ for all $v \in V$, where $\vec{0}_W$ is the zero vector in $W$.</p> <p>Linearity of $\Theta$: For all $v_1, v_2 \in V$ and $c \in \mathbb{R}$,</p> \[\Theta(v_1 + v_2) = \vec{0}_W = \vec{0}_W + \vec{0}_W = \Theta(v_1) + \Theta(v_2),\] \[\Theta(c \cdot v) = \vec{0}_W = c \cdot \vec{0}_W = c \cdot \Theta(v).\] <p>Thus, $\Theta$ is linear, and hence $\Theta \in \mathcal{L}(V, W)$.</p> <p>Additive Identity Property: For all $F \in \mathcal{L}(V, W)$ and $v \in V$,</p> \[(F + \Theta)(v) = F(v) + \Theta(v) = F(v) + \vec{0}_W = F(v).\] <p>Therefore, $F + \Theta = F$ for all $F \in \mathcal{L}(V, W)$, establishing the existence of an additive identity in $\mathcal{L}(V, W)$.</p> <p>Existence of Additive Inverses: For each $F \in \mathcal{L}(V, W)$, we must exhibit a linear map $-F \in \mathcal{L}(V, W)$ such that $F + (-F) = \Theta$, where $\Theta$ is the additive identity in $\mathcal{L}(V, W)$.</p> <p>Define $-F: V \to W$ by $(-F)(v) = -F(v)$ for all $v \in V$.</p> <p>Linearity of $-F$: For all $v_1, v_2 \in V$ and $c \in \mathbb{R}$,</p> \[(-F)(v_1 + v_2) = -F(v_1 + v_2) = -[F(v_1) + F(v_2)] =\] \[-F(v_1) - F(v_2) = (-F)(v_1) + (-F)(v_2),\] \[(-F)(c \cdot v) = -F(c \cdot v) = -[c \cdot F(v)] = c \cdot (-F(v)) = c \cdot (-F)(v).\] <p>Thus, $-F$ is linear, and hence $-F \in \mathcal{L}(V, W)$.</p> <p>Additive Inverse Property: For all $F \in \mathcal{L}(V, W)$ and $v \in V$,</p> \[(F + (-F))(v) = F(v) + (-F)(v) = F(v) - F(v) = \vec{0}_W = \Theta(v).\] <p>Therefore, $F + (-F) = \Theta$, establishing the existence of additive inverses in $\mathcal{L}(V, W)$.</p> <p>Distributivity of Scalar Multiplication with Respect to Vector Addition: Let $c \in \mathbb{R}$ and $F, G \in \mathcal{L}(V, W)$. We must show that</p> \[c \cdot (F + G) = c \cdot F + c \cdot G.\] <p>For all $v \in V$,</p> \[[c \cdot (F + G)](v) = c \cdot (F + G)(v) = c \cdot [F(v) + G(v)] =\] \[c \cdot F(v) + c \cdot G(v) = [c \cdot F](v) + [c \cdot G](v) = [c \cdot F + c \cdot G](v).\] <p>Since this holds for all $v \in V$, we conclude that $c \cdot (F + G) = c \cdot F + c \cdot G$.</p> <p>Distributivity of Scalar Multiplication with Respect to Field Addition: Let $c, d \in \mathbb{R}$ and $F \in \mathcal{L}(V, W)$. We must show that</p> \[(c + d) \cdot F = c \cdot F + d \cdot F.\] <p>For all $v \in V$,</p> \[[(c + d) \cdot F](v) = (c + d) \cdot F(v) = c \cdot F(v) + d \cdot F(v) =\] \[[c \cdot F](v) + [d \cdot F](v) = [c \cdot F + d \cdot F](v).\] <p>Since this holds for all $v \in V$, we conclude that $(c + d) \cdot F = c \cdot F + d \cdot F$.</p> <p>Compatibility of Scalar Multiplication with Field Multiplication: Let $c, d \in \mathbb{R}$ and $F \in \mathcal{L}(V, W)$. We must show that</p> \[c \cdot (d \cdot F) = (c d) \cdot F.\] <p>For all $v \in V$,</p> \[[c \cdot (d \cdot F)](v) = c \cdot [d \cdot F(v)] = (c d) \cdot F(v) = [(c d) \cdot F](v).\] <p>Since this holds for all $v \in V$, we conclude that $c \cdot (d \cdot F) = (c d) \cdot F$.</p> <p>Identity Element of Scalar Multiplication: Let $F \in \mathcal{L}(V, W)$. We must show that</p> \[1 \cdot F = F.\] <p>For all $v \in V$,</p> \[[1 \cdot F](v) = 1 \cdot F(v) = F(v).\] <p>Thus, $1 \cdot F = F$. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentray</strong></p> <p>In infinite-dimensional settings, while some properties may differ, the vector space structure of $\mathcal{L}(V, W)$ remains valid as long as operations are well-defined.</p> </blockquote>]]></content><author><name></name></author><category term="A Commentary of the Pre-Practice for Boston University&apos;s EC525 Course"/><category term="Mathematics"/><summary type="html"><![CDATA[Given Conditions If $V$ and $W$ are two different vector spaces, a \textit{linear map} $L$ from $V$ to $W$ is a function $L:V\to W$ such that $L(v+v^{\prime})=L(v)+L(v^{\prime})$ for all $v,v^{\prime}\in V$. $L(s\cdot v)=s\cdot L(v)$ for all $s\in\mathbb{R}$, $v\in V$.]]></summary></entry><entry><title type="html">Problem ‚Ö†: Vector Spaces</title><link href="https://shuhongdai.github.io/blog/2023/EC525_1/" rel="alternate" type="text/html" title="Problem ‚Ö†: Vector Spaces"/><published>2023-12-09T00:00:00+00:00</published><updated>2023-12-09T00:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2023/EC525_1</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2023/EC525_1/"><![CDATA[<h2 id="given-conditions">Given Conditions</h2> <p>A <em>vector space</em> (over the reals) is a set $V$ together with two binary operations which we call addition (+) and scalar multiplication (¬∑). The addition operation is a function which takes two elements of $V$, say $v$ and $w$, and returns a third element of $V$, which we write as $v+w$, while scalar multiplication is a function which takes a real number and an element of $V$,say $s$ and $v$, and returms another element of $V$,which we write as $s\cdot v.$ The operations are called‚Äúbinary‚Äùbecause they take two arguments. The two operations must satisfy the following properties:</p> <ul> <li>Addition operation is commutative: $v+w=w+v$ for all $w$ and $v.$</li> <li>There exists an additive identity element, which we write as $\vec{0}$ such that $\vec{0}+w=w$ for all $w.$</li> <li>Scalar multiplication distributes over addition in $V{:}s\cdot(v+w)=s\cdot v+s\cdot w.$</li> <li>Scalar multiplication distributes over addition in $\mathbb{R}{:}$ if $a,b\in\mathbb{R}$ and $w\in V$, then $(a+b)\cdot w=a\cdot w+b\cdot w$, where in the left hand side of the equation the addition is the familiar addition operation in $\mathbb{R}$, but on the right hand side the addition is the addition operation in $V$.</li> <li>Associativity of addition: $(v+w)+z=v+(w+z).$</li> <li>Compatibility of scalar multiplication with real-number multiplication: if $s$ and $c$ are real numbers and $v\in V$,then $(sc)\cdot v=s\cdot(c\cdot v).$</li> <li>$1\cdot v= v$ for all $v\in V.$</li> <li>Every element $v\in V$ has an <em>additive inverse</em> $w$ such that $v+w=\vec{0}.$</li> </ul> <p><strong>Quick Navigation</strong> ‚¨áÔ∏è</p> <ul> <li> <p><a href="https://shuhongdai.github.io/blog/2023/EC525_0/">Preface: Motivation and Overview</a></p> </li> <li> <p><strong>Problem ‚Ö†: Vector Spaces</strong> (You are currently browsing this post)</p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_2/">Problem ‚Ö°: Linear Transformations</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_3/">Problem ‚Ö¢: Group Theory</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_4/">Problem ‚Ö£: Dual Spaces and Functional Analysis</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_5/">Problem ‚Ö§: Infinite Sequences and Combinatorial Principles</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_6/">Problem ‚Ö•: Set Theory and Cardinality</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_7/">Conclusion</a></p> </li> </ul> <hr/> <h2 id="a-let-vmathbbr-and-let-the-addition-and-multiplication-operations-be-the-standard-addition-and-multiplication-in-mathbbr-show-that-v-is-a-vector-space">(a) Let $V=\mathbb{R}$ and let the addition and multiplication operations be the standard addition and multiplication in $\mathbb{R}$. Show that $V$ is a vector space.</h2> <p><em>Proof.</em> To establish that $V$ is a vector space over the field $\mathbb{R}$, it suffices to verify that the eight vector space axioms are satisfied. Axiom 1: Commutativity of Addition</p> \[\forall u, v \in V, \quad u + v = v + u.\] <p>Verification: The standard addition of real numbers is commutative; hence, for any $u, v \in \mathbb{R}$,</p> \[u + v = v + u.\] <p>Thus, Axiom 1 holds.</p> <p>Axiom 2: Associativity of Addition</p> \[\forall u, v, w \in V, \quad (u + v) + w = u + (v + w).\] <p>Verification: The standard addition of real numbers is associative; therefore, for any $u, v, w \in \mathbb{R}$,</p> \[(u + v) + w = u + (v + w).\] <p>Thus, Axiom 2 is satisfied.</p> <p>Axiom 3: Existence of Additive Identity</p> \[\exists \tilde{0} \in V \text{ such that } \forall v \in V, \quad \tilde{0} + v = v.\] <p>Verification: In $\mathbb{R}$, the additive identity is $0$. For any $v \in \mathbb{R}$,</p> \[0 + v = v.\] <p>Hence, $\tilde{0} = 0$ serves as the additive identity, satisfying Axiom 3.</p> <p>Axiom 4: Existence of Additive Inverses</p> \[\forall v \in V, \exists w \in V \text{ such that } v + w = \tilde{0}.\] <p>Verification: For any $v \in \mathbb{R}$, the additive inverse is $-v$. Thus,</p> \[v + (-v) = 0 = \tilde{0}.\] <p>Therefore, every element $v$ in $V$ has an additive inverse $-v$, satisfying Axiom 4.</p> <p>Axiom 5: Scalar Multiplication Distributes over Vector Addition</p> \[\forall s \in \mathbb{R}, \forall u, v \in V, \quad s \cdot (u + v) = s \cdot u + s \cdot v.\] <p>Verification: The distributive property of real numbers ensures that for any $s, u, v \in \mathbb{R}$,</p> \[s \cdot (u + v) = s \cdot u + s \cdot v.\] <p>Thus, Axiom 5 is satisfied.</p> <p>Axiom 6: Scalar Addition Distributes over Scalar Multiplication</p> \[\forall a, b \in \mathbb{R}, \forall v \in V, \quad (a + b) \cdot v = a \cdot v + b \cdot v.\] <p>Verification: The distributive property of real numbers implies that for any $a, b, v \in \mathbb{R}$,</p> \[(a + b) \cdot v = a \cdot v + b \cdot v.\] <p>Therefore, Axiom 6 holds.</p> <p>Axiom 7: Compatibility of Scalar Multiplication with Field Multiplication</p> \[\forall s, c \in \mathbb{R}, \forall v \in V, \quad (s \cdot c) \cdot v = s \cdot (c \cdot v).\] <p>Verification: The associativity of multiplication in $\mathbb{R}$ ensures that for any $s, c, v \in \mathbb{R}$,</p> \[(s \cdot c) \cdot v = s \cdot (c \cdot v).\] <p>Hence, Axiom 7 is satisfied.</p> <p>Axiom 8: Scalar Multiplicative Identity</p> \[\forall v \in V, \quad 1 \cdot v = v.\] <p>Verification: In $\mathbb{R}$, the multiplicative identity is $1$. For any $v \in \mathbb{R}$,</p> \[1 \cdot v = v.\] <p>Thus, Axiom 8 holds.</p> <p>Having verified all eight vector space axioms, $V = \mathbb{R}$ with standard addition and scalar multiplication is a vector space over $\mathbb{R}$. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentary</strong></p> <p>The question (a) and the subsequent (b) may seem ‚Äúobvious,‚Äù but proving them from the most basic axioms is neither common nor necessary in the study of engineering and applied sciences. However, I personally believe it contains three layers of meaning:</p> <ul> <li>Any mathematical theorem is derived through progressive steps from the most fundamental axioms, and the assumptions are similar. There is a strong intuitive, and consequently logical, consideration here. It is not as simple as it seems. (Here‚Äôs a famous joke: How do you determine if a 3-year-old child has a talent for mathematics? Ask them why $\textit{1+2 = 2+1}$. If they answer, ‚ÄúBecause of the commutative property of addition,‚Äù their mathematical talent is average. If they answer, ‚ÄúBecause the set of integers forms an Abelian group under addition,‚Äù then they are highly gifted.)</li> <li>Even knowing these proof methods, presenting them in a formal manner, particularly using \(\LaTeX\), professional expressions, and more tools, are more challenging tasks.</li> <li>When writing papers in the field of computer science, is there still room for expansion in your mathematical derivations?</li> </ul> </blockquote> <hr/> <h2 id="b-let-x-be-some-arbitrary-space-and-let-f-be-the-set-of-functions-fxtomathbbr-given-any-two-functions-f-and-g-we-can-define-their-sum-kfg-as-the-function-kxfxgx-further-given-a-scalar-s-and-a-function-f-we-define-the-scalar-multiplication-kscdot-f-as-the-function-kxstimes-fx-where-times-indicates-standard-multiplication-of-real-numbers-show-that-f-with-these-addition-and-scalar-multiplication-operators-is-a-vector-space">(b) Let $X$ be some arbitrary space and let $F$ be the set of functions $f:X\to\mathbb{R}.$ Given any two functions $f$ and $g$, we can define their sum $k:=f+g$ as the function $k(x)=f(x)+g(x).$ Further, given a scalar $s$ and a function $f$, we define the scalar multiplication $k:=s\cdot f$ as the function $k(x)=s\times f(x)$, where $\times$ indicates standard multiplication of real numbers. Show that $F$ with these addition and scalar multiplication operators is a vector space.</h2> <p><em>Proof.</em> Let $X$ be an arbitrary set, and let $F$ denote the set of all functions from $X$ to $\mathbb{R}$, i.e., $F =$ { $f \mid f: X \to \mathbb{R}$ }. Define the operations of addition and scalar multiplication on $F$ as follows:</p> \[\forall f, g \in F, \quad (f + g)(x) = f(x) + g(x) \quad \text{for all } x \in X,\] \[\forall s \in \mathbb{R}, \forall f \in F, \quad (s \cdot f)(x) = s \cdot f(x) \quad \text{for all } x \in X.\] <p>To establish that $F$ is a vector space over the field $\mathbb{R}$, it is requisite to verify that the eight axioms defining a vector space are satisfied under the aforementioned operations.</p> <p>Axiom 1: Commutativity of Addition</p> \[\forall f, g \in F, \quad f + g = g + f.\] <p>Verification: For any $f, g \in F$ and for all $x \in X$,</p> \[(f + g)(x) = f(x) + g(x) = g(x) + f(x) = (g + f)(x).\] <p>Since this equality holds for all $x \in X$, it follows that $f + g = g + f$. Hence, addition is commutative.</p> <p>Axiom 2: Associativity of Addition</p> \[\forall f, g, h \in F, \quad (f + g) + h = f + (g + h).\] <p>Verification: For any $f, g, h \in F$ and for all $x \in X$,</p> \[((f + g) + h)(x) = (f + g)(x) + h(x) = (f(x) + g(x)) + h(x),\] \[(f + (g + h))(x) = f(x) + (g + h)(x) = f(x) + (g(x) + h(x)).\] <p>By the associativity of real number addition,</p> \[(f(x) + g(x)) + h(x) = f(x) + (g(x) + h(x)),\] <p>which implies</p> \[((f + g) + h)(x) = (f + (g + h))(x).\] <p>Since this holds for all $x \in X$, it follows that $(f + g) + h = f + (g + h)$. Hence, addition is associative.</p> <p>Axiom 3: Existence of Additive Identity</p> \[\exists \tilde{0} \in F \text{ such that } \forall f \in F, \quad \tilde{0} + f = f.\] <p>Verification: Define $\tilde{0}$ as the zero function, i.e., $\tilde{0}(x) = 0$ for all $x \in X$. For any $f \in F$ and for all $x \in X$,</p> \[(\tilde{0} + f)(x) = \tilde{0}(x) + f(x) = 0 + f(x) = f(x).\] <p>Thus, $\tilde{0} + f = f$ for all $f \in F$, establishing the existence of an additive identity in $F$.</p> <p>Axiom 4: Existence of Additive Inverses</p> \[\forall f \in F, \exists g \in F \text{ such that } f + g = \tilde{0}.\] <p>Verification: For any $f \in F$, define $g = -f$, where $(-f)(x) = -f(x)$ for all $x \in X$. Then, for all $x \in X$,</p> \[(f + g)(x) = f(x) + (-f(x)) = 0 = \tilde{0}(x).\] <p>Therefore, $f + g = \tilde{0}$, confirming the existence of an additive inverse for every element in $F$.</p> <p>Axiom 5: Scalar Multiplication Distributes over Vector Addition</p> \[\forall s \in \mathbb{R}, \forall f, g \in F, \quad s \cdot (f + g) = s \cdot f + s \cdot g.\] <p>Verification: For any $s \in \mathbb{R}$, $f, g \in F$, and for all $x \in X$,</p> \[(s \cdot (f + g))(x) = s \cdot (f + g)(x) = s \cdot (f(x) + g(x)),\] \[(s \cdot f + s \cdot g)(x) = (s \cdot f)(x) + (s \cdot g)(x) = s \cdot f(x) + s \cdot g(x).\] <p>By the distributive property of real numbers,</p> \[s \cdot (f(x) + g(x)) = s \cdot f(x) + s \cdot g(x),\] <p>which implies</p> \[(s \cdot (f + g))(x) = (s \cdot f + s \cdot g)(x).\] <p>Since this holds for all $x \in X$, it follows that $s \cdot (f + g) = s \cdot f + s \cdot g$. Thus, scalar multiplication distributes over vector addition.</p> <p>Axiom 6: Scalar Addition Distributes over Scalar Multiplication</p> \[\forall a, b \in \mathbb{R}, \forall f \in F, \quad (a + b) \cdot f = a \cdot f + b \cdot f.\] <p>Verification: For any $a, b \in \mathbb{R}$, $f \in F$, and for all $x \in X$,</p> \[((a + b) \cdot f)(x) = (a + b) \cdot f(x),\] \[(a \cdot f + b \cdot f)(x) = (a \cdot f)(x) + (b \cdot f)(x) = a \cdot f(x) + b \cdot f(x).\] <p>By the distributive property of real numbers,</p> \[(a + b) \cdot f(x) = a \cdot f(x) + b \cdot f(x),\] <p>which implies</p> \[((a + b) \cdot f)(x) = (a \cdot f + b \cdot f)(x).\] <p>Since this equality holds for all $x \in X$, it follows that $(a + b) \cdot f = a \cdot f + b \cdot f$. Therefore, scalar addition distributes over scalar multiplication.</p> <p>Axiom 7: Compatibility of Scalar Multiplication with Field Multiplication</p> \[\forall a, b \in \mathbb{R}, \forall f \in F, \quad a \cdot (b \cdot f) = (a \cdot b) \cdot f.\] <p>Verification: For any $a, b \in \mathbb{R}$, $f \in F$, and for all $x \in X$,</p> \[(a \cdot (b \cdot f))(x) = a \cdot (b \cdot f)(x) = a \cdot (b \cdot f(x))\] \[((a \cdot b) \cdot f)(x) = (a \cdot b) \cdot f(x).\] <p>By the associative property of real number multiplication,</p> \[a \cdot (b \cdot f(x)) = (a \cdot b) \cdot f(x),\] <p>which implies</p> \[(a \cdot (b \cdot f))(x) = ((a \cdot b) \cdot f)(x).\] <p>Since this holds for all $x \in X$, it follows that $a \cdot (b \cdot f) = (a \cdot b) \cdot f$. Hence, scalar multiplication is compatible with field multiplication.</p> <p>Axiom 8: Existence of Scalar Multiplicative Identity</p> \[\forall f \in F, \quad 1 \cdot f = f.\] <p>Verification: For any $f \in F$ and for all $x \in X$,</p> \[(1 \cdot f)(x) = 1 \cdot f(x) = f(x).\] <p>Therefore, $1 \cdot f = f$ for all $f \in F$, establishing the existence of a scalar multiplicative identity in $F$.</p> <p>After verifying all eight axioms, we conclude that $F =$ { $f \mid f: X \to \mathbb{R}$ } with standard function addition and scalar multiplication forms a vector space over $\mathbb{R}$. $\textbf{Q.E.D.}$</p> <hr/> <h2 id="c-show-that-the-additive-identity-is-unique-specifically-iin-v-is-such-that-there-exists-some-win-v-such-that-iww-then-ivec0">(c) Show that the additive identity is unique. Specifically, $i\in V$ is such that there exists some $w\in V$ such that $i+w=w$, then $i=\vec{0}$.</h2> <p>Let $V$ be a vector space over the field $\mathbb{R}$, equipped with the operations of vector addition $+$ and scalar multiplication $\cdot$. Denote by $\vec{0}$ the additive identity in $V$, satisfying</p> \[\forall v \in V, \quad \vec{0} + v = v.\] <p><strong>Theorem 1.c.1.</strong> <em>In any vector space $V$ over $\mathbb{R}$, the additive identity is unique.</em></p> <p><em>Proof.</em> Suppose $i \in V$ is an element such that there exists $w \in V$ with</p> \[i + w = w.\] <p>Consider the additive identity $\vec{0}$ in $V$, which satisfies</p> \[\forall v \in V, \quad \vec{0} + v = v.\] <p>In particular, for the vector $w \in V$, we have</p> \[\vec{0} + w = w.\] <p>Given that both $i$ and $\vec{0}$ satisfy the property of being additive identities with respect to the vector $w$, we have</p> \[i + w = w \quad \text{and} \quad \vec{0} + w = w.\] <p>Therefore, we can equate the two expressions:</p> \[i + w = \vec{0} + w.\] <p>By the associativity and cancellation properties of vector addition, we can deduce the following:</p> \[i + w = \vec{0} + w \implies (i + w) = (\vec{0} + w).\] <p>Subtracting $w$ from both sides, we obtain</p> \[i = \vec{0}.\] <p>$\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentary</strong></p> <p>The cancellation property in a vector space states that if $u + v = u + w$, then $v = w$. Applying this property to the equation $i + w = \vec{0} + w$, we cancel $w$ from both sides to conclude that $i = \vec{0}$.</p> </blockquote> <hr/> <h2 id="d-show-that-vec0-satisfes-scdotvec0vec0-for-all-sinmathbbr">(d) Show that $\vec{0}$ satisfes $s\cdot\vec{0}=\vec{0}$ for all $s\in\mathbb{R}.$</h2> <p>Let $V$ be a vector space over the field $\mathbb{R}$, equipped with the operations of vector addition $+$ and scalar multiplication $\cdot$. Denote by $\vec{0}$ the additive identity in $V$, satisfying</p> \[\forall v \in V, \quad \vec{0} + v = v.\] <p><strong>Theorem 1.d.1.</strong> <em>In any vector space $V$ over $\mathbb{R}$, the scalar multiplication of any real number $s$ with the additive identity $\vec{0}$ satisfies $s \cdot \vec{0} = \vec{0}$.</em></p> <p><em>Proof.</em> Let $s \in \mathbb{R}$ be an arbitrary scalar. Consider the scalar multiplication $s \cdot \vec{0}$.</p> <p>By the distributive property of scalar multiplication over vector addition (Axiom 5), we have:</p> \[s \cdot (\vec{0} + \vec{0}) = s \cdot \vec{0} + s \cdot \vec{0}.\] <p>Since $\vec{0}$ is the additive identity, it satisfies:</p> \[\vec{0} + \vec{0} = \vec{0}.\] <p>Substituting this into the previous equation yields:</p> \[s \cdot \vec{0} = s \cdot \vec{0} + s \cdot \vec{0}.\] <p>Let $x = s \cdot \vec{0}$. Then the equation becomes:</p> \[x = x + x.\] <p>Subtracting $x$ from both sides (utilizing the existence of additive inverses, Axiom 4), we obtain:</p> \[x - x = (x + x) - x.\] <p>Simplifying both sides:</p> \[\vec{0} = x.\] <p>Thus,</p> \[s \cdot \vec{0} = \vec{0}.\] <p>Since $s \in \mathbb{R}$ was arbitrary, the equality holds for all real scalars $s$. $\textbf{Q.E.D.}$</p> <hr/> <h2 id="e-show-that-0inmathbbr-satisfies-0cdot-wvec0-for-all-win-v">(e) Show that $0\in\mathbb{R}$ satisfies $0\cdot w=\vec{0}$ for all $w\in V$.</h2> <p>Let $V$ be a vector space over the field $\mathbb{R}$, endowed with the operations of vector addition $+$ and scalar multiplication $\cdot$. Denote by $\vec{0}$ the additive identity in $V$, satisfying</p> \[\forall v \in V, \quad \vec{0} + v = v.\] <p><strong>Theorem 1.e.1.</strong> <em>In any vector space $V$ over $\mathbb{R}$, the scalar zero acts as the additive identity under scalar multiplication; that is, for all $w \in V$, $0 \cdot w = \vec{0}$.</em></p> <p><em>Proof.</em> Let $w \in V$ be arbitrary. Consider the scalar multiplication of $0$ with $w$:</p> \[0 \cdot w.\] <p>By the distributive property of scalar multiplication over scalar addition (Axiom 6), we have:</p> \[(0 + 0) \cdot w = 0 \cdot w + 0 \cdot w.\] <p>Calculating the left-hand side using the fact that $0 + 0 = 0$ in $\mathbb{R}$:</p> \[0 \cdot w = 0 \cdot w + 0 \cdot w.\] <p>Let $x = 0 \cdot w$. Substituting, we obtain:</p> \[x = x + x.\] <p>To isolate $x$, we subtract $x$ from both sides of the equation (utilizing the existence of additive inverses, Axiom 4):</p> \[x - x = (x + x) - x.\] <p>Simplifying both sides:</p> \[\vec{0} = x.\] <p>Thus,</p> \[0 \cdot w = \vec{0}.\] <p>Since $w \in V$ was arbitrary, the equality holds for all $w \in V$. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentary</strong> Through the application of the distributive property of scalar multiplication over scalar addition and the properties of additive inverses within the vector space $V$, it has been rigorously established that for the scalar zero $0 \in \mathbb{R}$ and for any vector $w \in V$, the scalar multiplication $0 \cdot w$ yields the additive identity $\vec{0}$. Therefore, $0 \cdot w = \vec{0}$ holds universally within the structure of the vector space $V$.</p> </blockquote> <hr/> <h2 id="f-suppose-just-for-this-question-that-the-vector-spaces-need-not-have-additive-inverses-so-that-you-cannot-rely-on-that-axiom-when-proving-this-statement-show-that-nevertheless-if-iin-v-is-such-that-iww-tildetextfor-all-wthen-ivec0-note-that-this-is-a-weaker-result-than-question-c-why">(f) Suppose (just for this question) that the vector spaces need not have additive inverses, so that you cannot rely on that axiom when proving this statement. Show that nevertheless, if $i\in V$ is such that $i+w=w$ $\tilde{\text{for all }w}$,then $i=\vec{0}.$ Note that this is a weaker result than question (c) (why?).</h2> <blockquote> <p><strong>Commentary</strong></p> <p>Let $V$ be a vector space over the field $\mathbb{R}$, endowed with the operations of vector addition $+$ and scalar multiplication $\cdot$. In this context, we consider a modification of the standard vector space axioms by omitting the requirement for the existence of additive inverses. Specifically, we investigate the uniqueness of the additive identity under this relaxed structure.</p> </blockquote> <p><strong>Theorem 1.f.1.</strong> <em>In a vector space $V$ over $\mathbb{R}$, where the existence of additive inverses is not assumed, if an element $i \in V$ satisfies $i + w = w$ for all $w \in V$, then $i$ coincides with the additive identity $\vec{0}$.</em></p> <p><em>Proof.</em> Assume $V$ is a vector space over $\mathbb{R}$ with the operations $+$ and $\cdot$ satisfying all vector space axioms except the existence of additive inverses. Let $\vec{0} \in V$ denote the additive identity, satisfying</p> \[\forall w \in V, \quad \vec{0} + w = w.\] <p>Suppose $i \in V$ is an element such that</p> \[\forall w \in V, \quad i + w = w.\] <p>Consider an arbitrary element $w \in V$. By the given condition, we have</p> \[i + w = w.\] <p>Subtracting $\vec{0}$ from both sides (noting that $\vec{0}$ acts as the additive identity), we obtain</p> \[i + w + \vec{0} = w + \vec{0}.\] <p>However, since $\vec{0}$ is the additive identity, it satisfies</p> \[\vec{0} + v = v \quad \text{for all } v \in V.\] <p>Thus, the equation simplifies to</p> \[i + w = w.\] <p>Given that $i + w = w$ holds for all $w \in V$, we can deduce the following by substituting $w = \vec{0}$:</p> \[i + \vec{0} = \vec{0}.\] <p>But since $\vec{0}$ is the additive identity,</p> \[i + \vec{0} = i.\] <p>Combining the two results, we obtain</p> \[i = \vec{0}.\] <p>Hence, the element $i$ satisfying $i + w = w$ for all $w \in V$ must be equal to the additive identity $\vec{0}$. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentary</strong></p> <p>The result obtained herein is notably weaker than the corresponding assertion in Problem c, where the existence of additive inverses is assumed. In Problem c, the proof leverages the existence of additive inverses to facilitate the cancellation of terms, thereby directly establishing the equality $i = \vec{0}$ through subtraction. Specifically, the presence of additive inverses allows for the manipulation:</p> \[i + w = \vec{0} + w \implies i = \vec{0},\] <p>by subtracting $w$ from both sides.</p> <p>In contrast, Problem e circumvents the necessity of additive inverses by employing a more restrictive approach, relying solely on the properties of the additive identity. Consequently, while both proofs culminate in the uniqueness of the additive identity, the proof in Problem e is constrained by the absence of additive inverses and thus offers a less general mechanism for establishing $i = \vec{0}$.</p> </blockquote> <hr/> <h3 id="g-show-that-the-additive-inverses-are-unique-that-is-if-vin-v-and-w-and-wprime-are-two-additive-inverses-for-v-then-wwprime">(g) Show that the additive inverses are unique. That is, if $v\in V$ and $w$ and $w^{\prime}$ are two additive inverses for $v$, then $w=w^{\prime}$.</h3> <p>Let $V$ be a vector space over the field $\mathbb{R}$, equipped with the operations of vector addition $+$ and scalar multiplication $\cdot$. Within this framework, for each vector $v \in V$, there exists an additive inverse $w \in V$ such that</p> \[v + w = \vec{0},\] <p>where $\vec{0}$ denotes the additive identity in $V$.</p> <p><strong>Theorem 1.g.1.</strong> <em>In any vector space $V$ over $\mathbb{R}$, the additive inverse of a vector $v \in V$ is unique. That is, if $w$ and $w‚Äô$ are both additive inverses of $v$, then $w = w‚Äô$.</em></p> <p><em>Proof.</em> Let $V$ be a vector space over $\mathbb{R}$ satisfying the standard vector space axioms, including the existence of additive inverses. Suppose $w, w‚Äô \in V$ are both additive inverses of a vector $v \in V$. By definition of additive inverses, we have</p> \[v + w = \vec{0}, \quad \text{and} \quad v + w' = \vec{0}.\] <p>Consider the following sequence of equalities:</p> \[v + w = \vec{0}, \quad \text{and} \quad v + w' = \vec{0}.\] <p>Subtracting the first equation from the second yields</p> \[(v + w') - (v + w) = \vec{0} - \vec{0}.\] <p>Applying the associativity and commutativity of vector addition, we have</p> \[v + w' - v - w = \vec{0}.\] <p>Rearranging terms, this simplifies to</p> \[(v - v) + (w' - w) = \vec{0}.\] <p>Since $v - v = \vec{0}$ (by the definition of additive inverses and the additive identity), the equation becomes</p> \[\vec{0} + (w' - w) = \vec{0}.\] <p>By the property of the additive identity, $\vec{0} + x = x$ for any $x \in V$. Therefore,</p> \[w' - w = \vec{0}.\] <p>Adding $w$ to both sides yields</p> \[w' = w.\] <p>Thus, $w = w‚Äô$ establishing that the additive inverse of $v$ is unique. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentray</strong> The uniqueness of additive inverses is a pivotal property in the theory of vector spaces. It ensures that each vector possesses exactly one inverse, thereby preventing ambiguity in vector operations and facilitating the construction of linear combinations and subspaces.</p> </blockquote> <hr/> <h3 id="h-show-that-if-w-neq-v-and-v-is-the-additive-inverse-of-v-then-w--v-neq-vec0">(h) Show that if $w \neq v$ and $v‚Äô$ is the additive inverse of $v$, then $w + v‚Äô \neq \vec{0}$.</h3> <p>Let $V$ be a vector space over the field $\mathbb{R}$, equipped with the operations of vector addition $+$ and scalar multiplication $\cdot$. Within this framework, for each vector $v \in V$, there exists a unique additive inverse $v‚Äô \in V$ such that</p> \[v + v' = \vec{0},\] <p>where $\vec{0}$ denotes the additive identity in $V$.</p> <p><strong>Theorem 1.h.1.</strong> <em>In any vector space $V$ over $\mathbb{R}$, if $v‚Äô$ is the additive inverse of $v \in V$, then for any $w \in V$ satisfying $w \neq v$, the equation $w + v‚Äô \neq \vec{0}$ holds.</em></p> <p><em>Proof.</em> Assume $V$ is a vector space over $\mathbb{R}$ satisfying all vector space axioms, including the existence of additive inverses. Let $v \in V$ be an arbitrary vector, and let $v‚Äô \in V$ be its additive inverse, satisfying</p> \[v + v' = \vec{0}.\] <p>Suppose $w \in V$ is another vector such that $w \neq v$.</p> <p>Assume for the sake of contradiction that</p> \[w + v' = \vec{0}.\] <p>Then adding $v$ to both sides of the equation yield \(v + (w + v') = v + \vec{0}.\)</p> <p>By the associativity of vector addition (Axiom 3), this can be rewritten as</p> \[(v + w) + v' = v.\] <p>Given that $v + v‚Äô = \vec{0}$, substitute to obtain</p> \[\vec{0} + w = v.\] <p>By the definition of the additive identity (Axiom 2),</p> \[w = v.\] <p>This contradicts the initial assumption that $w \neq v$. Therefore, the assumption that $w + v‚Äô = \vec{0}$ must be false. Consequently,</p> \[w + v' \neq \vec{0}.\] <p>Thus, for any $w \in V$ with $w \neq v$, it holds that $w + v‚Äô \neq \vec{0}$. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentray</strong></p> <p>Through a contradiction argument grounded in the fundamental axioms of vector spaces, it has been rigorously demonstrated that if $v‚Äô$ is the additive inverse of $v \in V$, then for any $w \in V$ distinct from $v$, the sum $w + v‚Äô$ cannot equal the additive identity $\vec{0}$.</p> <p>The result established herein is a weaker assertion compared to the conclusion derived in Question (c), where the uniqueness of the additive identity itself was proven. Specifically, Question (c) demonstrated that any element $i \in V$ satisfying $i + w = w$ for some $w \in V$ must coincide with the additive identity $\vec{0}$. This directly leveraged the existence of additive inverses to facilitate the cancellation of terms, thereby establishing the equality $i = \vec{0}$ unequivocally.</p> <p>In contrast, Question (h) addresses a more nuanced scenario wherein the addition of an additive inverse $v‚Äô$ to a vector $w \neq v$ does not result in the additive identity $\vec{0}$. This conclusion does not rely on the existence of additive inverses for vectors other than $v$ and does not assert the uniqueness of additive inverses per se. Instead, it affirms that distinct vectors retain their distinctiveness when combined with the additive inverse of a specific vector, provided they are not identical to that vector.</p> <p>Thus, while Question (c) establishes a fundamental uniqueness property of the additive identity within the vector space, Question (h) extends this uniqueness to the interaction between distinct vectors and the additive inverse of a particular vector, albeit in a less general context. The latter does not encompass the full breadth of additive inverse uniqueness across the entire vector space but rather focuses on the non-vanishing of specific vector combinations.</p> </blockquote> <hr/> <h3 id="i-show-that--1-in-mathbbr-satisfies--1-cdot-v-is-the-additive-inverse-of-v-for-all-v-in-v-this-justifies-the-notation--v-to-indicate-the-additive-inverse-of-v">(i) Show that $-1 \in \mathbb{R}$ satisfies $(-1) \cdot v$ is the additive inverse of $v$ for all $v \in V$. This justifies the notation $-v$ to indicate the additive inverse of $v$.</h3> <p><strong>Theorem 1.i.1.</strong> <em>In any vector space $V$ over $\mathbb{R}$, the scalar multiplication by $-1$ yields the additive inverse of any vector $v \in V$. That is, for all $v \in V$,</em> \((-1) \cdot v = -v.\)</p> <p><em>Proof.</em> Let $V$ be a vector space over $\mathbb{R}$ satisfying all vector space axioms, including the existence of additive inverses. Let $v \in V$ be an arbitrary vector, and let $v‚Äô \in V$ denote its additive inverse, satisfying</p> \[v + v' = \vec{0}.\] <p>Consider the scalar multiplication of $-1$ with $v$:</p> \[(-1) \cdot v.\] <p>We analyze the sum of $v$ and $(-1) \cdot v$:</p> \[v + [(-1) \cdot v].\] <p>By the distributive property of scalar multiplication over vector addition (Axiom 5), we have</p> \[v + [(-1) \cdot v] = 1 \cdot v + (-1) \cdot v.\] <p>Applying the distributive property of scalar addition over scalar multiplication (Axiom 6), we obtain</p> \[1 \cdot v + (-1) \cdot v = (1 + (-1)) \cdot v.\] <p>Simplifying the scalar sum:</p> \[1 + (-1) = 0.\] <p>Thus,</p> \[(1 + (-1)) \cdot v = 0 \cdot v.\] <p>By the previously established property that $0 \cdot v = \vec{0}$ for all $v \in V$ (Theorem 1.e.1), it follows that</p> \[0 \cdot v = \vec{0}.\] <p>Substituting back, we have</p> \[v + [(-1) \cdot v] = \vec{0}.\] <p>By the definition of the additive inverse, $v‚Äô$ is the unique element in $V$ satisfying</p> \[v + v' = \vec{0}.\] <p>Comparing the two expressions, we observe that</p> \[v + [(-1) \cdot v] = v + v' = \vec{0}.\] <p>By the uniqueness of additive inverses (Theorem 1.g.1), it must be that</p> \[(-1) \cdot v = v'.\] <p>Therefore, scalar multiplication by $-1$ yields the additive inverse of $v$, establishing that</p> \[(-1) \cdot v = -v.\] <p>This equivalence justifies the conventional notation $-v$ to represent the additive inverse of $v$ in $V$. $\textbf{Q.E.D.}$</p> <hr/> <h3 id="j-show-that-if-v-neq-vec0-then-s-cdot-v-neq-vec0-for-all-s-neq-0">(j) Show that if $v \neq \vec{0}$, then $s \cdot V \neq \vec{0}$ for all $s \neq 0$.</h3> <p><strong>Theorem 1.j.1.</strong> <em>In any vector space $V$ over $\mathbb{R}$, if $v \neq \vec{0}$, then for all scalars $s \neq 0$, the scalar multiplication $s \cdot v$ satisfies $s \cdot v \neq \vec{0}$.</em></p> <p><em>Proof.</em> Let $V$ be a vector space over $\mathbb{R}$ satisfying all vector space axioms, including the existence of additive inverses and the distributive properties of scalar and vector addition. Let $v \in V$ be an arbitrary vector such that $v \neq \vec{0}$, and let $s \in \mathbb{R}$ be an arbitrary scalar with $s \neq 0$.</p> <p>For the sake of contradiction, assume that:</p> \[s \cdot v = \vec{0}.\] <p>Consider the scalar $\frac{1}{s} \in \mathbb{R}$, which exists since $s \neq 0$. Perform scalar multiplication of both sides of the equation $s \cdot v = \vec{0}$ by $\frac{1}{s}$:</p> \[\frac{1}{s} \cdot (s \cdot v) = \frac{1}{s} \cdot \vec{0}.\] <p>By the associativity of scalar multiplication (Axiom 7),</p> \[\left( \frac{1}{s} \cdot s \right) \cdot v = \vec{0},\] <p>which simplifies to</p> \[1 \cdot v = \vec{0},\] <p>since $\frac{1}{s} \cdot s = 1$.</p> <p>By the property of the scalar multiplicative identity (Axiom 8),</p> \[1 \cdot v = v.\] <p>Therefore, we have</p> \[v = \vec{0}.\] <p>This conclusion directly contradicts our initial assumption that $v \neq \vec{0}$. Hence, the assumption that $s \cdot v = \vec{0}$ must be false. Consequently, it must hold that</p> \[s \cdot v \neq \vec{0}.\] <p>Since $v \in V$ and $s \in \mathbb{R}$ were arbitrary, with the conditions $v \neq \vec{0}$ and $s \neq 0$, the result follows universally. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentary</strong></p> <p>This result underscores the injectivity of scalar multiplication by non-zero scalars within vector spaces, ensuring that non-zero vectors remain non-trivial under such operations. Specifically, Theorem 1.j.1 complements earlier established properties of vector spaces, particularly regarding the uniqueness of additive inverses and the behavior of the additive identity under scalar multiplication. While Question (c) addressed the uniqueness of the additive identity itself, and Question (g) affirmed the uniqueness of additive inverses, this result emphasizes the preservation of vector non-triviality under non-zero scalar transformations.</p> </blockquote> <hr/> <h3 id="k-show-that-if-v-is-equal-to-its-own-additive-inverse-then-v--vec0">(k) Show that if $v$ is equal to its own additive inverse, then $v = \vec{0}$.</h3> <p><strong>Teorem 1.k.1.</strong> <em>In any vector space $V$ over $\mathbb{R}$, if a vector $v \in V$ satisfies $v = -v$, then $v$ is the additive identity $\vec{0}$.</em></p> <p><em>Proof.</em> Consider the vector addition of $v$ with itself:</p> \[v + v.\] <p>Substituting $v = -v$ into the above expression, we obtain</p> \[v + v = (-v) + v.\] <p>By the commutativity of vector addition (Axiom 1),</p> \[(-v) + v = v + (-v).\] <p>By the definition of additive inverses, $v + (-v) = \vec{0}$. Therefore,</p> \[v + v = \vec{0}.\] <p>Thus, we have established that</p> \[2v = \vec{0},\] <p>where $2v$ denotes the scalar multiplication of $v$ by $2$, i.e.,</p> \[2v = 2 \cdot v.\] <p>Next, consider scalar multiplication by $\frac{1}{2}$, which is permissible since $\frac{1}{2} \in \mathbb{R}$ and $2 \neq 0$. Multiply both sides of the equation $2v = \vec{0}$ by $\frac{1}{2}$:</p> \[\frac{1}{2} \cdot (2v) = \frac{1}{2} \cdot \vec{0}.\] <p>By the associativity of scalar multiplication (Axiom 7),</p> \[\left( \frac{1}{2} \cdot 2 \right) \cdot v = \frac{1}{2} \cdot \vec{0}.\] <p>Simplifying the scalar product,</p> \[1 \cdot v = \frac{1}{2} \cdot \vec{0}.\] <p>By the property of the scalar multiplicative identity (Axiom 8),</p> \[1 \cdot v = v.\] <p>Additionally, by Theorem 1.e.1, scalar multiplication by zero yields the additive identity:</p> \[\frac{1}{2} \cdot \vec{0} = \vec{0}.\] <p>Therefore, we have</p> \[v = \vec{0}.\] <p>Thus, any vector $v \in V$ satisfying $v = -v$ must be the additive identity $\vec{0}$. $\textbf{Q.E.D.}$</p> <blockquote> <p><strong>Commentary</strong></p> <p>The theorem precludes the existence of non-trivial vectors that annihilate themselves under addition.</p> </blockquote>]]></content><author><name></name></author><category term="A Commentary of the Pre-Practice for Boston University&apos;s EC525 Course"/><category term="Mathematics"/><summary type="html"><![CDATA[Given Conditions A vector space (over the reals) is a set $V$ together with two binary operations which we call addition (+) and scalar multiplication (¬∑). The addition operation is a function which takes two elements of $V$, say $v$ and $w$, and returns a third element of $V$, which we write as $v+w$, while scalar multiplication is a function which takes a real number and an element of $V$,say $s$ and $v$, and returms another element of $V$,which we write as $s\cdot v.$ The operations are called‚Äúbinary‚Äùbecause they take two arguments. The two operations must satisfy the following properties:]]></summary></entry><entry><title type="html">Preface: Motivation and Overview</title><link href="https://shuhongdai.github.io/blog/2023/EC525_0/" rel="alternate" type="text/html" title="Preface: Motivation and Overview"/><published>2023-12-02T00:00:00+00:00</published><updated>2023-12-02T00:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2023/EC525_0</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2023/EC525_0/"><![CDATA[<h2 id="introduction">Introduction</h2> <p><a href="https://ashok.cutkosky.com/">Professor Ashok Cutkosky</a> at Boston University is teaching the renowned course <a href="https://optmlclass.github.io/">EC525: Optimization for Machine Learning (Fall 2023)</a>, which delves into the cutting-edge algorithms behind the optimization techniques that power modern deep learning models. I believe this course is especially valuable for anyone looking to build a deeper understanding of the theoretical underpinnings of deep learning, particularly when it comes to deriving complex formulas and mastering the sophisticated mathematical expressions often used in academic research.</p> <p>One thing I really appreciate about Professor Cutkosky‚Äôs approach is his awareness of how challenging proof-writing can be for many students. To address this, he has provided <a href="/assets/pdf/2023-12-02/Exercises_on_Abstract_Structures.pdf">a series of exercises</a> designed specifically to help students hone their proof-writing skills. These exercises are a great way to solidify the foundational concepts that drive the optimization techniques discussed in the course.</p> <blockquote> <p>Additionally, in these exercises, Professor Cutkosky provides several helpful suggestions and guidelines to assist with proof-writing:</p> <ol> <li><strong>Stick to the basics</strong>: Focus on the definitions and axioms provided in the problem. Avoid the temptation to bring in more advanced concepts like vector space bases or singular value decompositions.</li> <li><strong>Don‚Äôt assume things are obvious</strong>: Many statements might seem self-evident, but crafting a rigorous proof often requires more careful thought.</li> <li><strong>Use the axioms methodically</strong>: When stuck, work through the list of axioms one by one. It‚Äôs easy to overlook less commonly used ones that might be the key to the solution.</li> <li><strong>Leverage contrapositive reasoning</strong>: Remember that the contrapositive of a statement, $A \implies B$, is logically equivalent to $\neg B \implies \neg A$. This technique can often simplify the proof process.</li> </ol> <p>These strategies are crucial for tackling the challenges of abstract proof-writing.</p> </blockquote> <p>To follow Professor Cutkosky‚Äôs course and share my own solutions and reflections on these exercises, I‚Äôve launched this blog series titled <strong>A Commentary of the Pre-Practice for Boston University‚Äôs EC525 Course</strong>. It is expected to include 8 posts: this preface, 6 posts with detailed commentary on each of the 6 problems, and a concluding post. I will update this series periodically, and you can click <a href="https://shuhongdai.github.io/blog/category/a-review-of-the-pre-practice-for-boston-university-s-ec525-course/">here</a> to bookmark it or access it directly via the hyperlink at the end of the page.</p> <blockquote> <p>Notably, based on their types, I will assign appropriate titles to the blogs for each of the 6 problems to make them easier to reference.</p> </blockquote> <hr/> <h2 id="quick-navigation">Quick Navigation</h2> <ul> <li> <p><strong>Preface: Motivation and Overview</strong> (You are currently browsing this post)</p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2023/EC525_1/">Problem ‚Ö†: Vector Spaces</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_2/">Problem ‚Ö°: Linear Transformations</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_3/">Problem ‚Ö¢: Group Theory</a></p> </li> <li> <p><a href="(https://shuhongdai.github.io/blog/2024/EC525_4/)">Problem ‚Ö£: Dual Spaces and Functional Analysis</a></p> </li> <li> <p><a href="(https://shuhongdai.github.io/blog/2024/EC525_5/)">Problem ‚Ö§: Infinite Sequences and Combinatorial Principles</a></p> </li> <li> <p><a href="https://shuhongdai.github.io/blog/2024/EC525_6/">Problem ‚Ö•: Set Theory and Cardinality</a></p> </li> <li> <p>Conclusion</p> </li> </ul>]]></content><author><name></name></author><category term="A Commentary of the Pre-Practice for Boston University&apos;s EC525 Course"/><category term="Mathematics"/><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>