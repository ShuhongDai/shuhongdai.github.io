<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shuhongdai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shuhongdai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-19T06:44:39+00:00</updated><id>https://shuhongdai.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Quick Mental Model for Estimating LLM GPU Memory Use</title><link href="https://shuhongdai.github.io/blog/2025/A_Quick_Mental_Model_for_Estimating_LLM_GPU_Memory_Use/" rel="alternate" type="text/html" title="A Quick Mental Model for Estimating LLM GPU Memory Use"/><published>2025-11-17T01:00:00+00:00</published><updated>2025-11-17T01:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/A_Quick_Mental_Model_for_Estimating_LLM_GPU_Memory_Use</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/A_Quick_Mental_Model_for_Estimating_LLM_GPU_Memory_Use/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>When browsing new open-source LLM releases, I often have a simple question in mind:<br/> <strong>Will this model actually fit on my GPU?</strong></p> <p>Sometimes the model page shows numbers like \(7\text{B}\), \(14\text{B}\), or \(70\text{B}\), but that alone doesn’t immediately translate into how much memory the model needs once loaded and running. And when I only want a quick sanity check, I don’t want to:</p> <ul> <li>download tens of gigabytes of weights,</li> <li>install a full environment,</li> <li>start a runtime,</li> <li>and only then discover that the model does not fit on the device at all.</li> </ul> <p>For this kind of lightweight judgment, a rough mental model is far more helpful than an exact calculator. It doesn’t need to be accurate to the megabyte. It only needs to answer a practical question: <strong>“Roughly fits?” or “Clearly too large?”</strong></p> <p>This note summarizes the approximation that I use. It’s not a formal derivation. It’s simply a way to reason about LLM memory requirements quickly, in a way that works consistently across models.</p> <hr/> <h2 id="what-actually-occupies-gpu-memory">What Actually Occupies GPU Memory</h2> <p>For inference (not training), only a few components meaningfully consume GPU memory:</p> <ol> <li>The model weights</li> <li>The key–value cache used during autoregressive generation</li> <li>Runtime overhead: intermediate buffers, framework allocations, small activations</li> </ol> <p>Optimizer state does not exist during inference, so the overall picture is simpler than training.</p> <p>My routine is just:</p> <ul> <li>estimate weight memory,</li> <li>add the KV cache,</li> <li>apply a small safety margin.</li> </ul> <hr/> <h2 id="from-parameters-to-vram">From Parameters to VRAM</h2> <p>Model parameter counts are usually prominently displayed: \(7\text{B}\), \(13\text{B}\), \(34\text{B}\), \(70\text{B}\), and so on. Converting this into VRAM is straightforward once we remember how many bytes each parameter uses.</p> <p>Typical cases:</p> <ul> <li><strong>FP16 / BF16:</strong> \(2\) bytes per parameter</li> <li><strong>FP32:</strong> \(4\) bytes</li> <li><strong>8-bit quantization:</strong> about \(1\) byte</li> <li><strong>4-bit quantization:</strong> about \(0.5\) byte</li> </ul> <p>This is already enough for fast estimation:</p> <ul> <li><strong>7B FP16 model</strong> → roughly \(7 \times 2\) GB ≈ <strong>14 GB</strong></li> <li><strong>13B FP16 model</strong> → \(13 \times 2\) GB ≈ <strong>26 GB</strong></li> <li><strong>7B 4-bit model</strong> → \(7 \times 0.5\) GB ≈ <strong>3.5 GB</strong></li> </ul> <p>This accounts only for the parameters, not the KV cache.</p> <hr/> <h2 id="kv-cache-and-context-length">KV Cache and Context Length</h2> <p>During generation, transformer decoders maintain a key–value cache for each attention layer. Its size scales with:</p> <ul> <li>number of layers,</li> <li>hidden dimension,</li> <li>context length.</li> </ul> <p>The exact computation is more detailed, but for estimation purposes, only the magnitude matters. In practice: <strong>KV cache often contributes hundreds of megabytes to several gigabytes</strong>, depending on the context window.</p> <p>For many modern \(6\text{B}\)–\(8\text{B}\)-scale models in FP16:</p> <ul> <li>every \(1\text{k}\) tokens of context usually costs <strong>a few hundred MB</strong> of KV cache</li> <li>a \(4\text{k}\)–\(8\text{k}\) context easily adds <strong>1–3 GB</strong></li> <li>long-context models might require more</li> </ul> <p>This rough rule-of-thumb is accurate enough to determine whether a model with a particular context window fits on an average \(24\text{GB}\)–\(48\text{GB}\) GPU.</p> <hr/> <h2 id="putting-the-pieces-together">Putting the Pieces Together</h2> <p>Once the weight size and KV cache are roughly known, the total memory is just the sum plus a safety margin. In practice, I use a simple heuristic.</p> <p>First, compute an approximate parameter memory from:</p> \[\text{ParameterMemory} \approx \text{NumberOfParameters} \times \text{BytesPerParameter}\] <p>Then add room for the KV cache, based on the context length. Finally, leave some headroom to account for framework overhead and fragmentation.</p> <p>If I want to compress this into a single sentence I can recall mentally, it would be: Take the parameter size, add a few gigabytes for KV cache, add a buffer, and that’s your practical VRAM requirement.</p> <hr/> <h2 id="additional-notes">Additional Notes</h2> <p>A few architectural details can influence memory usage in practice, even when using the simple estimation model above:</p> <ol> <li> <p><strong>KV cache usage varies across architectures</strong> Hidden sizes and layer counts differ between models. For example, even at similar 7B scales, newer efficient architectures such as <strong>Qwen2.5</strong> and <strong>Mistral</strong> typically use less KV memory per 1k tokens than earlier LLaMA-style models, and smaller models require even less. The “hundreds of MB per 1k tokens” rule still holds, but the exact amount can vary.</p> </li> <li> <p><strong>A small amount of extra weight tensors exists</strong> Beyond the main linear weights, models include embeddings, LayerNorm parameters, and other small tensors. These usually contribute under 5% of the total size, so the rough estimate remains valid, but the actual VRAM will be slightly higher than the simple parameter × bytes calculation.</p> </li> <li> <p><strong>Quantized models may include additional metadata</strong> Although 4-bit and 8-bit quantization can be estimated as 0.5 or 1 byte per parameter, many implementations store per-channel scales, zero-points, or other auxiliary data. This means the practical VRAM usage is often somewhat higher than the theoretical minimum.</p> </li> </ol> <p>These nuances don’t affect the overall direction of the estimate but matter when pushing GPU limits or optimizing for tight VRAM constraints.</p>]]></content><author><name></name></author><category term="LLMs"/><category term="GPU Memory"/><summary type="html"><![CDATA[Before downloading a large model or spinning up a container, it’s useful to know whether an open-source LLM will actually fit on your GPU.]]></summary></entry><entry><title type="html">Designing a Maintainable Replay Buffer in RL Systems</title><link href="https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_RL_Systems/" rel="alternate" type="text/html" title="Designing a Maintainable Replay Buffer in RL Systems"/><published>2025-10-21T12:31:00+00:00</published><updated>2025-10-21T12:31:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_RL_Systems</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_RL_Systems/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In RL implementations, the replay buffer often appears as a modest component that is essential but rarely the center of discussion. It stores transitions and serves mini-batches for training updates, and in many introductory materials, it is presented as a straightforward queue with random access.</p> <p>However, once RL systems move beyond toy prototypes and begin supporting extensible algorithms, varied environments, or long-running experiments, the replay buffer quickly becomes a structural foundation rather than a convenience. Its design influences not only training stability but also code clarity, maintainability, and the ease with which new ideas can be incorporated into the system.</p> <p>This post offers a engineering-oriented reflection on replay buffer design: what purposes it actually serves, what structures tend to lead to long-term stability, and what principles help prevent complications as a project grows.</p> <hr/> <h2 id="the-replay-buffer-as-a-dataflow-node">The Replay Buffer as a Dataflow Node</h2> <p>Although we often describe a replay buffer as a “storage mechanism,” in practice it functions as a <strong>dataflow node</strong> inside an RL system. It mediates between:</p> <ul> <li>The environment, which produces experience,</li> <li>The learning update loop, which consumes it,</li> <li>Auxiliary modules (logging, metrics, evaluation),</li> <li>And in many cases, reproducibility mechanisms.</li> </ul> <p>Shifting perspective from “container” to “dataflow component” clarifies its role. The buffer is not just a passive holder of transitions; it enforces data consistency, defines boundaries between modules, and often determines how easily an RL system can scale or adapt to new requirements.</p> <p>As RL algorithms diversify—off-policy, on-policy with auxiliary replay, offline RL, model-based RL—the replay buffer subtly shifts shape, yet the underlying structural demands stay surprisingly consistent.</p> <hr/> <h2 id="common-replay-buffer-structures-in-existing-systems">Common Replay Buffer Structures in Existing Systems</h2> <p>Across different RL codebases, replay buffers often take one of a few recognizable forms:</p> <ul> <li><strong>Simple Python-list–based buffers</strong>, prioritizing simplicity over structure.</li> <li><strong>Dictionary-based buffers</strong>, offering flexibility through named fields.</li> <li><strong>Preallocated NumPy array buffers</strong>, emphasizing performance and predictable behavior.</li> <li><strong>Dataset-style buffers</strong>, seen in offline RL or large-scale frameworks such as RLlib.</li> </ul> <p>Each of these reflects a particular engineering priority—ease of prototyping, multi-field flexibility, high throughput training, or dataset compatibility. None is inherently incorrect; instead, they sit at different points in the design space. Recognizing this variety helps clarify what constraints and opportunities a more maintainable version should satisfy.</p> <hr/> <h2 id="why-maintainability-matters">Why Maintainability Matters</h2> <p>The replay buffer is one of the few components that interacts with <strong>every</strong> part of an RL pipeline:</p> <ul> <li>Environment interaction</li> <li>Training loops</li> <li>Logging &amp; monitoring</li> <li>Sampling strategies</li> <li>Distributed actors (if applicable)</li> </ul> <p>Because of this centrality, small inconsistencies—shape mismatches, implicit assumptions, overly coupled fields—tend to propagate widely. A design that works for a narrow experiment may later resist extensions such as:</p> <ul> <li>Adding new features (e.g., log-prob, target values),</li> <li>Switching to new environments with richer info fields,</li> <li>Supporting truncated vs. terminated distinctions,</li> <li>Integrating prioritized replay or sequence sampling.</li> </ul> <p>Maintainability, therefore, is less about making the implementation “clean” and more about preserving <strong>structural integrity under change</strong>.</p> <hr/> <h2 id="design-principles-for-a-maintainable-replay-buffer">Design Principles for a Maintainable Replay Buffer</h2> <p>A replay buffer that aims to support a wide range of algorithms and experiments should follow several straightforward but impactful principles. These principles arise not from performance tuning but from the need for clear and robust system behavior.</p> <h3 id="1-a-clear-and-explicit-data-schema"><strong>1. A Clear and Explicit Data Schema</strong></h3> <p>Each field—observations, actions, rewards, next observations, termination indicators—should be explicitly represented. Buffers that try to infer structure implicitly often break when new algorithms introduce additional fields.</p> <p>A good schema clearly defines:</p> <ul> <li>What each field contains</li> <li>Its shape</li> <li>Its dtype</li> <li>When and how it gets written</li> </ul> <p>This clarity reduces ambiguity during sampling and training.</p> <hr/> <h3 id="2-independence-between-fields"><strong>2. Independence Between Fields</strong></h3> <p>Transitions should not be stored as monolithic tuples. Instead, each field should maintain its own array or storage structure. This approach improves:</p> <ul> <li>Testability</li> <li>Clarity</li> <li>The ability to extend fields independently</li> <li>Compatibility with batch indexing</li> </ul> <p>Field independence also minimizes the risk of “ripple effects” when experimenting with alternative data encodings or additional metadata.</p> <hr/> <h3 id="3-preallocation-with-predictable-behavior"><strong>3. Preallocation with Predictable Behavior</strong></h3> <p>A fixed-size ring buffer with preallocated arrays is a stable and predictable design. It avoids:</p> <ul> <li>Memory fragmentation</li> <li>Resizing overhead</li> <li>Ambiguous buffer growth behavior</li> </ul> <p>A simple write pointer and size counter are often all that is needed. Predictability is more valuable than cleverness in this case.</p> <hr/> <h3 id="4-decoupled-sampling-logic"><strong>4. Decoupled Sampling Logic</strong></h3> <p>Sampling strategies evolve rapidly in RL research. Keeping sampling logic separate from data storage enables easier experimentation with:</p> <ul> <li>Uniform random sampling</li> <li>Stratified sampling</li> <li>Prioritized replay</li> <li>Sequential sampling for RNN-based agents</li> <li>Temporal batch sampling for long-horizon tasks</li> </ul> <p>A buffer whose storage does not constrain sampling enables more flexible algorithm development.</p> <hr/> <h3 id="5-stable-batch-shapes-and-typing"><strong>5. Stable Batch Shapes and Typing</strong></h3> <p>One of the most common sources of bugs is inconsistent shapes. By fixing shapes and dtypes at initialization, and validating them at write time, the buffer ensures:</p> <ul> <li>Training loops remain stable</li> <li>Models receive predictable inputs</li> <li>Misconfigured environments are detected early</li> </ul> <p>This principle applies across vector observations, discrete actions, continuous actions, or any other modality.</p> <hr/> <h2 id="a-practical-replay-buffer-structure">A Practical Replay Buffer Structure</h2> <p>A maintainable replay buffer often adopts a design similar to the following conceptual structure:</p> <ul> <li>A <strong>capacity</strong>, defining maximum size</li> <li>A <strong>write index</strong>, controlling where new transitions are stored</li> <li>A <strong>current size</strong>, indicating the valid range</li> <li>A dictionary of <strong>fields</strong>, each with its own preallocated array</li> <li>A sampling interface that accepts batch indices and returns batched transitions</li> </ul> <p>This structure supports:</p> <ul> <li>Easy extension（adding new fields is straightforward）</li> <li>Consistent sampling（batch index array drives selection）</li> <li>Ability to swap or enhance sampling mechanisms</li> <li>Compatibility with on-policy or off-policy algorithms</li> </ul> <p>What matters is not the exact API surface, but the emphasis on <strong>clarity</strong>, <strong>decoupling</strong>, and <strong>predictability</strong>.</p> <hr/> <h2 id="memory-and-performance-considerations">Memory and Performance Considerations</h2> <p>Replay buffers operate under tight memory constraints in some RL settings (e.g., long-horizon environments or high-frequency transitions). Reasonable engineering considerations include:</p> <ul> <li>Choosing appropriate dtypes (e.g., <code class="language-plaintext highlighter-rouge">float32</code> vs. <code class="language-plaintext highlighter-rouge">float64</code>)</li> <li>Managing non-essential fields carefully</li> <li>Using contiguous arrays to reduce overhead</li> <li>In distributed setups, deciding which side handles storage</li> </ul> <p>While performance matters, in most research or mid-scale systems, clarity and consistency should take precedence.</p> <hr/> <h2 id="the-role-of-replay-buffers-in-larger-rl-architectures">The Role of Replay Buffers in Larger RL Architectures</h2> <p>As RL systems grow, so does the role of the replay buffer:</p> <ul> <li>In <strong>off-policy RL</strong>, it is a core stabilizer.</li> <li>In <strong>offline RL</strong>, it becomes the primary dataset abstraction.</li> <li>In <strong>model-based RL</strong>, it may store both real and imagined transitions.</li> <li>In <strong>multi-agent RL</strong>, it may coordinate data from multiple agents or environments.</li> <li>In <strong>distributed RL</strong>, it may act as a central data service.</li> </ul> <p>A well-designed replay buffer scales gracefully across these contexts without structural changes.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>The replay buffer is one of the most important infrastructural components in RL systems. Although often overshadowed by policy networks or optimization algorithms, its design directly impacts the clarity, reliability, and extensibility of an RL codebase. A maintainable buffer is built on simple but robust principles: clear schema, independent fields, predictable behavior, and decoupled sampling.</p>]]></content><author><name></name></author><category term="RL"/><category term="System Design"/><category term="Data Structures"/><summary type="html"><![CDATA[A structured and engineering-focused reflection on replay buffer design in RL, emphasizing clarity, extensibility, and long-term maintainability.]]></summary></entry><entry><title type="html">Tracing the Root Cause of Missing GPUs in Docker Containers</title><link href="https://shuhongdai.github.io/blog/2025/Tracing_the_Root_Cause_of_Missing_GPUs_in_Docker_Containers/" rel="alternate" type="text/html" title="Tracing the Root Cause of Missing GPUs in Docker Containers"/><published>2025-08-20T01:02:00+00:00</published><updated>2025-08-20T01:02:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Tracing_the_Root_Cause_of_Missing_GPUs_in_Docker_Containers</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Tracing_the_Root_Cause_of_Missing_GPUs_in_Docker_Containers/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The GPU on my host machine worked flawlessly. <code class="language-plaintext highlighter-rouge">nvidia-smi</code> showed four cleanly indexed devices. CUDA tests ran without complaint. Nothing looked suspicious. But inside Docker, those same GPUs simply vanished. The container insisted it had no GPU at all, even when launched with <code class="language-plaintext highlighter-rouge">--gpus all</code>. This was not an application-level issue. Problems like this never come from the code running inside the container. They come from a mismatch somewhere between Docker, the NVIDIA runtime, and the host’s driver stack. This post is a reconstruction of how I verified that assumption and worked my way through the layers until the system finally admitted what was wrong.</p> <hr/> <h2 id="the-initial-symptom">The Initial Symptom</h2> <p>The first sign of trouble came from a simple test:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">--gpus</span> all ubuntu:22.04 nvidia-smi
</code></pre></div></div> <p>The output didn’t show a GPU. It didn’t even show a driver mismatch. Instead I received:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.
</code></pre></div></div> <p>This message is always unhelpful. It can mean anything from missing libraries to container runtime failures to a completely broken driver. But since <code class="language-plaintext highlighter-rouge">nvidia-smi</code> worked perfectly on the host, the problem had to be elsewhere.</p> <p>I checked the host first, just to be certain.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-smi
Sun Aug 17 00:15:19 2025
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 550.54       Driver Version: 550.54       CUDA Version: 12.4     |
| GPU Name        Persistence-M| Bus-Id ... <span class="o">(</span>normal output<span class="o">)</span>                   |
+-----------------------------------------------------------------------------+
</code></pre></div></div> <p>Everything here was healthy. That told me the problem was not hardware. It also told me the Docker container was not receiving the correct runtime environment.</p> <hr/> <h2 id="a-quick-look-at-the-nvidia-container-toolkit">A Quick Look at the NVIDIA Container Toolkit</h2> <p>The next step was confirming that the NVIDIA container runtime existed on the host.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dpkg <span class="nt">-l</span> | <span class="nb">grep</span> <span class="nt">-i</span> nvidia-container
ii  nvidia-container-toolkit  1.16.2-1  amd64
</code></pre></div></div> <p>The toolkit was installed, at least according to the package manager. But “installed” is not the same as “integrated.” I checked Docker’s runtime configuration:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /etc/docker/daemon.json
<span class="o">{</span>
    <span class="s2">"runtimes"</span>: <span class="o">{</span>
        <span class="s2">"nvidia"</span>: <span class="o">{</span>
            <span class="s2">"path"</span>: <span class="s2">"nvidia-container-runtime"</span>,
            <span class="s2">"runtimeArgs"</span>: <span class="o">[]</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div> <p>At first glance this looked reasonable but Docker’s configuration files often lie by omission. I restarted Docker anyway, hoping for the rare case where a restart solves a real problem.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>systemctl restart docker
</code></pre></div></div> <p>It changed nothing.</p> <hr/> <h2 id="the-runtime-that-didnt-exist">The Runtime That Didn’t Exist</h2> <p>I tried manually invoking the runtime:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>which nvidia-container-runtime
/usr/bin/nvidia-container-runtime
</code></pre></div></div> <p>It was present. But inside Docker, the container still couldn’t see GPUs. I suspected mismatch in library paths, so I inspected the toolkit’s log:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>journalctl <span class="nt">-u</span> nvidia-container-runtime
</code></pre></div></div> <p>The log was silent with no errors and no warnings, which is often more suspicious than a screaming log file.</p> <hr/> <h2 id="trying-a-minimal-container">Trying a Minimal Container</h2> <p>Sometimes it helps to remove everything unrelated. I tried an empty Alpine container with explicit runtime:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--runtime</span><span class="o">=</span>nvidia <span class="nt">--rm</span> nvidia/cuda:12.4-base nvidia-smi
</code></pre></div></div> <p>The result was the same:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.
</code></pre></div></div> <p>At this point I was certain the issue wasn’t in the image. The problem was in the host-to-container plumbing.</p> <hr/> <h2 id="nvidia-container-cli">nvidia-container-cli</h2> <p>I checked low-level diagnostics:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-container-cli info
</code></pre></div></div> <p>This should list driver version, devices, and capabilities. Instead it printed:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ERRO[0000] could not load NVML: libnvidia-ml.so.1: cannot open shared object file: No such file or directory
</code></pre></div></div> <p>The NVML library is part of the NVIDIA driver. If the toolkit couldn’t load it, that meant the toolkit’s library search paths did not match the actual driver installation. Which usually happens after a driver upgrade that leaves symlinks pointing to the wrong place.</p> <p>I checked the library:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> /usr/lib/x86_64-linux-gnu/libnvidia-ml.so<span class="k">*</span>
</code></pre></div></div> <p>The files were indeed present, but I noticed they were under:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/lib/x86_64-linux-gnu/nvidia/current/
</code></pre></div></div> <p>while the toolkit expected:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/lib/x86_64-linux-gnu/
</code></pre></div></div> <p>This mismatch is easy to miss.</p> <hr/> <h2 id="the-missing-symlink">The Missing Symlink</h2> <p>On many systems, <code class="language-plaintext highlighter-rouge">/usr/lib/x86_64-linux-gnu/</code> should contain symlinks to the actual driver libraries, but mine didn’t. I created the symlink manually:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo ln</span> <span class="nt">-s</span> /usr/lib/x86_64-linux-gnu/nvidia/current/libnvidia-ml.so.1 <span class="se">\</span>
             /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
</code></pre></div></div> <p>Then I tried the diagnostic again:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-container-cli info
</code></pre></div></div> <p>This time it printed a full report with devices, drivers, and capabilities listed correctly. That told me NVIDIA’s container toolkit could finally see the GPU.</p> <hr/> <h2 id="the-final-test">The Final Test</h2> <p>With everything aligned, I launched a fresh container:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">--gpus</span> all nvidia/cuda:12.4-base nvidia-smi
</code></pre></div></div> <p>The output looked normal.</p>]]></content><author><name></name></author><category term="Docker"/><category term="NVIDIA"/><category term="CUDA"/><summary type="html"><![CDATA[A debugging record of why Docker refused to expose GPUs inside a container even though the host recognized them perfectly, and how every layer of the system contributed a small piece to the failure.]]></summary></entry><entry><title type="html">A Measure of Range Compression on Different Board Textures</title><link href="https://shuhongdai.github.io/blog/2025/A_Measure_of_Range_Compression_on_Different_Board_Textures/" rel="alternate" type="text/html" title="A Measure of Range Compression on Different Board Textures"/><published>2025-08-03T03:10:00+00:00</published><updated>2025-08-03T03:10:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/A_Measure_of_Range_Compression_on_Different_Board_Textures</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/A_Measure_of_Range_Compression_on_Different_Board_Textures/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>I noticed a pattern that I had always felt intuitively but never formalized. Certain flops seemed to “collapse” the strategic possibilities of both players, making ranges narrower, more predictable, and structurally simpler. Others seemed to do the opposite, expanding the number of viable continuations.</p> <p>This observation naturally led to a question:</p> <p><strong>Is there a mathematical way to measure how much a flop compresses (or expands) a range?</strong></p> <p>Poker players talk about “range advantage” and “nut advantage” frequently, but rarely about <em>range volume</em> what is the total weight of possible holdings consistent with rational play. Compressing this volume changes not only equities but also the informational structure of the hand.</p> <p>In this post, I propose a formal measure of <strong>range compression</strong>, analyze its mathematical properties, and illustrate how different board textures affect the relationship between pre-flop and post-flop ranges.</p> <hr/> <h2 id="range-volume">Range Volume</h2> <p>Let \(R\) be a player’s pre-flop range, represented as a set of weighted combinations:</p> \[R = \{ (h_i, w_i) \}_{i=1}^N,\] <p>where \(h_i\) is a specific starting hand and \(w_i\) is its probability weight.</p> <p>Define the <strong>range volume</strong> as:</p> \[V(R) = \sum_{i=1}^N w_i.\] <p>For standard pre-flop ranges normalized to 100%, we simply have \(V(R) = 1\). However, the concept becomes informative <strong>after conditioning on a board</strong>.</p> <hr/> <h2 id="conditioning-a-range-on-a-board">Conditioning a Range on a Board</h2> <p>Let \(B\) be a flop (e.g., \(\text{A♠ 7♦ 3♠}\)). Any starting hand inconsistent with \(B\) must be removed.</p> <p>Define the conditioned range:</p> \[R \mid B = \{ (h_i, w_i) : h_i \text{ does not conflict with } B \}.\] <p>Then the post-flop range volume is:</p> \[V(R \mid B) = \sum_{h_i \not\!\pitchfork\, B} w_i.\] <p>This is the <strong>raw volume</strong>, reflecting how much of the pre-flop range survived the flop card removal.</p> <hr/> <h2 id="range-compression-ratio">Range Compression Ratio</h2> <p>Now we define the central object of this article:</p> \[\rho(B) = \frac{V(R \mid B)}{V(R)}.\] <p>Because \(V(R) = 1\) pre-flop, \(\rho(B) = V(R \mid B)\) directly measures how much of the range remains viable after the board is revealed.</p> <p>Interpretation:</p> <ul> <li>If \(\rho(B) \approx 1\) → <strong>little compression</strong>, wide continuation range</li> <li>If \(\rho(B) \ll 1\) → <strong>high compression</strong>, many hands eliminated</li> <li>If two players have different \(\rho\), the one with higher \(\rho\) often holds <strong>range advantage</strong></li> </ul> <hr/> <h2 id="structural-decomposition-of-compression">Structural Decomposition of Compression</h2> <p>To understand what drives \(\rho(B)\), we decompose it into components.</p> <h3 id="1-card-removal-compression">1. Card-Removal Compression</h3> <p>This is the strict combinatorial effect:</p> \[\rho_{\text{removal}}(B) = \frac{\text{# of surviving combos}}{\text{# of total combos}}.\] <p>For example, removing an Ace from the deck eliminates 3 combinations of every Ax hand but leaves others untouched.</p> <hr/> <h3 id="2-strength-based-compression">2. Strength-Based Compression</h3> <p>Beyond card removal, a board may render many holdings strategically non-viable.</p> <p>To model this, define a viability indicator function:</p> \[\chi(h_i, B) = \begin{cases} 1, &amp; \text{if } h_i \text{ is playable on } B, \\ 0, &amp; \text{otherwise}. \end{cases}\] <p>Playability may be defined through:</p> <ul> <li>minimum equity threshold</li> <li>minimum EV threshold</li> <li>solver-derived continuation frequency</li> </ul> <p>Then define:</p> \[\rho_{\text{strength}}(B) = \sum_i w_i \chi(h_i, B).\] <hr/> <h3 id="3-total-compression">3. Total Compression</h3> <p>Putting the two together:</p> \[\rho(B) = \rho_{\text{removal}}(B) \times \rho_{\text{strength}}(B).\] <p>This formula mirrors classical probability decompositions:</p> <ul> <li>structural elimination (card removal)</li> <li>behavioral elimination (strategic folding)</li> </ul> <p>The same structure appears in Bayesian conditioning.</p> <hr/> <h2 id="examples">Examples</h2> <p>To illustrate how different boards shape the range, I ran simulations on a typical button-opening range against a big blind defend range.</p> <p>Here are approximate compression ratios for representative flops:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Board           ρ(B)
A♠ K♦ 5♣        0.31
7♣ 8♣ 9♠        0.54
Q♥ 7♠ 2♦        0.62
3♦ 3♣ 3♠        0.95
T♠ J♠ Q♠        0.28

</code></pre></div></div> <p>Interpretation:</p> <ul> <li><strong>A K 5 rainbow</strong> highly compresses ranges: many hands are dead or dominated.</li> <li><strong>777 or 333 boards</strong> preserve most of the range volume: few hands are eliminated.</li> <li><strong>T J Q monotone</strong> compresses massively due to strong dominance and nuttiness.</li> <li><strong>Q 7 2 rainbow</strong> is one of the least compressive typical flops.</li> </ul> <p>These results align well with expert intuition, but here they arise from formal volume computation.</p> <hr/> <h2 id="information-compression">Information Compression</h2> <p>Volume alone does not capture how <em>uncertain</em> a player’s range remains. A more refined measure is the <strong>entropy</strong> of the range:</p> \[H(R) = - \sum_i w_i \log w_i.\] <p>After conditioning on a board:</p> \[H(R \mid B) = - \sum_{h_i \not\!\pitchfork\, B} \frac{w_i}{\rho(B)} \log \left( \frac{w_i}{\rho(B)} \right).\] <p>Define information compression:</p> \[\kappa(B) = \frac{H(R \mid B)}{H(R)}.\] <p>Now we have two complementary measures:</p> <ul> <li>\(\rho(B)\) → <em>how many</em> hands survive</li> <li>\(\kappa(B)\) → <em>how much uncertainty</em> survives</li> </ul> <p>A flop like A K 5 drastically reduces both and a flop like 3 3 3 hardly affects either.</p> <hr/> <h2 id="an-information-theoretic-interpretation">An Information-Theoretic Interpretation</h2> <p>Let the range be a probability distribution over combos. Let actions be random variables dependent on the board. Then the mutual information between range and action is:</p> \[I(R; B) = H(R) - H(R \mid B).\] <p>A highly compressive board has high \(I(R; B)\). This reframes board texture as an information revelation process:</p> <ul> <li>Dry boards reveal almost nothing.</li> <li>A K x reveals a lot.</li> <li>Coordinated connected boards reveal a different kind of structure (relative nuts density).</li> </ul> <hr/> <h2 id="conclusion">Conclusion</h2> <p>This article introduced a mathematical framework for quantifying how different board textures compress pre-flop ranges. By defining:</p> <ul> <li>range volume \(\rho(B)\)</li> <li>information compression \(\kappa(B)\)</li> <li>and decomposing them into structural and strategic components</li> </ul> <p>we obtain a principled way to study the impact of board textures on strategic play.</p>]]></content><author><name></name></author><category term="Poker"/><category term="Probability"/><category term="Combinatorics"/><category term="Game Theory"/><category term="Information Theory"/><summary type="html"><![CDATA[A mathematical approach to quantifying how different flop textures compress or expand pre-flop ranges in No-Limit Hold’em.]]></summary></entry><entry><title type="html">Running dm-control on a Headless Server: A Complete Debugging Log</title><link href="https://shuhongdai.github.io/blog/2025/Running-_dm-control-_on_a_Headless_Server/" rel="alternate" type="text/html" title="Running dm-control on a Headless Server: A Complete Debugging Log"/><published>2025-06-15T21:12:00+00:00</published><updated>2025-06-15T21:12:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Running%20_dm-control%20_on_a_Headless_Server</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Running-_dm-control-_on_a_Headless_Server/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post documents the process of configuring <strong>dm-control</strong> on a <strong>headless Ubuntu server</strong> for reinforcement learning experiments that require pixel-based observations. The goal was straightforward: load a task from the DeepMind Control Suite and render frames for a CNN-based policy. However, running dm-control without a graphical display consistently triggered a series of rendering failures.</p> <p>Rather than providing a tutorial, this article records the issues encountered, the different directions explored, and the final configuration that proved reliable. Hopefully, this log will help anyone attempting to run dm-control in a similar environment.</p> <hr/> <h2 id="initial-attempts-and-rendering-errors">Initial Attempts and Rendering Errors</h2> <p>The starting point was simply loading a task and calling <code class="language-plaintext highlighter-rouge">env.physics.render()</code>. On a local machine, this works immediately. On a headless server, the first result was an error referencing <strong>EGL</strong> and <strong>OpenGL</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
AttributeError: 'NoneType' object has no attribute 'eglQueryString'

</code></pre></div></div> <p>Further attempts produced warnings about missing <code class="language-plaintext highlighter-rouge">DISPLAY</code> variables:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
X11: The DISPLAY environment variable is missing

</code></pre></div></div> <p>These messages made it clear that dm-control was attempting to initialize rendering through OpenGL/EGL, both of which require a graphics context the server did not provide. Disabling the <code class="language-plaintext highlighter-rouge">DISPLAY</code> variable or forcing EGL did not resolve the issue; the underlying environment simply lacked the dependencies required for hardware-accelerated rendering.</p> <hr/> <h2 id="investigating-the-role-of-pyopengl">Investigating the Role of PyOpenGL</h2> <p>The repeated mentions of EGL led to checking whether PyOpenGL played a role. Removing PyOpenGL temporarily modified the error messages, but it was automatically reinstalled when upgrading or reinstalling dm-control and Mujoco. This indicated that avoiding the OpenGL backend entirely was not feasible through uninstalling PyOpenGL alone.</p> <hr/> <h2 id="version-mismatch-between-dm-control-and-mujoco">Version Mismatch Between dm-control and Mujoco</h2> <p>At a later stage, entirely different errors appeared:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
eq_active not found
flex_xvert0 not found

</code></pre></div></div> <p>These errors are characteristic of <strong>Mujoco internal structure mismatches</strong>, suggesting that the installed versions of Mujoco and dm-control were not aligned. dm-control indexes Mujoco model structures by field name, and when they do not match, initialization fails even before rendering begins.</p> <p>This confirmed that two separate issues existed:</p> <ol> <li>Rendering backend not compatible with the server</li> <li>dm-control and Mujoco versions not compatible with each other</li> </ol> <p>Both needed to be addressed.</p> <hr/> <h2 id="moving-toward-a-software-rendering-approach">Moving Toward a Software Rendering Approach</h2> <p>Given that neither EGL nor OpenGL would work reliably on the server, the next candidate was <strong>OSMesa</strong>, a pure software renderer. OSMesa performs all rendering on the CPU and does not require a graphical display or GPU drivers. This makes it suitable for cloud or containerized environments.</p> <p>Ubuntu provides OSMesa through:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
libosmesa6-dev

</code></pre></div></div> <p>The key environment variable for Mujoco is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
MUJOCO_GL=osmesa

</code></pre></div></div> <p>With this backend, Mujoco no longer attempts to initialize hardware-accelerated contexts.</p> <hr/> <h2 id="identifying-a-stable-mujoco--dm-control-combination">Identifying a Stable Mujoco + dm-control Combination</h2> <p>After testing multiple combinations, the following versions proved to be both compatible with each other and functional under OSMesa:</p> <ul> <li><strong>mujoco 2.3.3</strong></li> <li><strong>dm-control 1.0.11</strong></li> <li>Python 3.9</li> </ul> <p>This combination avoids the structure-indexing errors seen in newer pairings and also avoids invoking backends requiring OpenGL or EGL by default.</p> <hr/> <h2 id="verifying-the-configuration">Verifying the Configuration</h2> <p>With OSMesa enabled and the compatible versions installed, the following minimal script successfully produced a rendered frame:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">MUJOCO_GL</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">osmesa</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">DISPLAY</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">""</span>

<span class="kn">from</span> <span class="n">dm_control</span> <span class="kn">import</span> <span class="n">suite</span>
<span class="kn">import</span> <span class="n">imageio</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">suite</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">cartpole</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">swingup</span><span class="sh">"</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">physics</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">camera_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Frame shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">imageio</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="sh">"</span><span class="s">soft_render.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
</code></pre></div></div> <p>The output confirmed that rendering worked:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Frame shape: (128, 128, 3)
Saved soft_render.png
</code></pre></div></div> <p>A few non-critical messages appear during interpreter shutdown, related to thread cleanup inside dm-control’s internal executor. These do not affect functionality.</p> <hr/> <h2 id="final-working-setup">Final Working Setup</h2> <p>Summarizing the configuration that worked consistently:</p> <ul> <li><strong>Python</strong>: 3.9</li> <li><strong>Mujoco</strong>: 2.3.3</li> <li><strong>dm-control</strong>: 1.0.11</li> <li><strong>System package</strong>: <code class="language-plaintext highlighter-rouge">libosmesa6-dev</code></li> <li><strong>Rendering backend</strong>: <code class="language-plaintext highlighter-rouge">MUJOCO_GL=osmesa</code></li> </ul> <p>This setup uses CPU-based software rendering, avoids any dependency on GPU drivers or graphical displays, and is compatible with reinforcement-learning algorithms requiring pixel observations.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Running dm-control on a headless server involves more than installing the library. Rendering backends, environment variables, and version compatibility all play important roles in determining whether Mujoco initializes correctly. After exploring several unsuccessful paths, the combination of <strong>OSMesa rendering</strong> and a <strong>compatible pair of dm-control and Mujoco versions</strong> proved to be a reliable solution.</p> <p>By documenting this process, this post aims to provide a clear reference for others setting up dm-control in a similar environment.</p>]]></content><author><name></name></author><category term="Reinforcement Learning"/><category term="Mujoco"/><category term="dm-control"/><category term="Rendering"/><summary type="html"><![CDATA[A practical record of configuring dm-control with Mujoco on a headless Ubuntu server, covering rendering failures, version mismatches, and the final workable setup.]]></summary></entry><entry><title type="html">Why SUMO’s Rendered Videos Should Never Be Used as RL Training Data</title><link href="https://shuhongdai.github.io/blog/2025/Why_SUMO-s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data/" rel="alternate" type="text/html" title="Why SUMO’s Rendered Videos Should Never Be Used as RL Training Data"/><published>2025-01-27T23:40:00+00:00</published><updated>2025-01-27T23:40:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Why_SUMO%E2%80%99s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Why_SUMO-s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The rendered images or videos generated by SUMO have no place in a RL pipeline. They may look clear, structured, and almost “dataset-like,” yet they fundamentally lack the properties that make a visual observation meaningful to an agent. What they provide is appearance, not information.</p> <p>This is not an insight I arrived at after trying to extract learning signals from SUMO’s frames. Rather, it becomes clear the moment one understands how SUMO represents its world internally. The simulator was never built around vision; it was built around structure—precise, explicit, symbolic structure. And because of this, its video output occupies a different conceptual layer entirely.</p> <hr/> <h2 id="visualization-hides-semantics">Visualization Hides Semantics</h2> <p>The rendered frame that SUMO shows us is, in essence, a human-friendly sketch of a world that exists somewhere else. The world SUMO actually simulates consists of continuous lane coordinates, exact positions and velocities, acceleration profiles, signal states, right-of-way relationships, and route intentions that evolve according to a coherent traffic model. This internal world is numerical and symbolic down to its core.</p> <p>None of this structure survives the trip into the visual renderer. A car in a SUMO frame is just a colored rectangle, detached from the lane graph it belongs to, stripped of its intentions and priorities, reduced to a geometry that no longer reveals why it will or will not proceed through an intersection. A traffic signal becomes a green or red dot with no connection to the control logic that governs it. Even spatial quantities, such as gaps or queue lengths, degrade into visual approximations that depend on camera perspective rather than on the simulator’s own measurements. It is tempting to think that pixels “contain enough information,” but the truth is that the renderer deliberately hides most of what matters.</p> <hr/> <h2 id="what-rl-actually-needs">What RL Actually Needs</h2> <p>RL does not learn from appearance, it learns from state. And a useful state is one that preserves the causal relationships driving the environment. With SUMO, those relationships live entirely in its internal representation: the topology of the road network, the numerical values describing motion, the logic that governs priority and right-of-way, the exact timing and sequencing of traffic lights. These are not afterthoughts; they are the environment.</p> <p>A video frame, regardless of how cleanly rendered, cannot express the rules that determine how traffic flows. It does not encode which vehicle is yielding, which is accelerating, which is constrained by downstream occupancy, or which is simply following a route that is invisible to the eye. Observing the render is, at best, witnessing the shadow of a system whose logic has already been stripped away.</p> <p>Training an RL agent on such shadows is not simply inefficient; it is conceptually misaligned. The agent is forced to reconstruct the environment’s logic from incomplete projections of it, even though SUMO already provides the fully-formed state in precise numerical terms. The agent ends up solving a perception problem that exists only because we discarded the very information the simulator was designed to offer.</p> <hr/> <h2 id="reconstructing-what-the-simulator-already-knows">Reconstructing What the Simulator Already Knows</h2> <p>If one insists on using SUMO’s images as input, the learning problem becomes unnecessarily inverted. To make sense of a frame, an agent must infer positions, velocities, lanes, intentions, and interactions—all of which SUMO already calculates. This re-derivation is not only redundant; it is structurally impossible to perform without error, because the necessary information does not exist in the pixels in the first place. SUMO’s renderer never intended to expose it.</p> <p>Vision makes sense when there is no alternative, such as in real-world driving where sensors are noisy and perception is inherently uncertain. But SUMO is not an uncertain environment. It is fully observable. Every variable that matters is crisp, deterministic, and accessible. To replace this with pixels is to voluntarily abandon the clarity that simulation exists to provide.</p> <p>It is important to remember why SUMO has visualization at all. The renderer exists so that humans can watch the simulation unfold. It is a debugging tool, a qualitative sanity check, a way to illustrate traffic scenarios for presentations and reports. The rendering is not a sensory modality of the environment, and it was never meant to be. Its purpose is interpretive, not generative.</p> <p>If an agent were to rely on SUMO’s visual output, it would be relying on something designed explicitly for human interpretation. And human interpretation thrives on abstractions, simplifications, and aesthetic conventions. All of which run counter to what machine learning expects from observational data.</p> <hr/> <h2 id="comment">Comment</h2> <p>The visual output of SUMO does not fail because it is low quality. It fails because it is the wrong abstraction. It belongs to a layer of the system intended for people, not agents. The real substance of SUMO such as the logic, the topology, the numerics and the decisions is found in its internal state, not in its renderings.</p> <p>This is why SUMO’s images should never be used as RL input. Not because they are flawed, but because they are unrelated to the simulation’s meaning. To rely on them is to ignore the very nature of the environment we are trying to learn from.</p>]]></content><author><name></name></author><category term="Simulation"/><category term="Traffic"/><category term="RL"/><category term="SUMO"/><summary type="html"><![CDATA[A examination of why the visual output of SUMO. Despite being clean and intuitive, it cannot serve as learning data for RL agents, and why this limitation is inherent in how the simulator is built.]]></summary></entry><entry><title type="html">Re-running an RL Experiment and Getting a Different Answer</title><link href="https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer/" rel="alternate" type="text/html" title="Re-running an RL Experiment and Getting a Different Answer"/><published>2024-11-17T02:08:00+00:00</published><updated>2024-11-17T02:08:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Not long ago, I tried to reproduce one of my RL experiments on a cloud server. The same code had run earlier on a local lab machine, and both hosts were equipped with <strong>NVIDIA RTX 4090 GPUs</strong>. The driver versions matched, the CUDA and PyTorch versions were identical, the environment dependencies mirrored each other, and every random seed was fixed. Under such conditions, the expectation was simple: the two training curves should overlap almost perfectly.</p> <p>But this time, they didn’t. For the first few thousand steps, everything behaved exactly as expected. The lines overlapped so closely that they were visually indistinguishable. As training continued, however, a slight divergence appeared—barely noticeable at first, then increasingly persistent. Eventually, the two runs settled into significantly different behaviors. All high-level variables were controlled, and yet the divergence persisted.It wasn’t dramatic, but it was unmistakable. And it prompted me to re-examine some of the more fragile aspects of RL systems that often go unnoticed.</p> <hr/> <h2 id="two-curves-that-should-have-been-one">Two Curves That Should Have Been One</h2> <p>The most striking part of this incident was how cleanly the divergence unfolded. During the early stage, the critic’s loss, the policy statistics, and the rewards from the environment aligned almost exactly between the two machines. The curves felt stable, even reassuring.</p> <p>Then the shift began. It wasn’t a sudden jump but a slow drift, like two lines that started parallel but eventually grew a small angle between them. Once the angle existed, the distance between the lines increased gradually and inevitably. What began as a tiny deviation eventually widened into a visible performance gap.</p> <p>This kind of “quiet drift” is rare in supervised learning, but painfully common in RL, where feedback loops amplify small differences.</p> <hr/> <h2 id="investigation">Investigation</h2> <p>I didn’t start by suspecting the GPUs. Instead, I reviewed the usual suspects in a calm, methodical way:</p> <p>Whether the environment returned identical resets, whether the model initialization and random number streams matched, whether the replay buffer might have desynchronized the sampling order, whether the logging system affected timing, and whether the training loop had implicit branches that could influence execution order.</p> <p>All these checks were quick to perform. Nothing at the framework or data-flow level explained the divergence. Which meant the issue had to be buried deeper than most RL bugs—deeper than Python, deeper than PyTorch, deeper than CUDA kernels invoked explicitly in code.</p> <hr/> <h2 id="clue">Clue</h2> <p>The earliest measurable drift appeared not in the policy’s actions, but in the critic’s value estimates. That itself was a clue. The critic is often the most numerically sensitive component in many RL algorithms, and its outputs feed directly into the policy update. If two identical systems begin to disagree, the critic is a natural place to look.</p> <p>The differences were tiny—barely outside floating-point noise—but detectable under scrutiny. In the early stage of training, those deviations did not affect behavior, but they were already present. And because the critic affects everything downstream, even a small mismatch is enough for a long feedback loop to magnify.</p> <hr/> <h2 id="a-subtle-difference-from-identical-hardware">A Subtle Difference From Identical Hardware</h2> <p>Eventually, the real cause became clear:<br/> two GPUs of the same model can still produce slightly different floating-point results.</p> <p>This sounds counterintuitive at first, but it’s neither rare nor mysterious. Several factors can produce such differences:</p> <ul> <li>Slight variations in how CUDA dispatches certain kernels</li> <li>Minor differences introduced by fused multiply–add behavior</li> <li>Driver-level optimizations that change arithmetic ordering</li> <li>Subtle kernel selection differences across installations</li> </ul> <p>The scale of these discrepancies is extremely small—on the order of the last few bits of a float. They are invisible in most workloads. In supervised learning, such noise is diluted by batching, averaging, and the absence of recurrent dependencies. In RL, however, the story is different. RL acts as an amplifier. A microscopic variance in a critic output can change a gradient, which changes a policy, which changes the data distribution, which changes the critic’s future inputs, and so on. Over tens of thousands of iterations, this accumulation can transform an imperceptible discrepancy into a meaningful behavioral difference.</p> <p>At the heart of this phenomenon is the recursive nature of RL training. The critic’s estimation errors influence the policy update. The policy influences the trajectory of states and rewards. These, in turn, influence how the critic is updated. The loop continues, step after step.</p> <p>This structure makes RL far more sensitive to numerical discrepancies than most other machine learning pipelines. A difference invisible at step 2,000 can become visible at step 50,000 simply because it is allowed to feed back into itself. This is not a bug; it is a property of RL.</p>]]></content><author><name></name></author><category term="Reinforcement Learning"/><category term="CUDA"/><category term="Numerical Stability"/><category term="Reproducibility"/><summary type="html"><![CDATA[A engineering reflection on why two RTX 4090 machines produced diverging RL curves despite identical code, seeds, and configurations. And what this reveals about RL’s numerical sensitivity.]]></summary></entry><entry><title type="html">On the Equity Difference Between Suited and Offsuit Starting Hands</title><link href="https://shuhongdai.github.io/blog/2024/On_the_Equity_Difference_Between_Suited_and_Offsuit_Starting_Hands/" rel="alternate" type="text/html" title="On the Equity Difference Between Suited and Offsuit Starting Hands"/><published>2024-09-01T02:50:00+00:00</published><updated>2024-09-01T02:50:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/On_the_Equity_Difference_Between_Suited_and_Offsuit_Starting_Hands</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/On_the_Equity_Difference_Between_Suited_and_Offsuit_Starting_Hands/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Earlier this year, during a casual session of No-Limit Hold’em, I picked up a hand like Q7. It was offsuit. Without thinking, I caught myself wishing it were suited. The feeling was immediate and familiar. Most players share it: being suited makes a hand <em>feel</em> noticeably better.</p> <p>But the more I thought about it, the more the question bothered me:</p> <p><strong>How much does being suited actually matter — not intuitively, but mathematically?</strong></p> <p>The answer is widely repeated in poker circles (“a few percent”), yet rarely justified. I wanted something more precise. So I decided to formalize the question, examine the underlying combinatorics, and finally validate the results with large-scale Monte Carlo simulation.</p> <p>This article is not about strategy.</p> <hr/> <h2 id="formalizing-the-problem">Formalizing the Problem</h2> <p>Let \(H\) be a starting hand and \(E(H)\) its equity against a uniformly random hand:</p> \[E(H) = \mathbb{P}(H \text{ wins}) \;+\; \tfrac{1}{2}\,\mathbb{P}(H \text{ ties}).\] <p>For any rank combination \(R\), let:</p> <ul> <li>\(R_s\) = suited version</li> <li>\(R_o\) = offsuit version</li> </ul> <p>The object of interest is the <strong>equity difference caused solely by suitedness</strong>:</p> \[\Delta(R) = E(R_s) - E(R_o).\] <p>This definition removes strategic context and isolates a purely probabilistic quantity. What follows is an attempt to understand \(\Delta(R)\) from first principles.</p> <hr/> <h2 id="decomposing-the-equity-difference">Decomposing the Equity Difference</h2> <p>Suited hands differ from offsuit hands only in the possibility of making a flush or flush-related draws. Thus we can conceptually decompose equity as:</p> \[\Delta(R) = \Delta_{\text{flush}}(R) + \Delta_{\text{backdoor}}(R) + \Delta_{\text{board}}(R).\] <p>This is not a strict identity, but a useful analytical decomposition.</p> <h3 id="1-flush-completion-contribution">1. Flush Completion Contribution</h3> <p>The probability that the board produces <strong>five cards of your suit</strong> is:</p> \[p_{\text{flush}} = \frac{\binom{11}{5}}{\binom{50}{5}} \approx 0.001965 \quad (0.1965\%).\] <p>At first glance this seems too small to matter. And indeed, <em>this alone</em> cannot explain the \(~1–2\%\) equity advantage that suited hands tend to have. The full equity impact requires considering draws, not just completed hands.</p> <hr/> <h3 id="2-backdoor-flush-contribution">2. Backdoor Flush Contribution</h3> <p>A backdoor flush occurs when the turn and river complete the suit after the flop supplies exactly two suited cards. The probability is:</p> \[p_{\text{backdoor}} = \underbrace{ \frac{\binom{11}{2}}{\binom{50}{3}} }_{\text{flop two-tone}} \times \underbrace{ \frac{9}{47} }_{\text{turn hit}} \times \underbrace{ \frac{9}{46} }_{\text{river hit}}.\] <p>Though small, the scenarios where backdoor draws contribute to equity are far more numerous than completed flushes, and they collectively account for a significant share of \(\Delta(R)\).</p> <hr/> <h3 id="3-board-texture-contribution">3. Board Texture Contribution</h3> <p>Even when no flush or draw exists, suitedness subtly alters a hand’s interaction with the board:</p> <ul> <li>additional semi-connectedness,</li> <li>more gutshot-plus-backdoor combinations,</li> <li>slightly improved domination behavior on multi-rank boards,</li> <li>marginal improvements in showdown distribution.</li> </ul> <p>Formally, this is captured by the conditional expectation:</p> \[\Delta_{\text{board}}(R) = \mathbb{E}\!\left[ E(R_s \mid B) - E(R_o \mid B) \right],\] <p>where \(B\) ranges over all possible boards. Although difficult to compute directly, this term explains part of the stability of \(\Delta(R)\) across rank shapes.</p> <hr/> <h2 id="combinatorial-perspective">Combinatorial Perspective</h2> <p>It is tempting to assume that suited hands should gain large equity from strong flush outcomes. But the combinatorics tell a different story.</p> <p>Out of all possible 7-card combinations consistent with a given starting hand, only a very small fraction produce flushes:</p> \[\frac{\binom{11}{3}}{\binom{50}{3}}, \quad \frac{\binom{11}{4}}{\binom{50}{4}}, \quad \frac{\binom{11}{5}}{\binom{50}{5}}.\] <p>These events are rare. The magnitude of \(\Delta(R)\) owes more to <strong>draw equity</strong> than to finished hands, and even then, the effect is bounded by the structure of the card distribution. This is why suitedness, while real and measurable, is universally modest.</p> <hr/> <h2 id="monte-carlo-simulation">Monte Carlo Simulation</h2> <p>To validate the theoretical picture, I ran a large-scale Monte Carlo simulation.<br/> The setup:</p> <ul> <li>Opponent hand uniformly sampled</li> <li>All boards fully enumerated by simulation</li> <li>10M iterations per hand rank (K7, QT, A2, etc.)</li> </ul> <p>The results (representative sample):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Hand    Suited    Offsuit    Difference
K7      49.12%    47.02%     +2.10%
QT      57.86%    56.40%     +1.46%
92      34.44%    33.20%     +1.24%
A2      54.79%    53.93%     +0.86%

</code></pre></div></div> <p>Two observations stood out:</p> <ol> <li>The difference is <strong>consistently small</strong>.</li> <li>The variation across hands is narrower than expected.</li> </ol> <p>Across all 169 starting hand types, \(\Delta(R)\) rarely leaves the interval:</p> \[0.8\% \lesssim \Delta(R) \lesssim 2.3\%.\] <p>This matches the combinatorial analysis surprisingly well.</p> <hr/> <h2 id="a-small-mathematical-statement">A Small Mathematical Statement</h2> <p>Although not a formal theorem, the following informal statement captures the essential structure:</p> <blockquote> <p><strong>For any non-paired starting hand \(R\), the equity difference between suited and offsuit versions is bounded by constants determined almost entirely by flush-related combinatorics and backdoor structure.</strong></p> </blockquote>]]></content><author><name></name></author><category term="Poker"/><category term="Probability"/><category term="Monte Carlo"/><category term="Combinatorics"/><category term="Game Theory"/><summary type="html"><![CDATA[A mathematical and computational examination of why suited hands hold a small but remarkably consistent equity advantage over their offsuit counterparts in Texas Hold’em.]]></summary></entry><entry><title type="html">Using Local v2rayN Proxy for Cloud Servers via SSH Reverse Tunnel</title><link href="https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel/" rel="alternate" type="text/html" title="Using Local v2rayN Proxy for Cloud Servers via SSH Reverse Tunnel"/><published>2024-02-02T00:38:10+00:00</published><updated>2024-02-02T00:38:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In mainland China, even cloud servers often struggle to access foreign open-source or academic resources. Many people interpret this as “network censorship,” but in my view the situation is more nuanced. It is not necessarily the result of direct, targeted enforcement against technical resources. Instead, it resembles a systemic consequence of long-term authoritarian governance, where private network operators behave with extreme caution in order to avoid any possible regulatory risk.</p> <p>Under such an environment—opaque rules, inconsistent enforcement, and heavy potential penalties—service providers tend to implement their own overly strict filtering. As a result, traffic to platforms like HuggingFace, GitHub, PyPI, and other purely technical services may be blocked or reset “just in case.” In practice, it becomes a case of <em>tying themselves up with their own rope</em>.</p> <p>For developers and researchers, this means that even a cloud server intended for normal machine learning tasks may have difficulty accessing essential foreign resources. This post documents the issues I encountered and the final working solution.</p> <hr/> <h2 id="attempt-1-running-a-proxy-directly-on-the-server">Attempt 1: Running a Proxy Directly on the Server</h2> <p>My first attempt was to deploy a proxy environment directly on the cloud server using sing-box or Xray, and to reuse my existing Shadowsocks 2022 (SS2022) nodes from my Windows machine.</p> <p>This approach ran into multiple problems:</p> <ul> <li>Different implementations of SS2022 use different field names</li> <li>The key format (base64) requirements vary by version</li> <li>Certain fields (e.g., <code class="language-plaintext highlighter-rouge">secondary</code>, <code class="language-plaintext highlighter-rouge">psk</code>) are not supported in older releases</li> <li>Some configurations pass syntax checks but fail during actual traffic forwarding</li> <li>sing-box support for SS2022 differs significantly among releases</li> </ul> <p>Even after repeated adjustments, the connection remained unstable or unusable.</p> <hr/> <h2 id="attempt-2-letting-the-server-reuse-my-local-v2rayn-proxy-successful">Attempt 2: Letting the Server Reuse My Local v2rayN Proxy (Successful)</h2> <p>Since my local Windows environment with v2rayN worked reliably, I attempted to make the cloud server reuse <strong>my desktop’s proxy</strong> via <strong>SSH reverse port forwarding</strong>.</p> <p>I established an SSH session from the cloud server:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-o</span> <span class="nv">ServerAliveInterval</span><span class="o">=</span>60 <span class="se">\</span>
    <span class="nt">-o</span> <span class="nv">ServerAliveCountMax</span><span class="o">=</span>3 <span class="se">\</span>
    <span class="nt">-R</span> 0.0.0.0:10808:127.0.0.1:10808 <span class="se">\</span>
    root@&lt;server-address&gt;
</code></pre></div></div> <p>This command:</p> <ul> <li>Opens <code class="language-plaintext highlighter-rouge">10808</code> on the cloud server</li> <li>Forwards all traffic from that port</li> <li>Back through the SSH tunnel to my Windows machine</li> <li>Where v2rayN listens on local port <code class="language-plaintext highlighter-rouge">10808</code></li> </ul> <p>Testing the proxy:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--socks5</span> 127.0.0.1:10808 https://ipinfo.io/ip
</code></pre></div></div> <p>The returned IP was exactly the exit node of my v2rayN setup, confirming that the cloud server was successfully using my local proxy.</p> <hr/> <h2 id="python-configuration">Python Configuration</h2> <p>Once <code class="language-plaintext highlighter-rouge">curl</code> worked, I needed Python (especially <code class="language-plaintext highlighter-rouge">requests</code> and HuggingFace Hub) to use SOCKS5.</p> <p>Python does not support SOCKS by default, so I installed:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pysocks
</code></pre></div></div> <p>Then set the environment variables:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">http_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
<span class="nb">export </span><span class="nv">https_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
<span class="nb">export </span><span class="nv">all_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
</code></pre></div></div> <p>After this, both <code class="language-plaintext highlighter-rouge">requests.get()</code> and HuggingFace model downloads worked correctly.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>It is important to note that deploying a proxy service directly on the server is not only feasible but is, in fact, the more standard, professional, and long-term maintainable approach. Whether using sing-box, Xray, Hysteria, or Tuic, one can build a fully independent outbound capability on the cloud server, which aligns better with the engineering practice of “managing your own network boundary.”</p> <p>However, this approach typically involves multiple layers of complexity: protocol specifications, key formats, server–client version compatibility, differences in supported cipher suites, and firewall behavior. This is especially true for SS2022, which lacks unified documentation and consistent implementation across projects. As a result, several subtle issues may arise during configuration and require careful troubleshooting.</p> <p>In contrast, reusing a local proxy through SSH reverse port forwarding serves as a fast, low-overhead, and almost configuration-free alternative. Its main advantage is that it works immediately and is not affected by the cloud provider’s network policies. The drawback, of course, is that it depends on the local machine being online, making it unsuitable as a long-term infrastructure solution.</p> <p>Based on this distinction: for time-sensitive situations, such as urgently needing to download model weights from HuggingFace, reusing a local proxy is extremely convenient. But for long-term project environments, if the goal is to maintain an autonomous and stable outbound capability on the server, deploying a proper proxy service on the server remains the recommended final solution.</p>]]></content><author><name></name></author><category term="Networking"/><category term="Proxy"/><category term="SSH"/><category term="DevOps"/><summary type="html"><![CDATA[A practical record of troubleshooting outbound network restrictions on Chinese cloud servers and enabling stable access to foreign academic resources.]]></summary></entry></feed>