<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shuhongdai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shuhongdai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-17T09:20:50+00:00</updated><id>https://shuhongdai.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Designing a Maintainable Replay Buffer in Reinforcement Learning Systems</title><link href="https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_Reinforcement_Learning_Systems/" rel="alternate" type="text/html" title="Designing a Maintainable Replay Buffer in Reinforcement Learning Systems"/><published>2025-10-21T12:31:00+00:00</published><updated>2025-10-21T12:31:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_Reinforcement_Learning_Systems</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_Reinforcement_Learning_Systems/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In RL implementations, the replay buffer often appears as a modest component that is essential but rarely the center of discussion. It stores transitions and serves mini-batches for training updates, and in many introductory materials, it is presented as a straightforward queue with random access.</p> <p>However, once RL systems move beyond toy prototypes and begin supporting extensible algorithms, varied environments, or long-running experiments, the replay buffer quickly becomes a structural foundation rather than a convenience. Its design influences not only training stability but also code clarity, maintainability, and the ease with which new ideas can be incorporated into the system.</p> <p>This post offers a engineering-oriented reflection on replay buffer design: what purposes it actually serves, what structures tend to lead to long-term stability, and what principles help prevent complications as a project grows.</p> <hr/> <h2 id="the-replay-buffer-as-a-dataflow-node">The Replay Buffer as a Dataflow Node</h2> <p>Although we often describe a replay buffer as a “storage mechanism,” in practice it functions as a <strong>dataflow node</strong> inside an RL system. It mediates between:</p> <ul> <li>The environment, which produces experience,</li> <li>The learning update loop, which consumes it,</li> <li>Auxiliary modules (logging, metrics, evaluation),</li> <li>And in many cases, reproducibility mechanisms.</li> </ul> <p>Shifting perspective from “container” to “dataflow component” clarifies its role. The buffer is not just a passive holder of transitions; it enforces data consistency, defines boundaries between modules, and often determines how easily an RL system can scale or adapt to new requirements.</p> <p>As RL algorithms diversify—off-policy, on-policy with auxiliary replay, offline RL, model-based RL—the replay buffer subtly shifts shape, yet the underlying structural demands stay surprisingly consistent.</p> <hr/> <h2 id="common-replay-buffer-structures-in-existing-systems">Common Replay Buffer Structures in Existing Systems</h2> <p>Across different RL codebases, replay buffers often take one of a few recognizable forms:</p> <ul> <li><strong>Simple Python-list–based buffers</strong>, prioritizing simplicity over structure.</li> <li><strong>Dictionary-based buffers</strong>, offering flexibility through named fields.</li> <li><strong>Preallocated NumPy array buffers</strong>, emphasizing performance and predictable behavior.</li> <li><strong>Dataset-style buffers</strong>, seen in offline RL or large-scale frameworks such as RLlib.</li> </ul> <p>Each of these reflects a particular engineering priority—ease of prototyping, multi-field flexibility, high throughput training, or dataset compatibility. None is inherently incorrect; instead, they sit at different points in the design space. Recognizing this variety helps clarify what constraints and opportunities a more maintainable version should satisfy.</p> <hr/> <h2 id="why-maintainability-matters">Why Maintainability Matters</h2> <p>The replay buffer is one of the few components that interacts with <strong>every</strong> part of an RL pipeline:</p> <ul> <li>Environment interaction</li> <li>Training loops</li> <li>Logging &amp; monitoring</li> <li>Sampling strategies</li> <li>Distributed actors (if applicable)</li> </ul> <p>Because of this centrality, small inconsistencies—shape mismatches, implicit assumptions, overly coupled fields—tend to propagate widely. A design that works for a narrow experiment may later resist extensions such as:</p> <ul> <li>Adding new features (e.g., log-prob, target values),</li> <li>Switching to new environments with richer info fields,</li> <li>Supporting truncated vs. terminated distinctions,</li> <li>Integrating prioritized replay or sequence sampling.</li> </ul> <p>Maintainability, therefore, is less about making the implementation “clean” and more about preserving <strong>structural integrity under change</strong>.</p> <hr/> <h2 id="design-principles-for-a-maintainable-replay-buffer">Design Principles for a Maintainable Replay Buffer</h2> <p>A replay buffer that aims to support a wide range of algorithms and experiments should follow several straightforward but impactful principles. These principles arise not from performance tuning but from the need for clear and robust system behavior.</p> <h3 id="1-a-clear-and-explicit-data-schema"><strong>1. A Clear and Explicit Data Schema</strong></h3> <p>Each field—observations, actions, rewards, next observations, termination indicators—should be explicitly represented. Buffers that try to infer structure implicitly often break when new algorithms introduce additional fields.</p> <p>A good schema clearly defines:</p> <ul> <li>What each field contains</li> <li>Its shape</li> <li>Its dtype</li> <li>When and how it gets written</li> </ul> <p>This clarity reduces ambiguity during sampling and training.</p> <hr/> <h3 id="2-independence-between-fields"><strong>2. Independence Between Fields</strong></h3> <p>Transitions should not be stored as monolithic tuples. Instead, each field should maintain its own array or storage structure. This approach improves:</p> <ul> <li>Testability</li> <li>Clarity</li> <li>The ability to extend fields independently</li> <li>Compatibility with batch indexing</li> </ul> <p>Field independence also minimizes the risk of “ripple effects” when experimenting with alternative data encodings or additional metadata.</p> <hr/> <h3 id="3-preallocation-with-predictable-behavior"><strong>3. Preallocation with Predictable Behavior</strong></h3> <p>A fixed-size ring buffer with preallocated arrays is a stable and predictable design. It avoids:</p> <ul> <li>Memory fragmentation</li> <li>Resizing overhead</li> <li>Ambiguous buffer growth behavior</li> </ul> <p>A simple write pointer and size counter are often all that is needed. Predictability is more valuable than cleverness in this case.</p> <hr/> <h3 id="4-decoupled-sampling-logic"><strong>4. Decoupled Sampling Logic</strong></h3> <p>Sampling strategies evolve rapidly in RL research. Keeping sampling logic separate from data storage enables easier experimentation with:</p> <ul> <li>Uniform random sampling</li> <li>Stratified sampling</li> <li>Prioritized replay</li> <li>Sequential sampling for RNN-based agents</li> <li>Temporal batch sampling for long-horizon tasks</li> </ul> <p>A buffer whose storage does not constrain sampling enables more flexible algorithm development.</p> <hr/> <h3 id="5-stable-batch-shapes-and-typing"><strong>5. Stable Batch Shapes and Typing</strong></h3> <p>One of the most common sources of bugs is inconsistent shapes. By fixing shapes and dtypes at initialization, and validating them at write time, the buffer ensures:</p> <ul> <li>Training loops remain stable</li> <li>Models receive predictable inputs</li> <li>Misconfigured environments are detected early</li> </ul> <p>This principle applies across vector observations, discrete actions, continuous actions, or any other modality.</p> <hr/> <h2 id="a-practical-replay-buffer-structure">A Practical Replay Buffer Structure</h2> <p>A maintainable replay buffer often adopts a design similar to the following conceptual structure:</p> <ul> <li>A <strong>capacity</strong>, defining maximum size</li> <li>A <strong>write index</strong>, controlling where new transitions are stored</li> <li>A <strong>current size</strong>, indicating the valid range</li> <li>A dictionary of <strong>fields</strong>, each with its own preallocated array</li> <li>A sampling interface that accepts batch indices and returns batched transitions</li> </ul> <p>This structure supports:</p> <ul> <li>Easy extension（adding new fields is straightforward）</li> <li>Consistent sampling（batch index array drives selection）</li> <li>Ability to swap or enhance sampling mechanisms</li> <li>Compatibility with on-policy or off-policy algorithms</li> </ul> <p>What matters is not the exact API surface, but the emphasis on <strong>clarity</strong>, <strong>decoupling</strong>, and <strong>predictability</strong>.</p> <hr/> <h2 id="memory-and-performance-considerations">Memory and Performance Considerations</h2> <p>Replay buffers operate under tight memory constraints in some RL settings (e.g., long-horizon environments or high-frequency transitions). Reasonable engineering considerations include:</p> <ul> <li>Choosing appropriate dtypes (e.g., <code class="language-plaintext highlighter-rouge">float32</code> vs. <code class="language-plaintext highlighter-rouge">float64</code>)</li> <li>Managing non-essential fields carefully</li> <li>Using contiguous arrays to reduce overhead</li> <li>In distributed setups, deciding which side handles storage</li> </ul> <p>While performance matters, in most research or mid-scale systems, clarity and consistency should take precedence.</p> <hr/> <h2 id="the-role-of-replay-buffers-in-larger-rl-architectures">The Role of Replay Buffers in Larger RL Architectures</h2> <p>As RL systems grow, so does the role of the replay buffer:</p> <ul> <li>In <strong>off-policy RL</strong>, it is a core stabilizer.</li> <li>In <strong>offline RL</strong>, it becomes the primary dataset abstraction.</li> <li>In <strong>model-based RL</strong>, it may store both real and imagined transitions.</li> <li>In <strong>multi-agent RL</strong>, it may coordinate data from multiple agents or environments.</li> <li>In <strong>distributed RL</strong>, it may act as a central data service.</li> </ul> <p>A well-designed replay buffer scales gracefully across these contexts without structural changes.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>The replay buffer is one of the most important infrastructural components in RL systems. Although often overshadowed by policy networks or optimization algorithms, its design directly impacts the clarity, reliability, and extensibility of an RL codebase. A maintainable buffer is built on simple but robust principles: clear schema, independent fields, predictable behavior, and decoupled sampling.</p>]]></content><author><name></name></author><category term="Reinforcement Learning"/><category term="System Design"/><category term="Data Structures"/><summary type="html"><![CDATA[A structured and engineering-focused reflection on replay buffer design in reinforcement learning, emphasizing clarity, extensibility, and long-term maintainability.]]></summary></entry><entry><title type="html">On the Equity Difference Between Suited and Offsuit Starting Hands</title><link href="https://shuhongdai.github.io/blog/2025/On_the_Equity_Difference_Between_Suited_and_Offsuit_Starting_Hands/" rel="alternate" type="text/html" title="On the Equity Difference Between Suited and Offsuit Starting Hands"/><published>2025-08-03T02:50:00+00:00</published><updated>2025-08-03T02:50:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/On_the_Equity_Difference_Between_Suited_and_Offsuit_Starting_Hands</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/On_the_Equity_Difference_Between_Suited_and_Offsuit_Starting_Hands/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Earlier this year, during a casual session of No-Limit Hold’em, I picked up a hand like Q7. It was offsuit. Without thinking, I caught myself wishing it were suited. The feeling was immediate and familiar. Most players share it: being suited makes a hand <em>feel</em> noticeably better.</p> <p>But the more I thought about it, the more the question bothered me:</p> <p><strong>How much does being suited actually matter — not intuitively, but mathematically?</strong></p> <p>The answer is widely repeated in poker circles (“a few percent”), yet rarely justified. I wanted something more precise. So I decided to formalize the question, examine the underlying combinatorics, and finally validate the results with large-scale Monte Carlo simulation.</p> <p>This article is not about strategy.</p> <hr/> <h2 id="formalizing-the-problem">Formalizing the Problem</h2> <p>Let \(H\) be a starting hand and \(E(H)\) its equity against a uniformly random hand:</p> \[E(H) = \mathbb{P}(H \text{ wins}) \;+\; \tfrac{1}{2}\,\mathbb{P}(H \text{ ties}).\] <p>For any rank combination \(R\), let:</p> <ul> <li>\(R_s\) = suited version</li> <li>\(R_o\) = offsuit version</li> </ul> <p>The object of interest is the <strong>equity difference caused solely by suitedness</strong>:</p> \[\Delta(R) = E(R_s) - E(R_o).\] <p>This definition removes strategic context and isolates a purely probabilistic quantity. What follows is an attempt to understand \(\Delta(R)\) from first principles.</p> <hr/> <h2 id="decomposing-the-equity-difference">Decomposing the Equity Difference</h2> <p>Suited hands differ from offsuit hands only in the possibility of making a flush or flush-related draws. Thus we can conceptually decompose equity as:</p> \[\Delta(R) = \Delta_{\text{flush}}(R) + \Delta_{\text{backdoor}}(R) + \Delta_{\text{board}}(R).\] <p>This is not a strict identity, but a useful analytical decomposition.</p> <h3 id="1-flush-completion-contribution">1. Flush Completion Contribution</h3> <p>The probability that the board produces <strong>five cards of your suit</strong> is:</p> \[p_{\text{flush}} = \frac{\binom{11}{5}}{\binom{50}{5}} \approx 0.001965 \quad (0.1965\%).\] <p>At first glance this seems too small to matter. And indeed, <em>this alone</em> cannot explain the \(~1–2\%\) equity advantage that suited hands tend to have. The full equity impact requires considering draws, not just completed hands.</p> <hr/> <h3 id="2-backdoor-flush-contribution">2. Backdoor Flush Contribution</h3> <p>A backdoor flush occurs when the turn and river complete the suit after the flop supplies exactly two suited cards. The probability is:</p> \[p_{\text{backdoor}} = \underbrace{ \frac{\binom{11}{2}}{\binom{50}{3}} }_{\text{flop two-tone}} \times \underbrace{ \frac{9}{47} }_{\text{turn hit}} \times \underbrace{ \frac{9}{46} }_{\text{river hit}}.\] <p>Though small, the scenarios where backdoor draws contribute to equity are far more numerous than completed flushes, and they collectively account for a significant share of \(\Delta(R)\).</p> <hr/> <h3 id="3-board-texture-contribution">3. Board Texture Contribution</h3> <p>Even when no flush or draw exists, suitedness subtly alters a hand’s interaction with the board:</p> <ul> <li>additional semi-connectedness,</li> <li>more gutshot-plus-backdoor combinations,</li> <li>slightly improved domination behavior on multi-rank boards,</li> <li>marginal improvements in showdown distribution.</li> </ul> <p>Formally, this is captured by the conditional expectation:</p> \[\Delta_{\text{board}}(R) = \mathbb{E}\!\left[ E(R_s \mid B) - E(R_o \mid B) \right],\] <p>where \(B\) ranges over all possible boards. Although difficult to compute directly, this term explains part of the stability of \(\Delta(R)\) across rank shapes.</p> <hr/> <h2 id="combinatorial-perspective">Combinatorial Perspective</h2> <p>It is tempting to assume that suited hands should gain large equity from strong flush outcomes. But the combinatorics tell a different story.</p> <p>Out of all possible 7-card combinations consistent with a given starting hand, only a very small fraction produce flushes:</p> \[\frac{\binom{11}{3}}{\binom{50}{3}}, \quad \frac{\binom{11}{4}}{\binom{50}{4}}, \quad \frac{\binom{11}{5}}{\binom{50}{5}}.\] <p>These events are rare. The magnitude of \(\Delta(R)\) owes more to <strong>draw equity</strong> than to finished hands, and even then, the effect is bounded by the structure of the card distribution. This is why suitedness, while real and measurable, is universally modest.</p> <hr/> <h2 id="monte-carlo-simulation">Monte Carlo Simulation</h2> <p>To validate the theoretical picture, I ran a large-scale Monte Carlo simulation.<br/> The setup:</p> <ul> <li>Opponent hand uniformly sampled</li> <li>All boards fully enumerated by simulation</li> <li>10M iterations per hand rank (K7, QT, A2, etc.)</li> </ul> <p>The results (representative sample):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Hand    Suited    Offsuit    Difference
K7      49.12%    47.02%     +2.10%
QT      57.86%    56.40%     +1.46%
92      34.44%    33.20%     +1.24%
A2      54.79%    53.93%     +0.86%

</code></pre></div></div> <p>Two observations stood out:</p> <ol> <li>The difference is <strong>consistently small</strong>.</li> <li>The variation across hands is narrower than expected.</li> </ol> <p>Across all 169 starting hand types, \(\Delta(R)\) rarely leaves the interval:</p> \[0.8\% \lesssim \Delta(R) \lesssim 2.3\%.\] <p>This matches the combinatorial analysis surprisingly well.</p> <hr/> <h2 id="a-small-mathematical-statement">A Small Mathematical Statement</h2> <p>Although not a formal theorem, the following informal statement captures the essential structure:</p> <blockquote> <p><strong>For any non-paired starting hand \(R\), the equity difference between suited and offsuit versions is bounded by constants determined almost entirely by flush-related combinatorics and backdoor structure.</strong></p> </blockquote>]]></content><author><name></name></author><category term="Poker"/><category term="Probability"/><category term="Monte Carlo"/><category term="Combinatorics"/><category term="Game Theory"/><summary type="html"><![CDATA[A mathematical and computational examination of why suited hands hold a small but remarkably consistent equity advantage over their offsuit counterparts in Texas Hold’em.]]></summary></entry><entry><title type="html">Running dm-control on a Headless Server: A Complete Debugging Log</title><link href="https://shuhongdai.github.io/blog/2025/Running-_dm-control-_on_a_Headless_Server/" rel="alternate" type="text/html" title="Running dm-control on a Headless Server: A Complete Debugging Log"/><published>2025-06-15T21:12:00+00:00</published><updated>2025-06-15T21:12:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Running%20_dm-control%20_on_a_Headless_Server</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Running-_dm-control-_on_a_Headless_Server/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post documents the process of configuring <strong>dm-control</strong> on a <strong>headless Ubuntu server</strong> for reinforcement learning experiments that require pixel-based observations. The goal was straightforward: load a task from the DeepMind Control Suite and render frames for a CNN-based policy. However, running dm-control without a graphical display consistently triggered a series of rendering failures.</p> <p>Rather than providing a tutorial, this article records the issues encountered, the different directions explored, and the final configuration that proved reliable. Hopefully, this log will help anyone attempting to run dm-control in a similar environment.</p> <hr/> <h2 id="initial-attempts-and-rendering-errors">Initial Attempts and Rendering Errors</h2> <p>The starting point was simply loading a task and calling <code class="language-plaintext highlighter-rouge">env.physics.render()</code>. On a local machine, this works immediately. On a headless server, the first result was an error referencing <strong>EGL</strong> and <strong>OpenGL</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
AttributeError: 'NoneType' object has no attribute 'eglQueryString'

</code></pre></div></div> <p>Further attempts produced warnings about missing <code class="language-plaintext highlighter-rouge">DISPLAY</code> variables:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
X11: The DISPLAY environment variable is missing

</code></pre></div></div> <p>These messages made it clear that dm-control was attempting to initialize rendering through OpenGL/EGL, both of which require a graphics context the server did not provide. Disabling the <code class="language-plaintext highlighter-rouge">DISPLAY</code> variable or forcing EGL did not resolve the issue; the underlying environment simply lacked the dependencies required for hardware-accelerated rendering.</p> <hr/> <h2 id="investigating-the-role-of-pyopengl">Investigating the Role of PyOpenGL</h2> <p>The repeated mentions of EGL led to checking whether PyOpenGL played a role. Removing PyOpenGL temporarily modified the error messages, but it was automatically reinstalled when upgrading or reinstalling dm-control and Mujoco. This indicated that avoiding the OpenGL backend entirely was not feasible through uninstalling PyOpenGL alone.</p> <hr/> <h2 id="version-mismatch-between-dm-control-and-mujoco">Version Mismatch Between dm-control and Mujoco</h2> <p>At a later stage, entirely different errors appeared:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
eq_active not found
flex_xvert0 not found

</code></pre></div></div> <p>These errors are characteristic of <strong>Mujoco internal structure mismatches</strong>, suggesting that the installed versions of Mujoco and dm-control were not aligned. dm-control indexes Mujoco model structures by field name, and when they do not match, initialization fails even before rendering begins.</p> <p>This confirmed that two separate issues existed:</p> <ol> <li>Rendering backend not compatible with the server</li> <li>dm-control and Mujoco versions not compatible with each other</li> </ol> <p>Both needed to be addressed.</p> <hr/> <h2 id="moving-toward-a-software-rendering-approach">Moving Toward a Software Rendering Approach</h2> <p>Given that neither EGL nor OpenGL would work reliably on the server, the next candidate was <strong>OSMesa</strong>, a pure software renderer. OSMesa performs all rendering on the CPU and does not require a graphical display or GPU drivers. This makes it suitable for cloud or containerized environments.</p> <p>Ubuntu provides OSMesa through:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
libosmesa6-dev

</code></pre></div></div> <p>The key environment variable for Mujoco is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
MUJOCO_GL=osmesa

</code></pre></div></div> <p>With this backend, Mujoco no longer attempts to initialize hardware-accelerated contexts.</p> <hr/> <h2 id="identifying-a-stable-mujoco--dm-control-combination">Identifying a Stable Mujoco + dm-control Combination</h2> <p>After testing multiple combinations, the following versions proved to be both compatible with each other and functional under OSMesa:</p> <ul> <li><strong>mujoco 2.3.3</strong></li> <li><strong>dm-control 1.0.11</strong></li> <li>Python 3.9</li> </ul> <p>This combination avoids the structure-indexing errors seen in newer pairings and also avoids invoking backends requiring OpenGL or EGL by default.</p> <hr/> <h2 id="verifying-the-configuration">Verifying the Configuration</h2> <p>With OSMesa enabled and the compatible versions installed, the following minimal script successfully produced a rendered frame:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">MUJOCO_GL</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">osmesa</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">DISPLAY</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">""</span>

<span class="kn">from</span> <span class="n">dm_control</span> <span class="kn">import</span> <span class="n">suite</span>
<span class="kn">import</span> <span class="n">imageio</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">suite</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">cartpole</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">swingup</span><span class="sh">"</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">physics</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">camera_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Frame shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">imageio</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="sh">"</span><span class="s">soft_render.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
</code></pre></div></div> <p>The output confirmed that rendering worked:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Frame shape: (128, 128, 3)
Saved soft_render.png
</code></pre></div></div> <p>A few non-critical messages appear during interpreter shutdown, related to thread cleanup inside dm-control’s internal executor. These do not affect functionality.</p> <hr/> <h2 id="final-working-setup">Final Working Setup</h2> <p>Summarizing the configuration that worked consistently:</p> <ul> <li><strong>Python</strong>: 3.9</li> <li><strong>Mujoco</strong>: 2.3.3</li> <li><strong>dm-control</strong>: 1.0.11</li> <li><strong>System package</strong>: <code class="language-plaintext highlighter-rouge">libosmesa6-dev</code></li> <li><strong>Rendering backend</strong>: <code class="language-plaintext highlighter-rouge">MUJOCO_GL=osmesa</code></li> </ul> <p>This setup uses CPU-based software rendering, avoids any dependency on GPU drivers or graphical displays, and is compatible with reinforcement-learning algorithms requiring pixel observations.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Running dm-control on a headless server involves more than installing the library. Rendering backends, environment variables, and version compatibility all play important roles in determining whether Mujoco initializes correctly. After exploring several unsuccessful paths, the combination of <strong>OSMesa rendering</strong> and a <strong>compatible pair of dm-control and Mujoco versions</strong> proved to be a reliable solution.</p> <p>By documenting this process, this post aims to provide a clear reference for others setting up dm-control in a similar environment.</p>]]></content><author><name></name></author><category term="Reinforcement Learning"/><category term="Mujoco"/><category term="dm-control"/><category term="Rendering"/><summary type="html"><![CDATA[A practical record of configuring dm-control with Mujoco on a headless Ubuntu server, covering rendering failures, version mismatches, and the final workable setup.]]></summary></entry><entry><title type="html">Why SUMO’s Rendered Videos Should Never Be Used as RL Training Data</title><link href="https://shuhongdai.github.io/blog/2025/Why_SUMO-s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data/" rel="alternate" type="text/html" title="Why SUMO’s Rendered Videos Should Never Be Used as RL Training Data"/><published>2025-01-27T23:40:00+00:00</published><updated>2025-01-27T23:40:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Why_SUMO%E2%80%99s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Why_SUMO-s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The rendered images or videos generated by SUMO have no place in a RL pipeline. They may look clear, structured, and almost “dataset-like,” yet they fundamentally lack the properties that make a visual observation meaningful to an agent. What they provide is appearance, not information.</p> <p>This is not an insight I arrived at after trying to extract learning signals from SUMO’s frames. Rather, it becomes clear the moment one understands how SUMO represents its world internally. The simulator was never built around vision; it was built around structure—precise, explicit, symbolic structure. And because of this, its video output occupies a different conceptual layer entirely.</p> <hr/> <h2 id="visualization-hides-semantics">Visualization Hides Semantics</h2> <p>The rendered frame that SUMO shows us is, in essence, a human-friendly sketch of a world that exists somewhere else. The world SUMO actually simulates consists of continuous lane coordinates, exact positions and velocities, acceleration profiles, signal states, right-of-way relationships, and route intentions that evolve according to a coherent traffic model. This internal world is numerical and symbolic down to its core. Relationships are explicit. Nothing is ambiguous. Every object has a meaning beyond how it looks.</p> <p>None of this structure survives the trip into the visual renderer. A car in a SUMO frame is just a colored rectangle, detached from the lane graph it belongs to, stripped of its intentions and priorities, reduced to a geometry that no longer reveals why it will or will not proceed through an intersection. A traffic signal becomes a green or red dot with no connection to the control logic that governs it. Even spatial quantities, such as gaps or queue lengths, degrade into visual approximations that depend on camera perspective rather than on the simulator’s own measurements. It is tempting to think that pixels “contain enough information,” but the truth is that the renderer deliberately hides most of what matters.</p> <hr/> <h2 id="what-rl-actually-needs">What RL Actually Needs</h2> <p>RL does not learn from appearance, it learns from state. And a useful state is one that preserves the causal relationships driving the environment. With SUMO, those relationships live entirely in its internal representation: the topology of the road network, the numerical values describing motion, the logic that governs priority and right-of-way, the exact timing and sequencing of traffic lights. These are not afterthoughts; they are the environment.</p> <p>A video frame, regardless of how cleanly rendered, cannot express the rules that determine how traffic flows. It does not encode which vehicle is yielding, which is accelerating, which is constrained by downstream occupancy, or which is simply following a route that is invisible to the eye. Observing the render is, at best, witnessing the shadow of a system whose logic has already been stripped away.</p> <p>Training an RL agent on such shadows is not simply inefficient; it is conceptually misaligned. The agent is forced to reconstruct the environment’s logic from incomplete projections of it, even though SUMO already provides the fully-formed state in precise numerical terms. The agent ends up solving a perception problem that exists only because we discarded the very information the simulator was designed to offer.</p> <hr/> <h2 id="reconstructing-what-the-simulator-already-knows">Reconstructing What the Simulator Already Knows</h2> <p>If one insists on using SUMO’s images as input, the learning problem becomes unnecessarily inverted. To make sense of a frame, an agent must infer positions, velocities, lanes, intentions, and interactions—all of which SUMO already calculates. This re-derivation is not only redundant; it is structurally impossible to perform without error, because the necessary information does not exist in the pixels in the first place. SUMO’s renderer never intended to expose it.</p> <p>Vision makes sense when there is no alternative, such as in real-world driving where sensors are noisy and perception is inherently uncertain. But SUMO is not an uncertain environment. It is fully observable. Every variable that matters is crisp, deterministic, and accessible. To replace this with pixels is to voluntarily abandon the clarity that simulation exists to provide.</p> <p>It is important to remember why SUMO has visualization at all. The renderer exists so that humans can watch the simulation unfold. It is a debugging tool, a qualitative sanity check, a way to illustrate traffic scenarios for presentations and reports. The rendering is not a sensory modality of the environment, and it was never meant to be. Its purpose is interpretive, not generative.</p> <p>If an agent were to rely on SUMO’s visual output, it would be relying on something designed explicitly for human interpretation. And human interpretation thrives on abstractions, simplifications, and aesthetic conventions—all of which run counter to what machine learning expects from observational data.</p> <hr/> <h2 id="comment">Comment</h2> <p>The visual output of SUMO does not fail because it is low quality. It fails because it is the wrong abstraction. It belongs to a layer of the system intended for people, not agents. The real substance of SUMO such as the logic, the topology, the numerics and the decisions is found in its internal state, not in its renderings.</p> <p>This is why SUMO’s images should never be used as RL input. Not because they are flawed, but because they are unrelated to the simulation’s meaning. To rely on them is to ignore the very nature of the environment we are trying to learn from.</p>]]></content><author><name></name></author><category term="Simulation"/><category term="Traffic"/><category term="RL"/><category term="SUMO"/><summary type="html"><![CDATA[A examination of why the visual output of SUMO. Despite being clean and intuitive, it cannot serve as learning data for RL agents, and why this limitation is inherent in how the simulator is built.]]></summary></entry><entry><title type="html">Re-running an RL Experiment and Getting a Different Answer</title><link href="https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer/" rel="alternate" type="text/html" title="Re-running an RL Experiment and Getting a Different Answer"/><published>2024-11-17T02:08:00+00:00</published><updated>2024-11-17T02:08:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Not long ago, I tried to reproduce one of my RL experiments on a cloud server. The same code had run earlier on a local lab machine, and both hosts were equipped with <strong>NVIDIA RTX 4090 GPUs</strong>. The driver versions matched, the CUDA and PyTorch versions were identical, the environment dependencies mirrored each other, and every random seed was fixed. Under such conditions, the expectation was simple: the two training curves should overlap almost perfectly.</p> <p>But this time, they didn’t. For the first few thousand steps, everything behaved exactly as expected. The lines overlapped so closely that they were visually indistinguishable. As training continued, however, a slight divergence appeared—barely noticeable at first, then increasingly persistent. Eventually, the two runs settled into significantly different behaviors. All high-level variables were controlled, and yet the divergence persisted.It wasn’t dramatic, but it was unmistakable. And it prompted me to re-examine some of the more fragile aspects of RL systems that often go unnoticed.</p> <hr/> <h2 id="two-curves-that-should-have-been-one">Two Curves That Should Have Been One</h2> <p>The most striking part of this incident was how cleanly the divergence unfolded. During the early stage, the critic’s loss, the policy statistics, and the rewards from the environment aligned almost exactly between the two machines. The curves felt stable, even reassuring.</p> <p>Then the shift began. It wasn’t a sudden jump but a slow drift, like two lines that started parallel but eventually grew a small angle between them. Once the angle existed, the distance between the lines increased gradually and inevitably. What began as a tiny deviation eventually widened into a visible performance gap.</p> <p>This kind of “quiet drift” is rare in supervised learning, but painfully common in RL, where feedback loops amplify small differences.</p> <hr/> <h2 id="investigation">Investigation</h2> <p>I didn’t start by suspecting the GPUs. Instead, I reviewed the usual suspects in a calm, methodical way:</p> <p>Whether the environment returned identical resets, whether the model initialization and random number streams matched, whether the replay buffer might have desynchronized the sampling order, whether the logging system affected timing, and whether the training loop had implicit branches that could influence execution order.</p> <p>All these checks were quick to perform. Nothing at the framework or data-flow level explained the divergence. Which meant the issue had to be buried deeper than most RL bugs—deeper than Python, deeper than PyTorch, deeper than CUDA kernels invoked explicitly in code.</p> <hr/> <h2 id="clue">Clue</h2> <p>The earliest measurable drift appeared not in the policy’s actions, but in the critic’s value estimates. That itself was a clue. The critic is often the most numerically sensitive component in many RL algorithms, and its outputs feed directly into the policy update. If two identical systems begin to disagree, the critic is a natural place to look.</p> <p>The differences were tiny—barely outside floating-point noise—but detectable under scrutiny. In the early stage of training, those deviations did not affect behavior, but they were already present. And because the critic affects everything downstream, even a small mismatch is enough for a long feedback loop to magnify.</p> <hr/> <h2 id="a-subtle-difference-from-identical-hardware">A Subtle Difference From Identical Hardware</h2> <p>Eventually, the real cause became clear:<br/> two GPUs of the same model can still produce slightly different floating-point results.</p> <p>This sounds counterintuitive at first, but it’s neither rare nor mysterious. Several factors can produce such differences:</p> <ul> <li>Slight variations in how CUDA dispatches certain kernels</li> <li>Minor differences introduced by fused multiply–add behavior</li> <li>Driver-level optimizations that change arithmetic ordering</li> <li>Subtle kernel selection differences across installations</li> </ul> <p>The scale of these discrepancies is extremely small—on the order of the last few bits of a float. They are invisible in most workloads. In supervised learning, such noise is diluted by batching, averaging, and the absence of recurrent dependencies. In RL, however, the story is different. RL acts as an amplifier. A microscopic variance in a critic output can change a gradient, which changes a policy, which changes the data distribution, which changes the critic’s future inputs, and so on. Over tens of thousands of iterations, this accumulation can transform an imperceptible discrepancy into a meaningful behavioral difference.</p> <p>At the heart of this phenomenon is the recursive nature of RL training. The critic’s estimation errors influence the policy update. The policy influences the trajectory of states and rewards. These, in turn, influence how the critic is updated. The loop continues, step after step.</p> <p>This structure makes RL far more sensitive to numerical discrepancies than most other machine learning pipelines. A difference invisible at step 2,000 can become visible at step 50,000 simply because it is allowed to feed back into itself. This is not a bug; it is a property of RL.</p>]]></content><author><name></name></author><category term="Reinforcement Learning"/><category term="CUDA"/><category term="Numerical Stability"/><category term="Reproducibility"/><summary type="html"><![CDATA[A engineering reflection on why two RTX 4090 machines produced diverging RL curves despite identical code, seeds, and configurations—and what this reveals about RL’s numerical sensitivity.]]></summary></entry><entry><title type="html">Using Local v2rayN Proxy for Cloud Servers via SSH Reverse Tunnel</title><link href="https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel/" rel="alternate" type="text/html" title="Using Local v2rayN Proxy for Cloud Servers via SSH Reverse Tunnel"/><published>2024-02-02T00:38:10+00:00</published><updated>2024-02-02T00:38:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In mainland China, even cloud servers often struggle to access foreign open-source or academic resources. Many people interpret this as “network censorship,” but in my view the situation is more nuanced. It is not necessarily the result of direct, targeted enforcement against technical resources. Instead, it resembles a systemic consequence of long-term authoritarian governance, where private network operators behave with extreme caution in order to avoid any possible regulatory risk.</p> <p>Under such an environment—opaque rules, inconsistent enforcement, and heavy potential penalties—service providers tend to implement their own overly strict filtering. As a result, traffic to platforms like HuggingFace, GitHub, PyPI, and other purely technical services may be blocked or reset “just in case.” In practice, it becomes a case of <em>tying themselves up with their own rope</em>.</p> <p>For developers and researchers, this means that even a cloud server intended for normal machine learning tasks may have difficulty accessing essential foreign resources. This post documents the issues I encountered and the final working solution.</p> <hr/> <h2 id="attempt-1-running-a-proxy-directly-on-the-server">Attempt 1: Running a Proxy Directly on the Server</h2> <p>My first attempt was to deploy a proxy environment directly on the cloud server using sing-box or Xray, and to reuse my existing Shadowsocks 2022 (SS2022) nodes from my Windows machine.</p> <p>This approach ran into multiple problems:</p> <ul> <li>Different implementations of SS2022 use different field names</li> <li>The key format (base64) requirements vary by version</li> <li>Certain fields (e.g., <code class="language-plaintext highlighter-rouge">secondary</code>, <code class="language-plaintext highlighter-rouge">psk</code>) are not supported in older releases</li> <li>Some configurations pass syntax checks but fail during actual traffic forwarding</li> <li>sing-box support for SS2022 differs significantly among releases</li> </ul> <p>Even after repeated adjustments, the connection remained unstable or unusable.</p> <hr/> <h2 id="attempt-2-letting-the-server-reuse-my-local-v2rayn-proxy-successful">Attempt 2: Letting the Server Reuse My Local v2rayN Proxy (Successful)</h2> <p>Since my local Windows environment with v2rayN worked reliably, I attempted to make the cloud server reuse <strong>my desktop’s proxy</strong> via <strong>SSH reverse port forwarding</strong>.</p> <p>I established an SSH session from the cloud server:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-o</span> <span class="nv">ServerAliveInterval</span><span class="o">=</span>60 <span class="se">\</span>
    <span class="nt">-o</span> <span class="nv">ServerAliveCountMax</span><span class="o">=</span>3 <span class="se">\</span>
    <span class="nt">-R</span> 0.0.0.0:10808:127.0.0.1:10808 <span class="se">\</span>
    root@&lt;server-address&gt;
</code></pre></div></div> <p>This command:</p> <ul> <li>Opens <code class="language-plaintext highlighter-rouge">10808</code> on the cloud server</li> <li>Forwards all traffic from that port</li> <li>Back through the SSH tunnel to my Windows machine</li> <li>Where v2rayN listens on local port <code class="language-plaintext highlighter-rouge">10808</code></li> </ul> <p>Testing the proxy:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--socks5</span> 127.0.0.1:10808 https://ipinfo.io/ip
</code></pre></div></div> <p>The returned IP was exactly the exit node of my v2rayN setup, confirming that the cloud server was successfully using my local proxy.</p> <hr/> <h2 id="python-configuration">Python Configuration</h2> <p>Once <code class="language-plaintext highlighter-rouge">curl</code> worked, I needed Python (especially <code class="language-plaintext highlighter-rouge">requests</code> and HuggingFace Hub) to use SOCKS5.</p> <p>Python does not support SOCKS by default, so I installed:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pysocks
</code></pre></div></div> <p>Then set the environment variables:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">http_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
<span class="nb">export </span><span class="nv">https_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
<span class="nb">export </span><span class="nv">all_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
</code></pre></div></div> <p>After this, both <code class="language-plaintext highlighter-rouge">requests.get()</code> and HuggingFace model downloads worked correctly.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>It is important to note that deploying a proxy service directly on the server is not only feasible but is, in fact, the more standard, professional, and long-term maintainable approach. Whether using sing-box, Xray, Hysteria, or Tuic, one can build a fully independent outbound capability on the cloud server, which aligns better with the engineering practice of “managing your own network boundary.”</p> <p>However, this approach typically involves multiple layers of complexity: protocol specifications, key formats, server–client version compatibility, differences in supported cipher suites, and firewall behavior. This is especially true for SS2022, which lacks unified documentation and consistent implementation across projects. As a result, several subtle issues may arise during configuration and require careful troubleshooting.</p> <p>In contrast, reusing a local proxy through SSH reverse port forwarding serves as a fast, low-overhead, and almost configuration-free alternative. Its main advantage is that it works immediately and is not affected by the cloud provider’s network policies. The drawback, of course, is that it depends on the local machine being online, making it unsuitable as a long-term infrastructure solution.</p> <p>Based on this distinction: for time-sensitive situations, such as urgently needing to download model weights from HuggingFace, reusing a local proxy is extremely convenient. But for long-term project environments, if the goal is to maintain an autonomous and stable outbound capability on the server, deploying a proper proxy service on the server remains the recommended final solution.</p>]]></content><author><name></name></author><category term="Networking"/><category term="Proxy"/><category term="SSH"/><category term="DevOps"/><summary type="html"><![CDATA[A practical record of troubleshooting outbound network restrictions on Chinese cloud servers and enabling stable access to foreign academic resources.]]></summary></entry></feed>