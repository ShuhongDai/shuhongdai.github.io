<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shuhongdai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shuhongdai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-28T14:03:52+00:00</updated><id>https://shuhongdai.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Access-Controlled Randomness in TFT: Unlockable Champions and the Structural Logic Behind Patch 16.3</title><link href="https://shuhongdai.github.io/blog/2026/TFT/" rel="alternate" type="text/html" title="Access-Controlled Randomness in TFT: Unlockable Champions and the Structural Logic Behind Patch 16.3"/><published>2026-01-28T00:38:10+00:00</published><updated>2026-01-28T00:38:10+00:00</updated><id>https://shuhongdai.github.io/blog/2026/TFT</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2026/TFT/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In earlier seasons of Teamfight Tactics, the card pool could be modeled in a relatively clean way as a shared and static random resource. Champions were preloaded into a global pool, access to that pool was conditioned primarily on player level, and strategic interaction emerged almost entirely through inventory competition. Within this framework, most balance issues could be addressed locally by tuning pool sizes or appearance probabilities, without altering the structure of the system itself. The introduction of <a href="https://teamfighttactics.leagueoflegends.com/en-us/news/game-updates/lore-and-legends-overview/">unlockable champions</a> in the current season does not fit comfortably into this model, and treating it as a conventional probability adjustment risks missing what has actually changed.</p> <p>At first glance, the mechanic appears straightforward. Players satisfy certain conditions, unlock specific champions, and those champions may then appear in their shops. Framed this way, the system looks like a simple expansion of available options. However, this interpretation becomes problematic as soon as one asks a more precise question: are these champions being added to the card pool, or is access to an existing pool being selectively granted? The distinction matters. Directly injecting dozens of new champions into a shared pool would create severe dilution effects, undermine competitive symmetry, and scale poorly across future seasons. For these reasons alone, a naive “add to pool” interpretation is difficult to reconcile with a stable long-term design.</p> <p>The more coherent reading, and the one adopted in this post, is that the card pool itself remains conceptually intact, while the rules governing how individual players access it have been restructured. Unlocking a champion does not primarily change what exists in the pool, but how that champion is weighted, filtered, and surfaced during shop generation for a given player. In this sense, the innovation is not a new set of cards, but a new access layer that sits between players and a shared resource. The sections that follow focus on this layer, examining how unlockable champions and <a href="https://teamfighttactics.leagueoflegends.com/en-us/news/game-updates/teamfight-tactics-patch-16-3/">recent adjustments to four- and five-cost units</a> can be understood as consequences of a single underlying architectural choice rather than isolated balance patches.</p> <hr/> <h2 id="the-access-control-layer-in-system-abstraction">The Access Control Layer in System Abstraction</h2> <p>In the classical TFT model, the card pool can be abstracted as a shared finite multiset:</p> \[\mathcal{C} = {(c_1, n_1), (c_2, n_2), \dots}\] <p>Here, \(c_k\) denotes a specific champion, while \(n_k\) denotes the remaining quantity of that champion in the pool.</p> <p>When a player $ i $ refreshes their shop at level \(\ell_i\), the operation can be described as a conditional random sampling process:</p> \[\text{Shop}_i \sim \text{Sample}\big(\mathcal{C} \mid \ell_i\big)\] <p>The conditioning occurs exclusively at the cost-tier level, through predefined level probability tables. Individual champions within the same cost tier are otherwise symmetric.</p> <p>The essential property of this model is that all players operate within the same probability space. Differences in outcomes arise only through inventory depletion caused by other players’ purchases.</p> <p>Attempting to introduce unlockable champions directly into the model above leads to an immediate structural conflict. Unlock state \(U_i\) is a player-specific variable, while the sampling space \(\mathcal{C}\) is global and shared. Without an additional abstraction layer, these two elements cannot be composed in a coherent way.</p> <p>This is precisely why two seemingly obvious approaches fail at the system level. Adding unlocked champions directly into the global pool undermines probability stability. Creating fully independent pools per player eliminates the competitive interaction that defines the game.</p> <p>In short, the traditional model lacks the expressive capacity required to represent player-specific access constraints.</p> <p>To preserve both a shared card pool and individualized unlock states, the system must introduce an intermediate layer. The pool itself cannot be sampled directly. Instead, what is sampled is a player-specific method of accessing the pool. I will refer to this layer as an access control layer. The pool remains global, but the path through which each player interacts with it becomes conditional.</p> <p>Under this revised abstraction, a global shared pool still exists:</p> \[\mathcal{C}_{\text{global}}\] <p>However, the shop presented to player \(i\) is generated by:</p> \[\text{Shop}*i \sim \text{Sample}\big(\mathcal{C}*{\text{global}},; w_i(\cdot)\big)\] <p>Here, \(w_i(c) \ge 0\) is a player-dependent weight function. It determines both whether a champion is visible to the player and how frequently it appears. This reframing leads to a crucial distinction. Unlocking does not alter the contents of the card pool. Instead, it modifies the weighting applied during sampling.</p> <p>For each player \(i\), define an unlock set:</p> \[U_i \subseteq \mathcal{C}_{\text{global}}\] <p>The weight function can then be expressed as:</p> \[w_i(c)= \left\{ \begin{aligned} 0, &amp;\quad \text{if } c \text{ is unlockable and } c \notin U_i \\ \alpha_i(c), &amp;\quad \text{if } c \in U_i \\ 1, &amp;\quad \text{if } c \text{ is a standard champion} \end{aligned} \right.\] <p>The term \(\alpha_i(c)\) represents a dynamically adjusted parameter. As will be discussed later, it supports mechanisms such as probability decay for ignored champions, compensation under competition, and suppression of uncontrolled three-star acquisition.</p> <p>Viewed from an implementation perspective, a shop refresh can be described schematically as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">function</span> <span class="nf">refresh_shop</span><span class="p">(</span><span class="n">player</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="n">global_card_pool</span><span class="p">.</span><span class="nf">remaining</span><span class="p">()</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">card</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">:</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="nf">base_weight</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">level_i</span><span class="p">)</span> <span class="o">*</span> <span class="nf">access_weight</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

    <span class="n">shop</span> <span class="o">=</span> <span class="nf">weighted_sample</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">slot_count</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">shop</span>
</code></pre></div></div> <p>The function <code class="language-plaintext highlighter-rouge">base_weight</code> corresponds to the traditional level-based cost distribution. The function <code class="language-plaintext highlighter-rouge">access_weight</code> represents the newly introduced interface.</p> <p>This design choice is important. All new mechanics are implemented through adjustments to <code class="language-plaintext highlighter-rouge">access_weight</code>, while the underlying pool structure remains untouched.</p> <p>Official descriptions state that unlocked champions appear in the rightmost shop slot upon refresh. At a system level, this is unlikely to be a purely visual decision.</p> <p>A more plausible interpretation is that shop generation is divided into multiple sampling stages. At least one slot is drawn from a subspace governed by a distinct weighting scheme, dedicated to unlockable champions. This interpretation aligns naturally with the access control abstraction and will be examined in detail in the next chapter.</p> <hr/> <h2 id="implementation-of-unlockable-champions">Implementation of Unlockable Champions</h2> <p>When a player unlocks a champion, nothing happens to the global card pool. No inventory is added. No card instance is created. No private copy of the pool is spawned. From the system’s perspective, the only event is that the player is now permitted to reference an entity that already exists. This makes unlocking closer to an access control update than to a modification of game data.</p> <p>For each player \(i\), the system maintains a set</p> \[U_i \subseteq \mathcal{C}_{\text{unlockable}},\] <p>representing the unlockable champions that the player is allowed to access.</p> <p>At the implementation level, this is likely no more than a set of identifiers:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>player.unlocked_champions = Set&lt;champion_id&gt;
</code></pre></div></div> <p>The unlock operation itself is therefore a pure state update:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">function</span> <span class="nf">unlock</span><span class="p">(</span><span class="n">player</span> <span class="n">i</span><span class="p">,</span> <span class="n">champion</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">i</span><span class="p">.</span><span class="n">unlocked_champions</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</code></pre></div></div> <p>Crucially, this operation has no immediate gameplay effect. The behavioral change only manifests at the next shop refresh.</p> <p>The official description emphasizes that an unlocked champion appears in the rightmost shop slot upon the next refresh. A natural and low-coupling implementation would separate the process as follows:</p> <ul> <li>Standard slots are sampled from the regular card pool, using the traditional cost-tier probabilities.</li> <li>A dedicated unlockable slot is sampled from the subset of unlocked champions, using a separate weighting scheme.</li> </ul> <p>In pseudocode, this could be expressed as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">function</span> <span class="nf">refresh_shop</span><span class="p">(</span><span class="n">player</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">shop</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">shop</span> <span class="o">+=</span> <span class="nf">sample_standard_pool</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">standard_slots</span><span class="p">)</span>
    <span class="n">shop</span> <span class="o">+=</span> <span class="nf">sample_unlockable_pool</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">unlockable_slot</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">shop</span>
</code></pre></div></div> <p>This structure has several desirable properties. Existing logic remains untouched. The unlockable system can be inserted or removed as a module. The visual layout of the shop aligns directly with its underlying semantics.</p> <p>One of the most technically revealing rules is the following: if a player unlocks a champion but repeatedly ignores it, the probability of seeing that champion decreases, down to a floor of 20 percent. To implement this rule, the system must maintain, for each player–champion pair, a state variable</p> \[s_i(c) \in [\beta, 1], \quad \beta = 0.2.\] <p>Here, \(s_i(c)\) represents the current weight modifier for champion \(c\) when sampled for player \(i\). The variable initializes at 1, decreases monotonically with consecutive non-purchases, and resets to 1 upon purchase.</p> <p>Combining this with the abstraction from the previous chapter, the effective weight becomes</p> \[w_i(c) = w_{\text{base}}(c, \ell_i) \cdot s_i(c),\] <p>where \(w_{\text{base}}\) encodes the standard level-dependent cost probabilities, and \(s_i(c)\) acts as a behavior-conditioned correction term.</p> <p>From an engineering standpoint, this structure is unusually clean. The base probabilities remain stable. Behavioral feedback is isolated in a multiplicative factor. The mechanism can be tuned, extended, or disabled without rewriting the core sampling logic.</p> <p>Without such decay, unlocked champions would permanently pollute the shop, even when the player has no intention of using them. The system would repeatedly surface options that the player has already rejected.</p> <p>With decay in place, the shop gradually converges toward the player’s revealed preferences. The system infers intent implicitly, without requiring any explicit declaration. In effect, this is a weakly adaptive random process.</p> <p>Another point that invites confusion is the statement that unlocked champions are drawn from a shared pool when multiple players unlock the same unit. This implies a strict separation of responsibilities: The inventory is global. The probabilities are local.</p> <p>Let \(N_c\) denote the global remaining count of an unlockable champion \(c\).</p> <p>When any player draws \(c\), the value of \(N_c\) decreases by one. However, the probability that \(c\) appears in a given player’s shop is governed by that player’s weight function \(w_i(c)\). This results in an asymmetric access model to a shared resource.</p> <p>If probabilities were shared but inventories were private, competition would effectively disappear. Card denial, pool reading, and all higher-order interactions around contested units would collapse.</p> <p>Once this architecture is in place, a issue becomes unavoidable. <strong>If a high-cost champion is unlocked by only a single player, and its inventory faces no competition, the system risks becoming overly permissive. Given enough time, a three-star outcome approaches certainty.</strong> The next chapter addresses how this problem is resolved, using the same architectural principles rather than ad hoc fixes.</p> <hr/> <h2 id="conditional-correction-in-patch-163">Conditional Correction in Patch 16.3</h2> <p>In <a href="https://teamfighttactics.leagueoflegends.com/en-us/news/game-updates/teamfight-tactics-patch-16-3/">Patch 16.3</a>, Teamfight Tactics introduced an adjustment to the behavior of unlockable champion:</p> <ul> <li>For unlockable 4-cost and 5-cost champions, it has become harder to reach 3-star when there is no competition, meaning only a single player has unlocked the champion.</li> <li>When there is competition, defined as at least one other player unlocking the same champion, reaching 2-star becomes easier, while reaching 3-star remains difficult.</li> </ul> <p>Among players, this change has often been interpreted in practical terms. Common explanations include the claim that the developers no longer want players to force 3-star 4-cost champions at level 8, or that solo unlock strategies were quietly nerfed through probability adjustments. Viewed through the architectural framework established in the previous chapters, however, this interpretation misses the underlying structure. The 16.3 update is not an isolated balance patch.</p> <p>The unlockable champion system implicitly assumes that access to a champion will, in most games, be shared by multiple players. It follows directly from two design commitments discussed earlier: inventory remains global, and competitive pressure is preserved through shared depletion.</p> <p>In high-level play, a different pattern quickly emerged. A single player would unlock a specific 4-cost or 5-cost champion, while other players deliberately avoided unlocking it. The result was a prolonged level 8 refresh strategy, with no effective competition for inventory. Over time, the randomness normally enforced by shared access was averaged out. Under these conditions, reaching 3-star became increasingly close to a certainty.</p> <p>From a systems perspective, this is not simply a strong strategy. It is an indication that the access control layer is being pushed into a state it was not designed to reward. The card pool remains shared in name, but its effective behavior collapses into near exclusivity.</p> <p>Before Patch 16.3, the probability of a player \(i\) seeing a given champion \(c\) could be abstracted as:</p> \[\mathbb{P}_i(c) = f(\ell_i, \text{cost}_c) \cdot s_i(c)\] <p>Here, \(\ell_i\) denotes player level, and \(s_i(c)\) captures behavioral modifiers such as the decay applied when an unlocked champion is repeatedly ignored. Crucially, this formulation does not encode whether other players have access to the same champion.</p> <p>The 16.3 update introduces a structural change rather than a numeric tweak. The probability model now depends on an additional input:</p> \[\mathbb{P}_i(c) = f(\ell_i, \text{cost}_c, n_c) \cdot s_i(c)\] <p>The new variable \(n_c\) represents the number of players who have unlocked champion \(c\). This change alters the signature of the probability function itself. In implementation terms, it requires the system to explicitly query global unlock state and incorporate it into local sampling logic.</p> <p>Consider the case where \(n_c = 1\). A single player holds access to a high-cost champion, while inventory depletion proceeds unopposed. Under these circumstances, the stochastic element of the card pool is weakened by repetition over time. The system no longer tests judgment under uncertainty, but patience under certainty.</p> <p>From a design standpoint, this outcome is undesirable. It shifts optimization away from decision-making and toward mechanical persistence. The system’s response in 16.3 is therefore to reduce access weight in this specific branch, not by shrinking inventory or enforcing hard caps, but by lowering the effective sampling rate.</p> <p>At first glance, the second part of the update appears counterintuitive. If another player is contesting the same champion, why should reaching 2-star become easier rather than harder? The answer lies in the system’s implicit target states. In practice, high-cost champions are designed around two distinct milestones. Two-star represents a stable and intended outcome. Three-star is meant to be exceptional, requiring significant cost and meaningful interaction.</p> <p>When \(n_c \ge 2\), the system returns to a familiar competitive regime. Inventory pressure is real, and outcomes depend on timing and choices rather than isolation. Under these conditions, the system can safely increase the likelihood of reaching the intermediate state without undermining its broader incentives. Three-star remains constrained by inventory limits and cumulative probability decay.</p> <p>It is tempting to frame the 16.3 changes as a punishment aimed at specific player behavior. That framing, I would argue, is misleading. The system does not evaluate whether a strategy is correct or incorrect. It responds to what the strategy optimizes.</p> <p>Before 16.3, solo unlock combined with extended refreshing was implicitly rewarded. After 16.3, the same behavior is recognized as an abnormal access pattern and adjusted accordingly. Competition, by contrast, is treated as a stabilizing signal that allows probabilities to behave more traditionally.</p>]]></content><author><name></name></author><category term="Game Systems"/><category term="Probability"/><category term="Architecture"/><category term="TFT"/><summary type="html"><![CDATA[A system-level analysis of unlockable champions in TFT, examining how access-controlled card pool design and the Patch 16.3 probability adjustments address extreme competitive states.]]></summary></entry><entry><title type="html">A Quick Mental Model for Estimating LLM GPU Memory Use</title><link href="https://shuhongdai.github.io/blog/2025/A_Quick_Mental_Model_for_Estimating_LLM_GPU_Memory_Use/" rel="alternate" type="text/html" title="A Quick Mental Model for Estimating LLM GPU Memory Use"/><published>2025-11-17T01:00:00+00:00</published><updated>2025-11-17T01:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/A_Quick_Mental_Model_for_Estimating_LLM_GPU_Memory_Use</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/A_Quick_Mental_Model_for_Estimating_LLM_GPU_Memory_Use/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>When browsing new open-source LLM releases, I often have a simple question in mind:<br/> <strong>Will this model actually fit on my GPU?</strong></p> <p>Sometimes the model page shows numbers like \(7\text{B}\), \(14\text{B}\), or \(70\text{B}\), but that alone doesn’t immediately translate into how much memory the model needs once loaded and running. And when I only want a quick sanity check, I don’t want to:</p> <ul> <li>download tens of gigabytes of weights,</li> <li>install a full environment,</li> <li>start a runtime,</li> <li>and only then discover that the model does not fit on the device at all.</li> </ul> <p>For this kind of lightweight judgment, a rough mental model is far more helpful than an exact calculator. It doesn’t need to be accurate to the megabyte. It only needs to answer a practical question: <strong>“Roughly fits?” or “Clearly too large?”</strong></p> <p>This note summarizes the approximation that I use. It’s not a formal derivation. It’s simply a way to reason about LLM memory requirements quickly, in a way that works consistently across models.</p> <hr/> <h2 id="what-actually-occupies-gpu-memory">What Actually Occupies GPU Memory</h2> <p>For inference (not training), only a few components meaningfully consume GPU memory:</p> <ol> <li>The model weights</li> <li>The key–value cache used during autoregressive generation</li> <li>Runtime overhead: intermediate buffers, framework allocations, small activations</li> </ol> <p>Optimizer state does not exist during inference, so the overall picture is simpler than training.</p> <p>My routine is just:</p> <ul> <li>estimate weight memory,</li> <li>add the KV cache,</li> <li>apply a small safety margin.</li> </ul> <hr/> <h2 id="from-parameters-to-vram">From Parameters to VRAM</h2> <p>Model parameter counts are usually prominently displayed: \(7\text{B}\), \(13\text{B}\), \(34\text{B}\), \(70\text{B}\), and so on. Converting this into VRAM is straightforward once we remember how many bytes each parameter uses.</p> <p>Typical cases:</p> <ul> <li><strong>FP16 / BF16:</strong> \(2\) bytes per parameter</li> <li><strong>FP32:</strong> \(4\) bytes</li> <li><strong>8-bit quantization:</strong> about \(1\) byte</li> <li><strong>4-bit quantization:</strong> about \(0.5\) byte</li> </ul> <p>This is already enough for fast estimation:</p> <ul> <li><strong>7B FP16 model</strong> → roughly \(7 \times 2\) GB ≈ <strong>14 GB</strong></li> <li><strong>13B FP16 model</strong> → \(13 \times 2\) GB ≈ <strong>26 GB</strong></li> <li><strong>7B 4-bit model</strong> → \(7 \times 0.5\) GB ≈ <strong>3.5 GB</strong></li> </ul> <p>This accounts only for the parameters, not the KV cache.</p> <hr/> <h2 id="kv-cache-and-context-length">KV Cache and Context Length</h2> <p>During generation, transformer decoders maintain a key–value cache for each attention layer. Its size scales with:</p> <ul> <li>number of layers,</li> <li>hidden dimension,</li> <li>context length.</li> </ul> <p>The exact computation is more detailed, but for estimation purposes, only the magnitude matters. In practice: <strong>KV cache often contributes hundreds of megabytes to several gigabytes</strong>, depending on the context window.</p> <p>For many modern \(6\text{B}\)–\(8\text{B}\)-scale models in FP16:</p> <ul> <li>every \(1\text{k}\) tokens of context usually costs <strong>a few hundred MB</strong> of KV cache</li> <li>a \(4\text{k}\)–\(8\text{k}\) context easily adds <strong>1–3 GB</strong></li> <li>long-context models might require more</li> </ul> <p>This rough rule-of-thumb is accurate enough to determine whether a model with a particular context window fits on an average \(24\text{GB}\)–\(48\text{GB}\) GPU.</p> <hr/> <h2 id="putting-the-pieces-together">Putting the Pieces Together</h2> <p>Once the weight size and KV cache are roughly known, the total memory is just the sum plus a safety margin. In practice, I use a simple heuristic.</p> <p>First, compute an approximate parameter memory from:</p> \[\text{ParameterMemory} \approx \text{NumberOfParameters} \times \text{BytesPerParameter}\] <p>Then add room for the KV cache, based on the context length. Finally, leave some headroom to account for framework overhead and fragmentation.</p> <p>If I want to compress this into a single sentence I can recall mentally, it would be: Take the parameter size, add a few gigabytes for KV cache, add a buffer, and that’s your practical VRAM requirement.</p> <hr/> <h2 id="additional-notes">Additional Notes</h2> <p>A few architectural details can influence memory usage in practice, even when using the simple estimation model above:</p> <ol> <li> <p><strong>KV cache usage varies across architectures</strong> Hidden sizes and layer counts differ between models. For example, even at similar 7B scales, newer efficient architectures such as <strong>Qwen2.5</strong> and <strong>Mistral</strong> typically use less KV memory per 1k tokens than earlier LLama-style models, and smaller models require even less. The “hundreds of MB per 1k tokens” rule still holds, but the exact amount can vary.</p> </li> <li> <p><strong>A small amount of extra weight tensors exists</strong> Beyond the main linear weights, models include embeddings, LayerNorm parameters, and other small tensors. These usually contribute under 5% of the total size, so the rough estimate remains valid, but the actual VRAM will be slightly higher than the simple parameter × bytes calculation.</p> </li> <li> <p><strong>Quantized models may include additional metadata</strong> Although 4-bit and 8-bit quantization can be estimated as 0.5 or 1 byte per parameter, many implementations store per-channel scales, zero-points, or other auxiliary data. This means the practical VRAM usage is often somewhat higher than the theoretical minimum.</p> </li> </ol> <p>These nuances don’t affect the overall direction of the estimate but matter when pushing GPU limits or optimizing for tight VRAM constraints.</p>]]></content><author><name></name></author><category term="LLMs"/><category term="GPU Memory"/><summary type="html"><![CDATA[Before downloading a large model or spinning up a container, it’s useful to know whether an open-source LLM will actually fit on your GPU.]]></summary></entry><entry><title type="html">Designing a Maintainable Replay Buffer in RL Systems</title><link href="https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_RL_Systems/" rel="alternate" type="text/html" title="Designing a Maintainable Replay Buffer in RL Systems"/><published>2025-10-21T12:31:00+00:00</published><updated>2025-10-21T12:31:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_RL_Systems</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_RL_Systems/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In many RL implementations, the replay buffer is introduced as a small but necessary tool, something that stores transitions and hands out mini-batches without attracting much attention. Early tutorials often depict it as a simple queue with random access, as if its role were little more than bookkeeping. Early tutorials often depict it as a simple queue with random access, as if its role were little more than bookkeeping.</p> <p>Once we move beyond toy setups, however, its character changes. As algorithms broaden and experiments run for weeks rather than minutes, the replay buffer shifts from a background utility to a structural anchor. I’ve found that its design quietly shapes not only training stability but also how intelligible and modifiable the surrounding codebase becomes. A system that invites experimentation usually reveals a buffer that has been treated with more care than the introductory treatments suggest.</p> <p>In this piece, I aim to reflect on replay buffer design from a pragmatic engineering perspective: what purposes the buffer ultimately serves, which structural choices tend to hold up under growing demands, and how certain principles help prevent chronic headaches in expanding projects.</p> <hr/> <h2 id="the-replay-buffer-as-a-dataflow-node">The Replay Buffer as a Dataflow Node</h2> <p>Although people often describe a replay buffer as mere “storage,” that framing is somewhat misleading. In practice, it acts as a dataflow node that sits between several competing processes:</p> <ul> <li>the environment generating new experience,</li> <li>the update loop that consumes it,</li> <li>auxiliary components such as logging or evaluation,</li> <li>and, quite often, reproducibility mechanisms lurking in the background.</li> </ul> <p>Thinking of the buffer as part of a larger dataflow clarifies its real function. It does not simply hold transitions; it mediates consistency, shapes the boundaries between modules, and perhaps unexpectedly affects how naturally an RL system can adapt as research directions evolve.</p> <p>Even as RL algorithms diverge (off-policy methods pushing one way, on-policy methods with auxiliary replay pulling another, offline RL taking on a dataset-like shape), the replay buffer’s underlying demands remain remarkably stable. It must provide order, predictability, and a coherent interface across shifting algorithmic choices.</p> <hr/> <h2 id="common-replay-buffer-structures-in-existing-systems">Common Replay Buffer Structures in Existing Systems</h2> <p>Looking across existing RL frameworks, certain structural patterns recur. I often see:</p> <ul> <li><strong>Buffers built on plain Python lists</strong>, favoring immediacy and minimalism.</li> <li><strong>Dictionary-style buffers</strong>, which trade a bit of tidiness for flexibility.</li> <li><strong>Preallocated NumPy (or similar) arrays</strong>, chosen when throughput and determinism matter.</li> <li><strong>Dataset-like implementations</strong>, particularly in offline RL or large frameworks such as RLlib.</li> </ul> <p>Each approach reflects a particular priority: early prototyping, customizable fields, dependable performance, or alignment with dataset tooling. The point is not that any one of them is categorically better; rather, they occupy distinct locations in the design space. Seeing this variety helps clarify the structural constraints a more durable design must address.</p> <hr/> <h2 id="why-maintainability-matters">Why Maintainability Matters</h2> <p>The replay buffer touches nearly everything in an RL pipeline:</p> <ul> <li>environment interaction,</li> <li>training procedures,</li> <li>logging and metric systems,</li> <li>sampling mechanisms,</li> <li>and occasionally distributed actors.</li> </ul> <p>Because of this centrality, small inconsistencies (e.g., shape mismatches, implicit assumptions, overly coupled fields) tend to propagate widely. A design that works for a narrow experiment may later resist extensions such as:</p> <ul> <li>Aadding fields like log-probs or auxiliary targets,</li> <li>adapting to environments that return richer info dictionaries,</li> <li>distinguishing truncation from true termination,</li> <li>or introducing prioritized or sequence-based sampling.</li> </ul> <p>Maintainability, therefore, is less about making the implementation “clean” and more about preserving structural integrity under change.</p> <hr/> <h2 id="design-principles-for-a-maintainable-replay-buffer">Design Principles for a Maintainable Replay Buffer</h2> <p>Supporting a wide spectrum of algorithms and experimental demands rarely requires intricate machinery. More often, it calls for a handful of straightforward design choices that reinforce stability and reduce conceptual friction.</p> <h3 id="1-a-clear-and-explicit-data-schema"><strong>1. A Clear and Explicit Data Schema</strong></h3> <p>Each field, such as observations, actions, rewards, next observations, and the various termination indicators, should be represented explicitly. Attempts to infer structure implicitly usually collapse when a new algorithm introduces an extra field or modifies an existing one.</p> <p>A well-defined schema states:</p> <ul> <li>what each field contains,</li> <li>its shape,</li> <li>its dtype,</li> <li>and the rules governing when it is written.</li> </ul> <p>This explicitness avoids interpretative ambiguity later, especially during sampling.</p> <hr/> <h3 id="2-independence-between-fields"><strong>2. Independence Between Fields</strong></h3> <p>Transitions need not be stored as monolithic tuples. In fact, isolating fields into separate arrays or storage units tends to make systems easier to test, reason about, and extend. It also simplifies batch indexing and accommodates experimental additions without unintended consequences.</p> <p>By decoupling fields, you reduce the chance of cascading side effects, it is an issue I’ve run into often when experimenting with additional annotations or metadata.</p> <hr/> <h3 id="3-preallocation-with-predictable-behavior"><strong>3. Preallocation with Predictable Behavior</strong></h3> <p>A fixed-size ring buffer backed by preallocated arrays typically offers the most stable behavior. It avoids issues such as:</p> <ul> <li>unpredictable memory growth,</li> <li>fragmentation,</li> <li>and costly resizes.</li> </ul> <p>A simple write pointer and a size counter usually suffice. In my experience, predictability is worth far more than cleverness here.</p> <hr/> <h3 id="4-decoupled-sampling-logic"><strong>4. Decoupled Sampling Logic</strong></h3> <p>Sampling tends to evolve quickly in RL research. Keeping sampling separate from storage makes it far easier to test new possibilities:</p> <ul> <li>uniform sampling,</li> <li>stratification,</li> <li>prioritized replay,</li> <li>sequence extraction for RNNs,</li> <li>long-horizon temporal sampling.</li> </ul> <p>When storage imposes no constraints on sampling, algorithmic exploration becomes far more straightforward.</p> <hr/> <h3 id="5-stable-batch-shapes-and-typing"><strong>5. Stable Batch Shapes and Typing</strong></h3> <p>A surprising number of bugs originate from shape inconsistencies. Ensuring that shapes and dtypes are fixed when the buffer initializes, and validating them whenever data are written, helps guarantee stable tensors for training routines, predictable input formats for models, and early detection of environment misconfigurations. This holds across vector observations, mixed modalities, discrete or continuous actions, and even more specialized forms of data.</p> <hr/> <h2 id="a-practical-replay-buffer-structure">A Practical Replay Buffer Structure</h2> <p>A maintainable replay buffer often adopts a design similar to the following conceptual structure:</p> <ul> <li>a defined <strong>capacity</strong>,</li> <li>a <strong>write index</strong> indicating the next insertion point,</li> <li>a <strong>current size</strong> reflecting valid data,</li> <li>a dictionary of <strong>fields</strong>, each holding a preallocated array,</li> <li>and a sampling interface that accepts indices and returns assembled batches.</li> </ul> <p>Such a structure supports smooth extensions (new fields slot naturally into place), stable sampling (driven entirely by batch indices), and the option to modify or replace sampling strategies without disturbing storage. It works for both on-policy and off-policy settings, provided the surrounding logic is appropriate.</p> <p>The precise API matters less than the emphasis on clarity, decoupling, and predictable behavior.</p> <hr/> <h2 id="memory-and-performance-considerations">Memory and Performance Considerations</h2> <p>Replay buffers sometimes operate close to memory limits, since long-horizon tasks or high-frequency transitions can generate substantial load. Sensible engineering choices include:</p> <ul> <li>selecting appropriate dtypes (e.g., <code class="language-plaintext highlighter-rouge">float32</code> unless higher precision is essential),</li> <li>trimming or compressing non-essential fields,</li> <li>keeping arrays contiguous to reduce overhead,</li> <li>and, in distributed scenarios, deciding carefully where storage resides.</li> </ul> <p>Performance is certainly relevant, but for most research-level systems, I find that clarity and invariants tend to matter more. Once those are in place, optimizing hotspots becomes easier and safer.</p> <hr/> <h2 id="the-role-of-replay-buffers-in-larger-rl-architectures">The Role of Replay Buffers in Larger RL Architectures</h2> <p>As RL systems scale, the replay buffer assumes different personas:</p> <ul> <li>In <strong>off-policy RL</strong>, it stabilizes learning by shaping the distribution of samples.</li> <li>In <strong>offline RL</strong>, it effectively is the dataset interface.</li> <li>In <strong>model-based RL</strong>, it may hold both real and generated transitions side by side.</li> <li>In <strong>multi-agent RL</strong>, it often mediates data across agents or environments.</li> <li>In <strong>distributed RL</strong>, it can serve as a central data service or coordination layer.</li> </ul> <p>A well-designed buffer tends to move across these contexts with little structural modification, which is a strong indicator that its design principles are sound.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>The replay buffer, though rarely celebrated, is one of the key infrastructural elements in reinforcement learning systems. Its design shapes how reliably the rest of the pipeline behaves and how easily new ideas can be integrated. A durable buffer is grounded in a few guiding practices such as explicit schemas, independent fields, predictable mechanics, and sampling logic that remains clearly separated from storage. When these practices are in place, the buffer becomes a stable foundation rather than a recurring point of fragility.</p>]]></content><author><name></name></author><category term="RL"/><category term="System Design"/><category term="Data Structures"/><summary type="html"><![CDATA[A structured and engineering-focused reflection on replay buffer design in RL, emphasizing clarity, extensibility, and long-term maintainability.]]></summary></entry><entry><title type="html">Tracing the Root Cause of Missing GPUs in Docker Containers</title><link href="https://shuhongdai.github.io/blog/2025/Tracing_the_Root_Cause_of_Missing_GPUs_in_Docker_Containers/" rel="alternate" type="text/html" title="Tracing the Root Cause of Missing GPUs in Docker Containers"/><published>2025-08-20T01:02:00+00:00</published><updated>2025-08-20T01:02:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Tracing_the_Root_Cause_of_Missing_GPUs_in_Docker_Containers</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Tracing_the_Root_Cause_of_Missing_GPUs_in_Docker_Containers/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The GPU on my host machine worked flawlessly. <code class="language-plaintext highlighter-rouge">nvidia-smi</code> showed four cleanly indexed devices. CUDA tests ran without complaint. Nothing looked suspicious. But inside Docker, those same GPUs simply vanished. The container insisted it had no GPU at all, even when launched with <code class="language-plaintext highlighter-rouge">--gpus all</code>. This was not an application-level issue. Problems like this never come from the code running inside the container. They come from a mismatch somewhere between Docker, the NVIDIA runtime, and the host’s driver stack. This post is a reconstruction of how I verified that assumption and worked my way through the layers until the system finally admitted what was wrong.</p> <hr/> <h2 id="the-initial-symptom">The Initial Symptom</h2> <p>The first sign of trouble came from a simple test:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">--gpus</span> all ubuntu:22.04 nvidia-smi
</code></pre></div></div> <p>The output didn’t show a GPU. It didn’t even show a driver mismatch. Instead I received:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.
</code></pre></div></div> <p>This message is always unhelpful. It can mean anything from missing libraries to container runtime failures to a completely broken driver. But since <code class="language-plaintext highlighter-rouge">nvidia-smi</code> worked perfectly on the host, the problem had to be elsewhere.</p> <p>I checked the host first, just to be certain.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-smi
Sun Aug 17 00:15:19 2025
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 550.54       Driver Version: 550.54       CUDA Version: 12.4     |
| GPU Name        Persistence-M| Bus-Id ... <span class="o">(</span>normal output<span class="o">)</span>                   |
+-----------------------------------------------------------------------------+
</code></pre></div></div> <p>Everything here was healthy. That told me the problem was not hardware. It also told me the Docker container was not receiving the correct runtime environment.</p> <hr/> <h2 id="a-quick-look-at-the-nvidia-container-toolkit">A Quick Look at the NVIDIA Container Toolkit</h2> <p>The next step was confirming that the NVIDIA container runtime existed on the host.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dpkg <span class="nt">-l</span> | <span class="nb">grep</span> <span class="nt">-i</span> nvidia-container
ii  nvidia-container-toolkit  1.16.2-1  amd64
</code></pre></div></div> <p>The toolkit was installed, at least according to the package manager. But “installed” is not the same as “integrated.” I checked Docker’s runtime configuration:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /etc/docker/daemon.json
<span class="o">{</span>
    <span class="s2">"runtimes"</span>: <span class="o">{</span>
        <span class="s2">"nvidia"</span>: <span class="o">{</span>
            <span class="s2">"path"</span>: <span class="s2">"nvidia-container-runtime"</span>,
            <span class="s2">"runtimeArgs"</span>: <span class="o">[]</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div> <p>At first glance this looked reasonable but Docker’s configuration files often lie by omission. I restarted Docker anyway, hoping for the rare case where a restart solves a real problem.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>systemctl restart docker
</code></pre></div></div> <p>It changed nothing.</p> <hr/> <h2 id="the-runtime-that-didnt-exist">The Runtime That Didn’t Exist</h2> <p>I tried manually invoking the runtime:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>which nvidia-container-runtime
/usr/bin/nvidia-container-runtime
</code></pre></div></div> <p>It was present. But inside Docker, the container still couldn’t see GPUs. I suspected mismatch in library paths, so I inspected the toolkit’s log:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>journalctl <span class="nt">-u</span> nvidia-container-runtime
</code></pre></div></div> <p>The log was silent with no errors and no warnings, which is often more suspicious than a screaming log file.</p> <hr/> <h2 id="trying-a-minimal-container">Trying a Minimal Container</h2> <p>Sometimes it helps to remove everything unrelated. I tried an empty Alpine container with explicit runtime:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--runtime</span><span class="o">=</span>nvidia <span class="nt">--rm</span> nvidia/cuda:12.4-base nvidia-smi
</code></pre></div></div> <p>The result was the same:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.
</code></pre></div></div> <p>At this point I was certain the issue wasn’t in the image. The problem was in the host-to-container plumbing.</p> <hr/> <h2 id="nvidia-container-cli">nvidia-container-cli</h2> <p>I checked low-level diagnostics:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-container-cli info
</code></pre></div></div> <p>This should list driver version, devices, and capabilities. Instead it printed:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ERRO[0000] could not load NVML: libnvidia-ml.so.1: cannot open shared object file: No such file or directory
</code></pre></div></div> <p>The NVML library is part of the NVIDIA driver. If the toolkit couldn’t load it, that meant the toolkit’s library search paths did not match the actual driver installation. Which usually happens after a driver upgrade that leaves symlinks pointing to the wrong place.</p> <p>I checked the library:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> /usr/lib/x86_64-linux-gnu/libnvidia-ml.so<span class="k">*</span>
</code></pre></div></div> <p>The files were indeed present, but I noticed they were under:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/lib/x86_64-linux-gnu/nvidia/current/
</code></pre></div></div> <p>while the toolkit expected:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/lib/x86_64-linux-gnu/
</code></pre></div></div> <p>This mismatch is easy to miss.</p> <hr/> <h2 id="the-missing-symlink">The Missing Symlink</h2> <p>On many systems, <code class="language-plaintext highlighter-rouge">/usr/lib/x86_64-linux-gnu/</code> should contain symlinks to the actual driver libraries, but mine didn’t. I created the symlink manually:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo ln</span> <span class="nt">-s</span> /usr/lib/x86_64-linux-gnu/nvidia/current/libnvidia-ml.so.1 <span class="se">\</span>
             /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
</code></pre></div></div> <p>Then I tried the diagnostic again:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-container-cli info
</code></pre></div></div> <p>This time it printed a full report with devices, drivers, and capabilities listed correctly. That told me NVIDIA’s container toolkit could finally see the GPU.</p> <hr/> <h2 id="the-final-test">The Final Test</h2> <p>With everything aligned, I launched a fresh container:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">--gpus</span> all nvidia/cuda:12.4-base nvidia-smi
</code></pre></div></div> <p>The output looked normal.</p>]]></content><author><name></name></author><category term="Docker"/><category term="NVIDIA"/><category term="CUDA"/><summary type="html"><![CDATA[A debugging record of why Docker refused to expose GPUs inside a container even though the host recognized them perfectly, and how every layer of the system contributed a small piece to the failure.]]></summary></entry><entry><title type="html">Running dm-control on a Headless Server: A Complete Debugging Log</title><link href="https://shuhongdai.github.io/blog/2025/Running-_dm-control-_on_a_Headless_Server/" rel="alternate" type="text/html" title="Running dm-control on a Headless Server: A Complete Debugging Log"/><published>2025-06-15T21:12:00+00:00</published><updated>2025-06-15T21:12:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Running%20_dm-control%20_on_a_Headless_Server</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Running-_dm-control-_on_a_Headless_Server/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post documents the process of configuring <strong>dm-control</strong> on a <strong>headless Ubuntu server</strong> for reinforcement learning experiments that require pixel-based observations. The goal was straightforward: load a task from the DeepMind Control Suite and render frames for a CNN-based policy. However, running dm-control without a graphical display consistently triggered a series of rendering failures.</p> <p>Rather than providing a tutorial, this article records the issues encountered, the different directions explored, and the final configuration that proved reliable. Hopefully, this log will help anyone attempting to run dm-control in a similar environment.</p> <hr/> <h2 id="initial-attempts-and-rendering-errors">Initial Attempts and Rendering Errors</h2> <p>The starting point was simply loading a task and calling <code class="language-plaintext highlighter-rouge">env.physics.render()</code>. On a local machine, this works immediately. On a headless server, the first result was an error referencing <strong>EGL</strong> and <strong>OpenGL</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
AttributeError: 'NoneType' object has no attribute 'eglQueryString'

</code></pre></div></div> <p>Further attempts produced warnings about missing <code class="language-plaintext highlighter-rouge">DISPLAY</code> variables:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
X11: The DISPLAY environment variable is missing

</code></pre></div></div> <p>These messages made it clear that dm-control was attempting to initialize rendering through OpenGL/EGL, both of which require a graphics context the server did not provide. Disabling the <code class="language-plaintext highlighter-rouge">DISPLAY</code> variable or forcing EGL did not resolve the issue; the underlying environment simply lacked the dependencies required for hardware-accelerated rendering.</p> <hr/> <h2 id="investigating-the-role-of-pyopengl">Investigating the Role of PyOpenGL</h2> <p>The repeated mentions of EGL led to checking whether PyOpenGL played a role. Removing PyOpenGL temporarily modified the error messages, but it was automatically reinstalled when upgrading or reinstalling dm-control and Mujoco. This indicated that avoiding the OpenGL backend entirely was not feasible through uninstalling PyOpenGL alone.</p> <hr/> <h2 id="version-mismatch-between-dm-control-and-mujoco">Version Mismatch Between dm-control and Mujoco</h2> <p>At a later stage, entirely different errors appeared:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
eq_active not found
flex_xvert0 not found

</code></pre></div></div> <p>These errors are characteristic of <strong>Mujoco internal structure mismatches</strong>, suggesting that the installed versions of Mujoco and dm-control were not aligned. dm-control indexes Mujoco model structures by field name, and when they do not match, initialization fails even before rendering begins.</p> <p>This confirmed that two separate issues existed:</p> <ol> <li>Rendering backend not compatible with the server</li> <li>dm-control and Mujoco versions not compatible with each other</li> </ol> <p>Both needed to be addressed.</p> <hr/> <h2 id="moving-toward-a-software-rendering-approach">Moving Toward a Software Rendering Approach</h2> <p>Given that neither EGL nor OpenGL would work reliably on the server, the next candidate was <strong>OSMesa</strong>, a pure software renderer. OSMesa performs all rendering on the CPU and does not require a graphical display or GPU drivers. This makes it suitable for cloud or containerized environments.</p> <p>Ubuntu provides OSMesa through:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
libosmesa6-dev

</code></pre></div></div> <p>The key environment variable for Mujoco is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
MUJOCO_GL=osmesa

</code></pre></div></div> <p>With this backend, Mujoco no longer attempts to initialize hardware-accelerated contexts.</p> <hr/> <h2 id="identifying-a-stable-mujoco--dm-control-combination">Identifying a Stable Mujoco + dm-control Combination</h2> <p>After testing multiple combinations, the following versions proved to be both compatible with each other and functional under OSMesa:</p> <ul> <li><strong>mujoco 2.3.3</strong></li> <li><strong>dm-control 1.0.11</strong></li> <li>Python 3.9</li> </ul> <p>This combination avoids the structure-indexing errors seen in newer pairings and also avoids invoking backends requiring OpenGL or EGL by default.</p> <hr/> <h2 id="verifying-the-configuration">Verifying the Configuration</h2> <p>With OSMesa enabled and the compatible versions installed, the following minimal script successfully produced a rendered frame:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">MUJOCO_GL</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">osmesa</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">DISPLAY</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">""</span>

<span class="kn">from</span> <span class="n">dm_control</span> <span class="kn">import</span> <span class="n">suite</span>
<span class="kn">import</span> <span class="n">imageio</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">suite</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">cartpole</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">swingup</span><span class="sh">"</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">physics</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">camera_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Frame shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">imageio</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="sh">"</span><span class="s">soft_render.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
</code></pre></div></div> <p>The output confirmed that rendering worked:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Frame shape: (128, 128, 3)
Saved soft_render.png
</code></pre></div></div> <p>A few non-critical messages appear during interpreter shutdown, related to thread cleanup inside dm-control’s internal executor. These do not affect functionality.</p> <hr/> <h2 id="final-working-setup">Final Working Setup</h2> <p>Summarizing the configuration that worked consistently:</p> <ul> <li><strong>Python</strong>: 3.9</li> <li><strong>Mujoco</strong>: 2.3.3</li> <li><strong>dm-control</strong>: 1.0.11</li> <li><strong>System package</strong>: <code class="language-plaintext highlighter-rouge">libosmesa6-dev</code></li> <li><strong>Rendering backend</strong>: <code class="language-plaintext highlighter-rouge">MUJOCO_GL=osmesa</code></li> </ul> <p>This setup uses CPU-based software rendering, avoids any dependency on GPU drivers or graphical displays, and is compatible with reinforcement-learning algorithms requiring pixel observations.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Running dm-control on a headless server involves more than installing the library. Rendering backends, environment variables, and version compatibility all play important roles in determining whether Mujoco initializes correctly. After exploring several unsuccessful paths, the combination of <strong>OSMesa rendering</strong> and a <strong>compatible pair of dm-control and Mujoco versions</strong> proved to be a reliable solution.</p> <p>By documenting this process, this post aims to provide a clear reference for others setting up dm-control in a similar environment.</p>]]></content><author><name></name></author><category term="Reinforcement Learning"/><category term="Mujoco"/><category term="dm-control"/><category term="Rendering"/><summary type="html"><![CDATA[A practical record of configuring dm-control with Mujoco on a headless Ubuntu server, covering rendering failures, version mismatches, and the final workable setup.]]></summary></entry><entry><title type="html">Why SUMO’s Rendered Videos Should Never Be Used as RL Training Data</title><link href="https://shuhongdai.github.io/blog/2025/Why_SUMO-s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data/" rel="alternate" type="text/html" title="Why SUMO’s Rendered Videos Should Never Be Used as RL Training Data"/><published>2025-01-27T23:40:00+00:00</published><updated>2025-01-27T23:40:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Why_SUMO%E2%80%99s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Why_SUMO-s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The rendered images or videos generated by SUMO have no place in a RL pipeline. They may look clear, structured, and almost “dataset-like,” yet they fundamentally lack the properties that make a visual observation meaningful to an agent. What they provide is appearance, not information.</p> <p>This is not an insight I arrived at after trying to extract learning signals from SUMO’s frames. Rather, it becomes clear the moment one understands how SUMO represents its world internally. The simulator was never built around vision; it was built around structure—precise, explicit, symbolic structure. And because of this, its video output occupies a different conceptual layer entirely.</p> <hr/> <h2 id="visualization-hides-semantics">Visualization Hides Semantics</h2> <p>The rendered frame that SUMO shows us is, in essence, a human-friendly sketch of a world that exists somewhere else. The world SUMO actually simulates consists of continuous lane coordinates, exact positions and velocities, acceleration profiles, signal states, right-of-way relationships, and route intentions that evolve according to a coherent traffic model. This internal world is numerical and symbolic down to its core.</p> <p>None of this structure survives the trip into the visual renderer. A car in a SUMO frame is just a colored rectangle, detached from the lane graph it belongs to, stripped of its intentions and priorities, reduced to a geometry that no longer reveals why it will or will not proceed through an intersection. A traffic signal becomes a green or red dot with no connection to the control logic that governs it. Even spatial quantities, such as gaps or queue lengths, degrade into visual approximations that depend on camera perspective rather than on the simulator’s own measurements. It is tempting to think that pixels “contain enough information,” but the truth is that the renderer deliberately hides most of what matters.</p> <hr/> <h2 id="what-rl-actually-needs">What RL Actually Needs</h2> <p>RL does not learn from appearance, it learns from state. And a useful state is one that preserves the causal relationships driving the environment. With SUMO, those relationships live entirely in its internal representation: the topology of the road network, the numerical values describing motion, the logic that governs priority and right-of-way, the exact timing and sequencing of traffic lights. These are not afterthoughts; they are the environment.</p> <p>A video frame, regardless of how cleanly rendered, cannot express the rules that determine how traffic flows. It does not encode which vehicle is yielding, which is accelerating, which is constrained by downstream occupancy, or which is simply following a route that is invisible to the eye. Observing the render is, at best, witnessing the shadow of a system whose logic has already been stripped away.</p> <p>Training an RL agent on such shadows is not simply inefficient; it is conceptually misaligned. The agent is forced to reconstruct the environment’s logic from incomplete projections of it, even though SUMO already provides the fully-formed state in precise numerical terms. The agent ends up solving a perception problem that exists only because we discarded the very information the simulator was designed to offer.</p> <hr/> <h2 id="reconstructing-what-the-simulator-already-knows">Reconstructing What the Simulator Already Knows</h2> <p>If one insists on using SUMO’s images as input, the learning problem becomes unnecessarily inverted. To make sense of a frame, an agent must infer positions, velocities, lanes, intentions, and interactions—all of which SUMO already calculates. This re-derivation is not only redundant; it is structurally impossible to perform without error, because the necessary information does not exist in the pixels in the first place. SUMO’s renderer never intended to expose it.</p> <p>Vision makes sense when there is no alternative, such as in real-world driving where sensors are noisy and perception is inherently uncertain. But SUMO is not an uncertain environment. It is fully observable. Every variable that matters is crisp, deterministic, and accessible. To replace this with pixels is to voluntarily abandon the clarity that simulation exists to provide.</p> <p>It is important to remember why SUMO has visualization at all. The renderer exists so that humans can watch the simulation unfold. It is a debugging tool, a qualitative sanity check, a way to illustrate traffic scenarios for presentations and reports. The rendering is not a sensory modality of the environment, and it was never meant to be. Its purpose is interpretive, not generative.</p> <p>If an agent were to rely on SUMO’s visual output, it would be relying on something designed explicitly for human interpretation. And human interpretation thrives on abstractions, simplifications, and aesthetic conventions. All of which run counter to what machine learning expects from observational data.</p> <hr/> <h2 id="comment">Comment</h2> <p>The visual output of SUMO does not fail because it is low quality. It fails because it is the wrong abstraction. It belongs to a layer of the system intended for people, not agents. The real substance of SUMO such as the logic, the topology, the numerics and the decisions is found in its internal state, not in its renderings.</p> <p>This is why SUMO’s images should never be used as RL input. Not because they are flawed, but because they are unrelated to the simulation’s meaning. To rely on them is to ignore the very nature of the environment we are trying to learn from.</p>]]></content><author><name></name></author><category term="Simulation"/><category term="Traffic"/><category term="RL"/><category term="SUMO"/><summary type="html"><![CDATA[A examination of why the visual output of SUMO. Despite being clean and intuitive, it cannot serve as learning data for RL agents, and why this limitation is inherent in how the simulator is built.]]></summary></entry><entry><title type="html">Re-running an RL Experiment and Getting a Different Answer</title><link href="https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer/" rel="alternate" type="text/html" title="Re-running an RL Experiment and Getting a Different Answer"/><published>2024-11-17T02:08:00+00:00</published><updated>2024-11-17T02:08:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Not long ago, I tried to reproduce one of my RL experiments on a cloud server. The same code had run earlier on a local lab machine, and both hosts were equipped with <strong>NVIDIA RTX 4090 GPUs</strong>. The driver versions matched, the CUDA and PyTorch versions were identical, the environment dependencies mirrored each other, and every random seed was fixed. Under such conditions, the expectation was simple: the two training curves should overlap almost perfectly.</p> <p>But this time, they didn’t. For the first few thousand steps, everything behaved exactly as expected. The lines overlapped so closely that they were visually indistinguishable. As training continued, however, a slight divergence appeared, barely noticeable at first, then increasingly persistent. Eventually, the two runs settled into significantly different behaviors. All high-level variables were controlled, and yet the divergence persisted.It wasn’t dramatic, but it was unmistakable. And it prompted me to re-examine some of the more fragile aspects of RL systems that often go unnoticed.</p> <hr/> <h2 id="two-curves-that-should-have-been-one">Two Curves That Should Have Been One</h2> <p>The most striking part of this incident was how cleanly the divergence unfolded. During the early stage, the critic’s loss, the policy statistics, and the rewards from the environment aligned almost exactly between the two machines. The curves felt stable, even reassuring.</p> <p>Then the shift began. It wasn’t a sudden jump but a slow drift, like two lines that started parallel but eventually grew a small angle between them. Once the angle existed, the distance between the lines increased gradually and inevitably. What began as a tiny deviation eventually widened into a visible performance gap.</p> <p>This kind of “quiet drift” is rare in supervised learning, but painfully common in RL, where feedback loops amplify small differences.</p> <hr/> <h2 id="investigation">Investigation</h2> <p>I didn’t start by suspecting the GPUs. Instead, I reviewed the usual suspects in a calm, methodical way:</p> <p>Whether the environment returned identical resets, whether the model initialization and random number streams matched, whether the replay buffer might have desynchronized the sampling order, whether the logging system affected timing, and whether the training loop had implicit branches that could influence execution order.</p> <p>All these checks were quick to perform. Nothing at the framework or data-flow level explained the divergence. Which meant the issue had to be buried deeper than most RL bugs—deeper than Python, deeper than PyTorch, deeper than CUDA kernels invoked explicitly in code.</p> <hr/> <h2 id="clue">Clue</h2> <p>The earliest measurable drift appeared not in the policy’s actions, but in the critic’s value estimates. That itself was a clue. The critic is often the most numerically sensitive component in many RL algorithms, and its outputs feed directly into the policy update. If two identical systems begin to disagree, the critic is a natural place to look.</p> <p>The differences were tiny that barely outside floating-point noise but detectable under scrutiny. In the early stage of training, those deviations did not affect behavior, but they were already present. And because the critic affects everything downstream, even a small mismatch is enough for a long feedback loop to magnify.</p> <hr/> <h2 id="a-subtle-difference-from-identical-hardware">A Subtle Difference From Identical Hardware</h2> <p>Eventually, the real cause became clear:<br/> two GPUs of the same model can still produce slightly different floating-point results.</p> <p>This sounds counterintuitive at first, but it’s neither rare nor mysterious. Several factors can produce such differences:</p> <ul> <li>Slight variations in how CUDA dispatches certain kernels</li> <li>Minor differences introduced by fused multiply–add behavior</li> <li>Driver-level optimizations that change arithmetic ordering</li> <li>Subtle kernel selection differences across installations</li> </ul> <p>The scale of these discrepancies is extremely small on the order of the last few bits of a float. They are invisible in most workloads. In supervised learning, such noise is diluted by batching, averaging, and the absence of recurrent dependencies. In RL, however, the story is different. RL acts as an amplifier. A microscopic variance in a critic output can change a gradient, which changes a policy, which changes the data distribution, which changes the critic’s future inputs, and so on. Over tens of thousands of iterations, this accumulation can transform an imperceptible discrepancy into a meaningful behavioral difference.</p> <p>At the heart of this phenomenon is the recursive nature of RL training. The critic’s estimation errors influence the policy update. The policy influences the trajectory of states and rewards. These, in turn, influence how the critic is updated. The loop continues, step after step.</p> <p>This structure makes RL far more sensitive to numerical discrepancies than most other machine learning pipelines. A difference invisible at step 2,000 can become visible at step 50,000 simply because it is allowed to feed back into itself. This is not a bug; it is a property of RL.</p>]]></content><author><name></name></author><category term="Reinforcement Learning"/><category term="CUDA"/><category term="Numerical Stability"/><category term="Reproducibility"/><summary type="html"><![CDATA[A engineering reflection on why two RTX 4090 machines produced diverging RL curves despite identical code, seeds, and configurations. And what this reveals about RL’s numerical sensitivity.]]></summary></entry><entry><title type="html">Using Local v2rayN Proxy for Cloud Servers via SSH Reverse Tunnel</title><link href="https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel/" rel="alternate" type="text/html" title="Using Local v2rayN Proxy for Cloud Servers via SSH Reverse Tunnel"/><published>2024-02-02T00:38:10+00:00</published><updated>2024-02-02T00:38:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In mainland China, cloud servers frequently experience unstable or inconsistent access to foreign open-source and academic platforms. This is often not due to targeted restrictions on specific technical resources, but rather to conservative traffic filtering and risk-averse network configurations adopted by some service providers.</p> <p>As a result, essential platforms such as HuggingFace, GitHub, and PyPI may occasionally become unreachable or suffer from intermittent resets, even when the server is intended solely for standard machine learning or research workloads.</p> <p>This post documents the connectivity issues I encountered on a cloud server and the final stable solution using SSH reverse tunneling.</p> <hr/> <h2 id="attempt-1-running-a-proxy-directly-on-the-server">Attempt 1: Running a Proxy Directly on the Server</h2> <p>My first attempt was to deploy a proxy environment directly on the cloud server using sing-box or Xray, and to reuse my existing Shadowsocks 2022 (SS2022) nodes from my Windows machine.</p> <p>This approach ran into multiple problems:</p> <ul> <li>Different implementations of SS2022 use different field names</li> <li>The key format (base64) requirements vary by version</li> <li>Certain fields (e.g., <code class="language-plaintext highlighter-rouge">secondary</code>, <code class="language-plaintext highlighter-rouge">psk</code>) are not supported in older releases</li> <li>Some configurations pass syntax checks but fail during actual traffic forwarding</li> <li>sing-box support for SS2022 differs significantly among releases</li> </ul> <p>Even after repeated adjustments, the connection remained unstable or unusable.</p> <hr/> <h2 id="attempt-2-letting-the-server-reuse-my-local-v2rayn-proxy-successful">Attempt 2: Letting the Server Reuse My Local v2rayN Proxy (Successful)</h2> <p>Since my local Windows environment with v2rayN worked reliably, I attempted to make the cloud server reuse <strong>my desktop’s proxy</strong> via <strong>SSH reverse port forwarding</strong>.</p> <p>I established an SSH session from the cloud server:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-o</span> <span class="nv">ServerAliveInterval</span><span class="o">=</span>60 <span class="se">\</span>
    <span class="nt">-o</span> <span class="nv">ServerAliveCountMax</span><span class="o">=</span>3 <span class="se">\</span>
    <span class="nt">-R</span> 0.0.0.0:10808:127.0.0.1:10808 <span class="se">\</span>
    root@&lt;server-address&gt;
</code></pre></div></div> <p>This command:</p> <ul> <li>Opens <code class="language-plaintext highlighter-rouge">10808</code> on the cloud server</li> <li>Forwards all traffic from that port</li> <li>Back through the SSH tunnel to my Windows machine</li> <li>Where v2rayN listens on local port <code class="language-plaintext highlighter-rouge">10808</code></li> </ul> <p>Testing the proxy:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--socks5</span> 127.0.0.1:10808 https://ipinfo.io/ip
</code></pre></div></div> <p>The returned IP was exactly the exit node of my v2rayN setup, confirming that the cloud server was successfully using my local proxy.</p> <hr/> <h2 id="python-configuration">Python Configuration</h2> <p>Once <code class="language-plaintext highlighter-rouge">curl</code> worked, I needed Python (especially <code class="language-plaintext highlighter-rouge">requests</code> and HuggingFace Hub) to use SOCKS5.</p> <p>Python does not support SOCKS by default, so I installed:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pysocks
</code></pre></div></div> <p>Then set the environment variables:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">http_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
<span class="nb">export </span><span class="nv">https_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
<span class="nb">export </span><span class="nv">all_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
</code></pre></div></div> <p>After this, both <code class="language-plaintext highlighter-rouge">requests.get()</code> and HuggingFace model downloads worked correctly.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>It is important to note that deploying a proxy service directly on the server is not only feasible but is, in fact, the more standard, professional, and long-term maintainable approach. Whether using sing-box, Xray, Hysteria, or Tuic, one can build a fully independent outbound capability on the cloud server, which aligns better with the engineering practice of “managing your own network boundary.”</p> <p>However, this approach typically involves multiple layers of complexity: protocol specifications, key formats, server–client version compatibility, differences in supported cipher suites, and firewall behavior. This is especially true for SS2022, which lacks unified documentation and consistent implementation across projects. As a result, several subtle issues may arise during configuration and require careful troubleshooting.</p> <p>In contrast, reusing a local proxy through SSH reverse port forwarding serves as a fast, low-overhead, and almost configuration-free alternative. Its main advantage is that it works immediately and is not affected by the cloud provider’s network policies. The drawback, of course, is that it depends on the local machine being online, making it unsuitable as a long-term infrastructure solution.</p> <p>Based on this distinction: for time-sensitive situations, such as urgently needing to download model weights from HuggingFace, reusing a local proxy is extremely convenient. But for long-term project environments, if the goal is to maintain an autonomous and stable outbound capability on the server, deploying a proper proxy service on the server remains the recommended final solution.</p>]]></content><author><name></name></author><category term="Networking"/><category term="Proxy"/><category term="SSH"/><category term="DevOps"/><summary type="html"><![CDATA[A practical record of troubleshooting outbound network restrictions on Chinese cloud servers and enabling stable access to foreign academic resources.]]></summary></entry></feed>