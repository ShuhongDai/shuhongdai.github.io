<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shuhongdai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shuhongdai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-28T10:23:27+00:00</updated><id>https://shuhongdai.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Quick Mental Model for Estimating LLM GPU Memory Use</title><link href="https://shuhongdai.github.io/blog/2025/A_Quick_Mental_Model_for_Estimating_LLM_GPU_Memory_Use/" rel="alternate" type="text/html" title="A Quick Mental Model for Estimating LLM GPU Memory Use"/><published>2025-11-17T01:00:00+00:00</published><updated>2025-11-17T01:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/A_Quick_Mental_Model_for_Estimating_LLM_GPU_Memory_Use</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/A_Quick_Mental_Model_for_Estimating_LLM_GPU_Memory_Use/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>When browsing new open-source LLM releases, I often have a simple question in mind:<br/> <strong>Will this model actually fit on my GPU?</strong></p> <p>Sometimes the model page shows numbers like \(7\text{B}\), \(14\text{B}\), or \(70\text{B}\), but that alone doesn’t immediately translate into how much memory the model needs once loaded and running. And when I only want a quick sanity check, I don’t want to:</p> <ul> <li>download tens of gigabytes of weights,</li> <li>install a full environment,</li> <li>start a runtime,</li> <li>and only then discover that the model does not fit on the device at all.</li> </ul> <p>For this kind of lightweight judgment, a rough mental model is far more helpful than an exact calculator. It doesn’t need to be accurate to the megabyte. It only needs to answer a practical question: <strong>“Roughly fits?” or “Clearly too large?”</strong></p> <p>This note summarizes the approximation that I use. It’s not a formal derivation. It’s simply a way to reason about LLM memory requirements quickly, in a way that works consistently across models.</p> <hr/> <h2 id="what-actually-occupies-gpu-memory">What Actually Occupies GPU Memory</h2> <p>For inference (not training), only a few components meaningfully consume GPU memory:</p> <ol> <li>The model weights</li> <li>The key–value cache used during autoregressive generation</li> <li>Runtime overhead: intermediate buffers, framework allocations, small activations</li> </ol> <p>Optimizer state does not exist during inference, so the overall picture is simpler than training.</p> <p>My routine is just:</p> <ul> <li>estimate weight memory,</li> <li>add the KV cache,</li> <li>apply a small safety margin.</li> </ul> <hr/> <h2 id="from-parameters-to-vram">From Parameters to VRAM</h2> <p>Model parameter counts are usually prominently displayed: \(7\text{B}\), \(13\text{B}\), \(34\text{B}\), \(70\text{B}\), and so on. Converting this into VRAM is straightforward once we remember how many bytes each parameter uses.</p> <p>Typical cases:</p> <ul> <li><strong>FP16 / BF16:</strong> \(2\) bytes per parameter</li> <li><strong>FP32:</strong> \(4\) bytes</li> <li><strong>8-bit quantization:</strong> about \(1\) byte</li> <li><strong>4-bit quantization:</strong> about \(0.5\) byte</li> </ul> <p>This is already enough for fast estimation:</p> <ul> <li><strong>7B FP16 model</strong> → roughly \(7 \times 2\) GB ≈ <strong>14 GB</strong></li> <li><strong>13B FP16 model</strong> → \(13 \times 2\) GB ≈ <strong>26 GB</strong></li> <li><strong>7B 4-bit model</strong> → \(7 \times 0.5\) GB ≈ <strong>3.5 GB</strong></li> </ul> <p>This accounts only for the parameters, not the KV cache.</p> <hr/> <h2 id="kv-cache-and-context-length">KV Cache and Context Length</h2> <p>During generation, transformer decoders maintain a key–value cache for each attention layer. Its size scales with:</p> <ul> <li>number of layers,</li> <li>hidden dimension,</li> <li>context length.</li> </ul> <p>The exact computation is more detailed, but for estimation purposes, only the magnitude matters. In practice: <strong>KV cache often contributes hundreds of megabytes to several gigabytes</strong>, depending on the context window.</p> <p>For many modern \(6\text{B}\)–\(8\text{B}\)-scale models in FP16:</p> <ul> <li>every \(1\text{k}\) tokens of context usually costs <strong>a few hundred MB</strong> of KV cache</li> <li>a \(4\text{k}\)–\(8\text{k}\) context easily adds <strong>1–3 GB</strong></li> <li>long-context models might require more</li> </ul> <p>This rough rule-of-thumb is accurate enough to determine whether a model with a particular context window fits on an average \(24\text{GB}\)–\(48\text{GB}\) GPU.</p> <hr/> <h2 id="putting-the-pieces-together">Putting the Pieces Together</h2> <p>Once the weight size and KV cache are roughly known, the total memory is just the sum plus a safety margin. In practice, I use a simple heuristic.</p> <p>First, compute an approximate parameter memory from:</p> \[\text{ParameterMemory} \approx \text{NumberOfParameters} \times \text{BytesPerParameter}\] <p>Then add room for the KV cache, based on the context length. Finally, leave some headroom to account for framework overhead and fragmentation.</p> <p>If I want to compress this into a single sentence I can recall mentally, it would be: Take the parameter size, add a few gigabytes for KV cache, add a buffer, and that’s your practical VRAM requirement.</p> <hr/> <h2 id="additional-notes">Additional Notes</h2> <p>A few architectural details can influence memory usage in practice, even when using the simple estimation model above:</p> <ol> <li> <p><strong>KV cache usage varies across architectures</strong> Hidden sizes and layer counts differ between models. For example, even at similar 7B scales, newer efficient architectures such as <strong>Qwen2.5</strong> and <strong>Mistral</strong> typically use less KV memory per 1k tokens than earlier LLaMA-style models, and smaller models require even less. The “hundreds of MB per 1k tokens” rule still holds, but the exact amount can vary.</p> </li> <li> <p><strong>A small amount of extra weight tensors exists</strong> Beyond the main linear weights, models include embeddings, LayerNorm parameters, and other small tensors. These usually contribute under 5% of the total size, so the rough estimate remains valid, but the actual VRAM will be slightly higher than the simple parameter × bytes calculation.</p> </li> <li> <p><strong>Quantized models may include additional metadata</strong> Although 4-bit and 8-bit quantization can be estimated as 0.5 or 1 byte per parameter, many implementations store per-channel scales, zero-points, or other auxiliary data. This means the practical VRAM usage is often somewhat higher than the theoretical minimum.</p> </li> </ol> <p>These nuances don’t affect the overall direction of the estimate but matter when pushing GPU limits or optimizing for tight VRAM constraints.</p>]]></content><author><name></name></author><category term="LLMs"/><category term="GPU Memory"/><summary type="html"><![CDATA[Before downloading a large model or spinning up a container, it’s useful to know whether an open-source LLM will actually fit on your GPU.]]></summary></entry><entry><title type="html">Designing a Maintainable Replay Buffer in RL Systems</title><link href="https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_RL_Systems/" rel="alternate" type="text/html" title="Designing a Maintainable Replay Buffer in RL Systems"/><published>2025-10-21T12:31:00+00:00</published><updated>2025-10-21T12:31:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_RL_Systems</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Designing_a_Maintainable_Replay_Buffer_in_RL_Systems/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In many RL implementations, the replay buffer is introduced as a small but necessary tool, something that stores transitions and hands out mini-batches without attracting much attention. Early tutorials often depict it as a simple queue with random access, as if its role were little more than bookkeeping. Early tutorials often depict it as a simple queue with random access, as if its role were little more than bookkeeping.</p> <p>Once we move beyond toy setups, however, its character changes. As algorithms broaden and experiments run for weeks rather than minutes, the replay buffer shifts from a background utility to a structural anchor. I’ve found that its design quietly shapes not only training stability but also how intelligible and modifiable the surrounding codebase becomes. A system that invites experimentation usually reveals a buffer that has been treated with more care than the introductory treatments suggest.</p> <p>In this piece, I aim to reflect on replay buffer design from a pragmatic engineering perspective: what purposes the buffer ultimately serves, which structural choices tend to hold up under growing demands, and how certain principles help prevent chronic headaches in expanding projects.</p> <hr/> <h2 id="the-replay-buffer-as-a-dataflow-node">The Replay Buffer as a Dataflow Node</h2> <p>Although people often describe a replay buffer as mere “storage,” that framing is somewhat misleading. In practice, it acts as a dataflow node that sits between several competing processes:</p> <ul> <li>the environment generating new experience,</li> <li>the update loop that consumes it,</li> <li>auxiliary components such as logging or evaluation,</li> <li>and, quite often, reproducibility mechanisms lurking in the background.</li> </ul> <p>Thinking of the buffer as part of a larger dataflow clarifies its real function. It does not simply hold transitions; it mediates consistency, shapes the boundaries between modules, and perhaps unexpectedly affects how naturally an RL system can adapt as research directions evolve.</p> <p>Even as RL algorithms diverge (off-policy methods pushing one way, on-policy methods with auxiliary replay pulling another, offline RL taking on a dataset-like shape), the replay buffer’s underlying demands remain remarkably stable. It must provide order, predictability, and a coherent interface across shifting algorithmic choices.</p> <hr/> <h2 id="common-replay-buffer-structures-in-existing-systems">Common Replay Buffer Structures in Existing Systems</h2> <p>Looking across existing RL frameworks, certain structural patterns recur. I often see:</p> <ul> <li><strong>Buffers built on plain Python lists</strong>, favoring immediacy and minimalism.</li> <li><strong>Dictionary-style buffers</strong>, which trade a bit of tidiness for flexibility.</li> <li><strong>Preallocated NumPy (or similar) arrays</strong>, chosen when throughput and determinism matter.</li> <li><strong>Dataset-like implementations</strong>, particularly in offline RL or large frameworks such as RLlib.</li> </ul> <p>Each approach reflects a particular priority: early prototyping, customizable fields, dependable performance, or alignment with dataset tooling. The point is not that any one of them is categorically better; rather, they occupy distinct locations in the design space. Seeing this variety helps clarify the structural constraints a more durable design must address.</p> <hr/> <h2 id="why-maintainability-matters">Why Maintainability Matters</h2> <p>The replay buffer touches nearly everything in an RL pipeline:</p> <ul> <li>environment interaction,</li> <li>training procedures,</li> <li>logging and metric systems,</li> <li>sampling mechanisms,</li> <li>and occasionally distributed actors.</li> </ul> <p>Because of this centrality, small inconsistencies (e.g., shape mismatches, implicit assumptions, overly coupled fields) tend to propagate widely. A design that works for a narrow experiment may later resist extensions such as:</p> <ul> <li>Aadding fields like log-probs or auxiliary targets,</li> <li>adapting to environments that return richer info dictionaries,</li> <li>distinguishing truncation from true termination,</li> <li>or introducing prioritized or sequence-based sampling.</li> </ul> <p>Maintainability, therefore, is less about making the implementation “clean” and more about preserving structural integrity under change.</p> <hr/> <h2 id="design-principles-for-a-maintainable-replay-buffer">Design Principles for a Maintainable Replay Buffer</h2> <p>Supporting a wide spectrum of algorithms and experimental demands rarely requires intricate machinery. More often, it calls for a handful of straightforward design choices that reinforce stability and reduce conceptual friction.</p> <h3 id="1-a-clear-and-explicit-data-schema"><strong>1. A Clear and Explicit Data Schema</strong></h3> <p>Each field, such as observations, actions, rewards, next observations, and the various termination indicators, should be represented explicitly. Attempts to infer structure implicitly usually collapse when a new algorithm introduces an extra field or modifies an existing one.</p> <p>A well-defined schema states:</p> <ul> <li>what each field contains,</li> <li>its shape,</li> <li>its dtype,</li> <li>and the rules governing when it is written.</li> </ul> <p>This explicitness avoids interpretative ambiguity later, especially during sampling.</p> <hr/> <h3 id="2-independence-between-fields"><strong>2. Independence Between Fields</strong></h3> <p>Transitions need not be stored as monolithic tuples. In fact, isolating fields into separate arrays or storage units tends to make systems easier to test, reason about, and extend. It also simplifies batch indexing and accommodates experimental additions without unintended consequences.</p> <p>By decoupling fields, you reduce the chance of cascading side effects, it is an issue I’ve run into often when experimenting with additional annotations or metadata.</p> <hr/> <h3 id="3-preallocation-with-predictable-behavior"><strong>3. Preallocation with Predictable Behavior</strong></h3> <p>A fixed-size ring buffer backed by preallocated arrays typically offers the most stable behavior. It avoids issues such as:</p> <ul> <li>unpredictable memory growth,</li> <li>fragmentation,</li> <li>and costly resizes.</li> </ul> <p>A simple write pointer and a size counter usually suffice. In my experience, predictability is worth far more than cleverness here.</p> <hr/> <h3 id="4-decoupled-sampling-logic"><strong>4. Decoupled Sampling Logic</strong></h3> <p>Sampling tends to evolve quickly in RL research. Keeping sampling separate from storage makes it far easier to test new possibilities:</p> <ul> <li>uniform sampling,</li> <li>stratification,</li> <li>prioritized replay,</li> <li>sequence extraction for RNNs,</li> <li>long-horizon temporal sampling.</li> </ul> <p>When storage imposes no constraints on sampling, algorithmic exploration becomes far more straightforward.</p> <hr/> <h3 id="5-stable-batch-shapes-and-typing"><strong>5. Stable Batch Shapes and Typing</strong></h3> <p>A surprising number of bugs originate from shape inconsistencies. Ensuring that shapes and dtypes are fixed when the buffer initializes, and validating them whenever data are written, helps guarantee stable tensors for training routines, predictable input formats for models, and early detection of environment misconfigurations. This holds across vector observations, mixed modalities, discrete or continuous actions, and even more specialized forms of data.</p> <hr/> <h2 id="a-practical-replay-buffer-structure">A Practical Replay Buffer Structure</h2> <p>A maintainable replay buffer often adopts a design similar to the following conceptual structure:</p> <ul> <li>a defined <strong>capacity</strong>,</li> <li>a <strong>write index</strong> indicating the next insertion point,</li> <li>a <strong>current size</strong> reflecting valid data,</li> <li>a dictionary of <strong>fields</strong>, each holding a preallocated array,</li> <li>and a sampling interface that accepts indices and returns assembled batches.</li> </ul> <p>Such a structure supports smooth extensions (new fields slot naturally into place), stable sampling (driven entirely by batch indices), and the option to modify or replace sampling strategies without disturbing storage. It works for both on-policy and off-policy settings, provided the surrounding logic is appropriate.</p> <p>The precise API matters less than the emphasis on clarity, decoupling, and predictable behavior.</p> <hr/> <h2 id="memory-and-performance-considerations">Memory and Performance Considerations</h2> <p>Replay buffers sometimes operate close to memory limits, since long-horizon tasks or high-frequency transitions can generate substantial load. Sensible engineering choices include:</p> <ul> <li>selecting appropriate dtypes (e.g., <code class="language-plaintext highlighter-rouge">float32</code> unless higher precision is essential),</li> <li>trimming or compressing non-essential fields,</li> <li>keeping arrays contiguous to reduce overhead,</li> <li>and, in distributed scenarios, deciding carefully where storage resides.</li> </ul> <p>Performance is certainly relevant, but for most research-level systems, I find that clarity and invariants tend to matter more. Once those are in place, optimizing hotspots becomes easier and safer.</p> <hr/> <h2 id="the-role-of-replay-buffers-in-larger-rl-architectures">The Role of Replay Buffers in Larger RL Architectures</h2> <p>As RL systems scale, the replay buffer assumes different personas:</p> <ul> <li>In <strong>off-policy RL</strong>, it stabilizes learning by shaping the distribution of samples.</li> <li>In <strong>offline RL</strong>, it effectively is the dataset interface.</li> <li>In <strong>model-based RL</strong>, it may hold both real and generated transitions side by side.</li> <li>In <strong>multi-agent RL</strong>, it often mediates data across agents or environments.</li> <li>In <strong>distributed RL</strong>, it can serve as a central data service or coordination layer.</li> </ul> <p>A well-designed buffer tends to move across these contexts with little structural modification, which is a strong indicator that its design principles are sound.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>The replay buffer, though rarely celebrated, is one of the key infrastructural elements in reinforcement learning systems. Its design shapes how reliably the rest of the pipeline behaves and how easily new ideas can be integrated. A durable buffer is grounded in a few guiding practices such as explicit schemas, independent fields, predictable mechanics, and sampling logic that remains clearly separated from storage. When these practices are in place, the buffer becomes a stable foundation rather than a recurring point of fragility.</p>]]></content><author><name></name></author><category term="RL"/><category term="System Design"/><category term="Data Structures"/><summary type="html"><![CDATA[A structured and engineering-focused reflection on replay buffer design in RL, emphasizing clarity, extensibility, and long-term maintainability.]]></summary></entry><entry><title type="html">Tracing the Root Cause of Missing GPUs in Docker Containers</title><link href="https://shuhongdai.github.io/blog/2025/Tracing_the_Root_Cause_of_Missing_GPUs_in_Docker_Containers/" rel="alternate" type="text/html" title="Tracing the Root Cause of Missing GPUs in Docker Containers"/><published>2025-08-20T01:02:00+00:00</published><updated>2025-08-20T01:02:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Tracing_the_Root_Cause_of_Missing_GPUs_in_Docker_Containers</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Tracing_the_Root_Cause_of_Missing_GPUs_in_Docker_Containers/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The GPU on my host machine worked flawlessly. <code class="language-plaintext highlighter-rouge">nvidia-smi</code> showed four cleanly indexed devices. CUDA tests ran without complaint. Nothing looked suspicious. But inside Docker, those same GPUs simply vanished. The container insisted it had no GPU at all, even when launched with <code class="language-plaintext highlighter-rouge">--gpus all</code>. This was not an application-level issue. Problems like this never come from the code running inside the container. They come from a mismatch somewhere between Docker, the NVIDIA runtime, and the host’s driver stack. This post is a reconstruction of how I verified that assumption and worked my way through the layers until the system finally admitted what was wrong.</p> <hr/> <h2 id="the-initial-symptom">The Initial Symptom</h2> <p>The first sign of trouble came from a simple test:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">--gpus</span> all ubuntu:22.04 nvidia-smi
</code></pre></div></div> <p>The output didn’t show a GPU. It didn’t even show a driver mismatch. Instead I received:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.
</code></pre></div></div> <p>This message is always unhelpful. It can mean anything from missing libraries to container runtime failures to a completely broken driver. But since <code class="language-plaintext highlighter-rouge">nvidia-smi</code> worked perfectly on the host, the problem had to be elsewhere.</p> <p>I checked the host first, just to be certain.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-smi
Sun Aug 17 00:15:19 2025
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 550.54       Driver Version: 550.54       CUDA Version: 12.4     |
| GPU Name        Persistence-M| Bus-Id ... <span class="o">(</span>normal output<span class="o">)</span>                   |
+-----------------------------------------------------------------------------+
</code></pre></div></div> <p>Everything here was healthy. That told me the problem was not hardware. It also told me the Docker container was not receiving the correct runtime environment.</p> <hr/> <h2 id="a-quick-look-at-the-nvidia-container-toolkit">A Quick Look at the NVIDIA Container Toolkit</h2> <p>The next step was confirming that the NVIDIA container runtime existed on the host.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dpkg <span class="nt">-l</span> | <span class="nb">grep</span> <span class="nt">-i</span> nvidia-container
ii  nvidia-container-toolkit  1.16.2-1  amd64
</code></pre></div></div> <p>The toolkit was installed, at least according to the package manager. But “installed” is not the same as “integrated.” I checked Docker’s runtime configuration:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /etc/docker/daemon.json
<span class="o">{</span>
    <span class="s2">"runtimes"</span>: <span class="o">{</span>
        <span class="s2">"nvidia"</span>: <span class="o">{</span>
            <span class="s2">"path"</span>: <span class="s2">"nvidia-container-runtime"</span>,
            <span class="s2">"runtimeArgs"</span>: <span class="o">[]</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div> <p>At first glance this looked reasonable but Docker’s configuration files often lie by omission. I restarted Docker anyway, hoping for the rare case where a restart solves a real problem.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>systemctl restart docker
</code></pre></div></div> <p>It changed nothing.</p> <hr/> <h2 id="the-runtime-that-didnt-exist">The Runtime That Didn’t Exist</h2> <p>I tried manually invoking the runtime:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>which nvidia-container-runtime
/usr/bin/nvidia-container-runtime
</code></pre></div></div> <p>It was present. But inside Docker, the container still couldn’t see GPUs. I suspected mismatch in library paths, so I inspected the toolkit’s log:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>journalctl <span class="nt">-u</span> nvidia-container-runtime
</code></pre></div></div> <p>The log was silent with no errors and no warnings, which is often more suspicious than a screaming log file.</p> <hr/> <h2 id="trying-a-minimal-container">Trying a Minimal Container</h2> <p>Sometimes it helps to remove everything unrelated. I tried an empty Alpine container with explicit runtime:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--runtime</span><span class="o">=</span>nvidia <span class="nt">--rm</span> nvidia/cuda:12.4-base nvidia-smi
</code></pre></div></div> <p>The result was the same:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.
</code></pre></div></div> <p>At this point I was certain the issue wasn’t in the image. The problem was in the host-to-container plumbing.</p> <hr/> <h2 id="nvidia-container-cli">nvidia-container-cli</h2> <p>I checked low-level diagnostics:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-container-cli info
</code></pre></div></div> <p>This should list driver version, devices, and capabilities. Instead it printed:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ERRO[0000] could not load NVML: libnvidia-ml.so.1: cannot open shared object file: No such file or directory
</code></pre></div></div> <p>The NVML library is part of the NVIDIA driver. If the toolkit couldn’t load it, that meant the toolkit’s library search paths did not match the actual driver installation. Which usually happens after a driver upgrade that leaves symlinks pointing to the wrong place.</p> <p>I checked the library:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> /usr/lib/x86_64-linux-gnu/libnvidia-ml.so<span class="k">*</span>
</code></pre></div></div> <p>The files were indeed present, but I noticed they were under:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/lib/x86_64-linux-gnu/nvidia/current/
</code></pre></div></div> <p>while the toolkit expected:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/lib/x86_64-linux-gnu/
</code></pre></div></div> <p>This mismatch is easy to miss.</p> <hr/> <h2 id="the-missing-symlink">The Missing Symlink</h2> <p>On many systems, <code class="language-plaintext highlighter-rouge">/usr/lib/x86_64-linux-gnu/</code> should contain symlinks to the actual driver libraries, but mine didn’t. I created the symlink manually:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo ln</span> <span class="nt">-s</span> /usr/lib/x86_64-linux-gnu/nvidia/current/libnvidia-ml.so.1 <span class="se">\</span>
             /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
</code></pre></div></div> <p>Then I tried the diagnostic again:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-container-cli info
</code></pre></div></div> <p>This time it printed a full report with devices, drivers, and capabilities listed correctly. That told me NVIDIA’s container toolkit could finally see the GPU.</p> <hr/> <h2 id="the-final-test">The Final Test</h2> <p>With everything aligned, I launched a fresh container:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">--gpus</span> all nvidia/cuda:12.4-base nvidia-smi
</code></pre></div></div> <p>The output looked normal.</p>]]></content><author><name></name></author><category term="Docker"/><category term="NVIDIA"/><category term="CUDA"/><summary type="html"><![CDATA[A debugging record of why Docker refused to expose GPUs inside a container even though the host recognized them perfectly, and how every layer of the system contributed a small piece to the failure.]]></summary></entry><entry><title type="html">Running dm-control on a Headless Server: A Complete Debugging Log</title><link href="https://shuhongdai.github.io/blog/2025/Running-_dm-control-_on_a_Headless_Server/" rel="alternate" type="text/html" title="Running dm-control on a Headless Server: A Complete Debugging Log"/><published>2025-06-15T21:12:00+00:00</published><updated>2025-06-15T21:12:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Running%20_dm-control%20_on_a_Headless_Server</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Running-_dm-control-_on_a_Headless_Server/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post documents the process of configuring <strong>dm-control</strong> on a <strong>headless Ubuntu server</strong> for reinforcement learning experiments that require pixel-based observations. The goal was straightforward: load a task from the DeepMind Control Suite and render frames for a CNN-based policy. However, running dm-control without a graphical display consistently triggered a series of rendering failures.</p> <p>Rather than providing a tutorial, this article records the issues encountered, the different directions explored, and the final configuration that proved reliable. Hopefully, this log will help anyone attempting to run dm-control in a similar environment.</p> <hr/> <h2 id="initial-attempts-and-rendering-errors">Initial Attempts and Rendering Errors</h2> <p>The starting point was simply loading a task and calling <code class="language-plaintext highlighter-rouge">env.physics.render()</code>. On a local machine, this works immediately. On a headless server, the first result was an error referencing <strong>EGL</strong> and <strong>OpenGL</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
AttributeError: 'NoneType' object has no attribute 'eglQueryString'

</code></pre></div></div> <p>Further attempts produced warnings about missing <code class="language-plaintext highlighter-rouge">DISPLAY</code> variables:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
X11: The DISPLAY environment variable is missing

</code></pre></div></div> <p>These messages made it clear that dm-control was attempting to initialize rendering through OpenGL/EGL, both of which require a graphics context the server did not provide. Disabling the <code class="language-plaintext highlighter-rouge">DISPLAY</code> variable or forcing EGL did not resolve the issue; the underlying environment simply lacked the dependencies required for hardware-accelerated rendering.</p> <hr/> <h2 id="investigating-the-role-of-pyopengl">Investigating the Role of PyOpenGL</h2> <p>The repeated mentions of EGL led to checking whether PyOpenGL played a role. Removing PyOpenGL temporarily modified the error messages, but it was automatically reinstalled when upgrading or reinstalling dm-control and Mujoco. This indicated that avoiding the OpenGL backend entirely was not feasible through uninstalling PyOpenGL alone.</p> <hr/> <h2 id="version-mismatch-between-dm-control-and-mujoco">Version Mismatch Between dm-control and Mujoco</h2> <p>At a later stage, entirely different errors appeared:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
eq_active not found
flex_xvert0 not found

</code></pre></div></div> <p>These errors are characteristic of <strong>Mujoco internal structure mismatches</strong>, suggesting that the installed versions of Mujoco and dm-control were not aligned. dm-control indexes Mujoco model structures by field name, and when they do not match, initialization fails even before rendering begins.</p> <p>This confirmed that two separate issues existed:</p> <ol> <li>Rendering backend not compatible with the server</li> <li>dm-control and Mujoco versions not compatible with each other</li> </ol> <p>Both needed to be addressed.</p> <hr/> <h2 id="moving-toward-a-software-rendering-approach">Moving Toward a Software Rendering Approach</h2> <p>Given that neither EGL nor OpenGL would work reliably on the server, the next candidate was <strong>OSMesa</strong>, a pure software renderer. OSMesa performs all rendering on the CPU and does not require a graphical display or GPU drivers. This makes it suitable for cloud or containerized environments.</p> <p>Ubuntu provides OSMesa through:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
libosmesa6-dev

</code></pre></div></div> <p>The key environment variable for Mujoco is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
MUJOCO_GL=osmesa

</code></pre></div></div> <p>With this backend, Mujoco no longer attempts to initialize hardware-accelerated contexts.</p> <hr/> <h2 id="identifying-a-stable-mujoco--dm-control-combination">Identifying a Stable Mujoco + dm-control Combination</h2> <p>After testing multiple combinations, the following versions proved to be both compatible with each other and functional under OSMesa:</p> <ul> <li><strong>mujoco 2.3.3</strong></li> <li><strong>dm-control 1.0.11</strong></li> <li>Python 3.9</li> </ul> <p>This combination avoids the structure-indexing errors seen in newer pairings and also avoids invoking backends requiring OpenGL or EGL by default.</p> <hr/> <h2 id="verifying-the-configuration">Verifying the Configuration</h2> <p>With OSMesa enabled and the compatible versions installed, the following minimal script successfully produced a rendered frame:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">MUJOCO_GL</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">osmesa</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">DISPLAY</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">""</span>

<span class="kn">from</span> <span class="n">dm_control</span> <span class="kn">import</span> <span class="n">suite</span>
<span class="kn">import</span> <span class="n">imageio</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">suite</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">cartpole</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">swingup</span><span class="sh">"</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">physics</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">camera_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Frame shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">imageio</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="sh">"</span><span class="s">soft_render.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
</code></pre></div></div> <p>The output confirmed that rendering worked:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Frame shape: (128, 128, 3)
Saved soft_render.png
</code></pre></div></div> <p>A few non-critical messages appear during interpreter shutdown, related to thread cleanup inside dm-control’s internal executor. These do not affect functionality.</p> <hr/> <h2 id="final-working-setup">Final Working Setup</h2> <p>Summarizing the configuration that worked consistently:</p> <ul> <li><strong>Python</strong>: 3.9</li> <li><strong>Mujoco</strong>: 2.3.3</li> <li><strong>dm-control</strong>: 1.0.11</li> <li><strong>System package</strong>: <code class="language-plaintext highlighter-rouge">libosmesa6-dev</code></li> <li><strong>Rendering backend</strong>: <code class="language-plaintext highlighter-rouge">MUJOCO_GL=osmesa</code></li> </ul> <p>This setup uses CPU-based software rendering, avoids any dependency on GPU drivers or graphical displays, and is compatible with reinforcement-learning algorithms requiring pixel observations.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Running dm-control on a headless server involves more than installing the library. Rendering backends, environment variables, and version compatibility all play important roles in determining whether Mujoco initializes correctly. After exploring several unsuccessful paths, the combination of <strong>OSMesa rendering</strong> and a <strong>compatible pair of dm-control and Mujoco versions</strong> proved to be a reliable solution.</p> <p>By documenting this process, this post aims to provide a clear reference for others setting up dm-control in a similar environment.</p>]]></content><author><name></name></author><category term="Reinforcement Learning"/><category term="Mujoco"/><category term="dm-control"/><category term="Rendering"/><summary type="html"><![CDATA[A practical record of configuring dm-control with Mujoco on a headless Ubuntu server, covering rendering failures, version mismatches, and the final workable setup.]]></summary></entry><entry><title type="html">Why SUMO’s Rendered Videos Should Never Be Used as RL Training Data</title><link href="https://shuhongdai.github.io/blog/2025/Why_SUMO-s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data/" rel="alternate" type="text/html" title="Why SUMO’s Rendered Videos Should Never Be Used as RL Training Data"/><published>2025-01-27T23:40:00+00:00</published><updated>2025-01-27T23:40:00+00:00</updated><id>https://shuhongdai.github.io/blog/2025/Why_SUMO%E2%80%99s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2025/Why_SUMO-s_Rendered_Videos_Should_Never_Be_Used_as_RL_Training_Data/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The rendered images or videos generated by SUMO have no place in a RL pipeline. They may look clear, structured, and almost “dataset-like,” yet they fundamentally lack the properties that make a visual observation meaningful to an agent. What they provide is appearance, not information.</p> <p>This is not an insight I arrived at after trying to extract learning signals from SUMO’s frames. Rather, it becomes clear the moment one understands how SUMO represents its world internally. The simulator was never built around vision; it was built around structure—precise, explicit, symbolic structure. And because of this, its video output occupies a different conceptual layer entirely.</p> <hr/> <h2 id="visualization-hides-semantics">Visualization Hides Semantics</h2> <p>The rendered frame that SUMO shows us is, in essence, a human-friendly sketch of a world that exists somewhere else. The world SUMO actually simulates consists of continuous lane coordinates, exact positions and velocities, acceleration profiles, signal states, right-of-way relationships, and route intentions that evolve according to a coherent traffic model. This internal world is numerical and symbolic down to its core.</p> <p>None of this structure survives the trip into the visual renderer. A car in a SUMO frame is just a colored rectangle, detached from the lane graph it belongs to, stripped of its intentions and priorities, reduced to a geometry that no longer reveals why it will or will not proceed through an intersection. A traffic signal becomes a green or red dot with no connection to the control logic that governs it. Even spatial quantities, such as gaps or queue lengths, degrade into visual approximations that depend on camera perspective rather than on the simulator’s own measurements. It is tempting to think that pixels “contain enough information,” but the truth is that the renderer deliberately hides most of what matters.</p> <hr/> <h2 id="what-rl-actually-needs">What RL Actually Needs</h2> <p>RL does not learn from appearance, it learns from state. And a useful state is one that preserves the causal relationships driving the environment. With SUMO, those relationships live entirely in its internal representation: the topology of the road network, the numerical values describing motion, the logic that governs priority and right-of-way, the exact timing and sequencing of traffic lights. These are not afterthoughts; they are the environment.</p> <p>A video frame, regardless of how cleanly rendered, cannot express the rules that determine how traffic flows. It does not encode which vehicle is yielding, which is accelerating, which is constrained by downstream occupancy, or which is simply following a route that is invisible to the eye. Observing the render is, at best, witnessing the shadow of a system whose logic has already been stripped away.</p> <p>Training an RL agent on such shadows is not simply inefficient; it is conceptually misaligned. The agent is forced to reconstruct the environment’s logic from incomplete projections of it, even though SUMO already provides the fully-formed state in precise numerical terms. The agent ends up solving a perception problem that exists only because we discarded the very information the simulator was designed to offer.</p> <hr/> <h2 id="reconstructing-what-the-simulator-already-knows">Reconstructing What the Simulator Already Knows</h2> <p>If one insists on using SUMO’s images as input, the learning problem becomes unnecessarily inverted. To make sense of a frame, an agent must infer positions, velocities, lanes, intentions, and interactions—all of which SUMO already calculates. This re-derivation is not only redundant; it is structurally impossible to perform without error, because the necessary information does not exist in the pixels in the first place. SUMO’s renderer never intended to expose it.</p> <p>Vision makes sense when there is no alternative, such as in real-world driving where sensors are noisy and perception is inherently uncertain. But SUMO is not an uncertain environment. It is fully observable. Every variable that matters is crisp, deterministic, and accessible. To replace this with pixels is to voluntarily abandon the clarity that simulation exists to provide.</p> <p>It is important to remember why SUMO has visualization at all. The renderer exists so that humans can watch the simulation unfold. It is a debugging tool, a qualitative sanity check, a way to illustrate traffic scenarios for presentations and reports. The rendering is not a sensory modality of the environment, and it was never meant to be. Its purpose is interpretive, not generative.</p> <p>If an agent were to rely on SUMO’s visual output, it would be relying on something designed explicitly for human interpretation. And human interpretation thrives on abstractions, simplifications, and aesthetic conventions. All of which run counter to what machine learning expects from observational data.</p> <hr/> <h2 id="comment">Comment</h2> <p>The visual output of SUMO does not fail because it is low quality. It fails because it is the wrong abstraction. It belongs to a layer of the system intended for people, not agents. The real substance of SUMO such as the logic, the topology, the numerics and the decisions is found in its internal state, not in its renderings.</p> <p>This is why SUMO’s images should never be used as RL input. Not because they are flawed, but because they are unrelated to the simulation’s meaning. To rely on them is to ignore the very nature of the environment we are trying to learn from.</p>]]></content><author><name></name></author><category term="Simulation"/><category term="Traffic"/><category term="RL"/><category term="SUMO"/><summary type="html"><![CDATA[A examination of why the visual output of SUMO. Despite being clean and intuitive, it cannot serve as learning data for RL agents, and why this limitation is inherent in how the simulator is built.]]></summary></entry><entry><title type="html">Re-running an RL Experiment and Getting a Different Answer</title><link href="https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer/" rel="alternate" type="text/html" title="Re-running an RL Experiment and Getting a Different Answer"/><published>2024-11-17T02:08:00+00:00</published><updated>2024-11-17T02:08:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Re-running_an_RL_Experiment_and_Getting_a_Different_Answer/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Not long ago, I tried to reproduce one of my RL experiments on a cloud server. The same code had run earlier on a local lab machine, and both hosts were equipped with <strong>NVIDIA RTX 4090 GPUs</strong>. The driver versions matched, the CUDA and PyTorch versions were identical, the environment dependencies mirrored each other, and every random seed was fixed. Under such conditions, the expectation was simple: the two training curves should overlap almost perfectly.</p> <p>But this time, they didn’t. For the first few thousand steps, everything behaved exactly as expected. The lines overlapped so closely that they were visually indistinguishable. As training continued, however, a slight divergence appeared—barely noticeable at first, then increasingly persistent. Eventually, the two runs settled into significantly different behaviors. All high-level variables were controlled, and yet the divergence persisted.It wasn’t dramatic, but it was unmistakable. And it prompted me to re-examine some of the more fragile aspects of RL systems that often go unnoticed.</p> <hr/> <h2 id="two-curves-that-should-have-been-one">Two Curves That Should Have Been One</h2> <p>The most striking part of this incident was how cleanly the divergence unfolded. During the early stage, the critic’s loss, the policy statistics, and the rewards from the environment aligned almost exactly between the two machines. The curves felt stable, even reassuring.</p> <p>Then the shift began. It wasn’t a sudden jump but a slow drift, like two lines that started parallel but eventually grew a small angle between them. Once the angle existed, the distance between the lines increased gradually and inevitably. What began as a tiny deviation eventually widened into a visible performance gap.</p> <p>This kind of “quiet drift” is rare in supervised learning, but painfully common in RL, where feedback loops amplify small differences.</p> <hr/> <h2 id="investigation">Investigation</h2> <p>I didn’t start by suspecting the GPUs. Instead, I reviewed the usual suspects in a calm, methodical way:</p> <p>Whether the environment returned identical resets, whether the model initialization and random number streams matched, whether the replay buffer might have desynchronized the sampling order, whether the logging system affected timing, and whether the training loop had implicit branches that could influence execution order.</p> <p>All these checks were quick to perform. Nothing at the framework or data-flow level explained the divergence. Which meant the issue had to be buried deeper than most RL bugs—deeper than Python, deeper than PyTorch, deeper than CUDA kernels invoked explicitly in code.</p> <hr/> <h2 id="clue">Clue</h2> <p>The earliest measurable drift appeared not in the policy’s actions, but in the critic’s value estimates. That itself was a clue. The critic is often the most numerically sensitive component in many RL algorithms, and its outputs feed directly into the policy update. If two identical systems begin to disagree, the critic is a natural place to look.</p> <p>The differences were tiny—barely outside floating-point noise—but detectable under scrutiny. In the early stage of training, those deviations did not affect behavior, but they were already present. And because the critic affects everything downstream, even a small mismatch is enough for a long feedback loop to magnify.</p> <hr/> <h2 id="a-subtle-difference-from-identical-hardware">A Subtle Difference From Identical Hardware</h2> <p>Eventually, the real cause became clear:<br/> two GPUs of the same model can still produce slightly different floating-point results.</p> <p>This sounds counterintuitive at first, but it’s neither rare nor mysterious. Several factors can produce such differences:</p> <ul> <li>Slight variations in how CUDA dispatches certain kernels</li> <li>Minor differences introduced by fused multiply–add behavior</li> <li>Driver-level optimizations that change arithmetic ordering</li> <li>Subtle kernel selection differences across installations</li> </ul> <p>The scale of these discrepancies is extremely small—on the order of the last few bits of a float. They are invisible in most workloads. In supervised learning, such noise is diluted by batching, averaging, and the absence of recurrent dependencies. In RL, however, the story is different. RL acts as an amplifier. A microscopic variance in a critic output can change a gradient, which changes a policy, which changes the data distribution, which changes the critic’s future inputs, and so on. Over tens of thousands of iterations, this accumulation can transform an imperceptible discrepancy into a meaningful behavioral difference.</p> <p>At the heart of this phenomenon is the recursive nature of RL training. The critic’s estimation errors influence the policy update. The policy influences the trajectory of states and rewards. These, in turn, influence how the critic is updated. The loop continues, step after step.</p> <p>This structure makes RL far more sensitive to numerical discrepancies than most other machine learning pipelines. A difference invisible at step 2,000 can become visible at step 50,000 simply because it is allowed to feed back into itself. This is not a bug; it is a property of RL.</p>]]></content><author><name></name></author><category term="Reinforcement Learning"/><category term="CUDA"/><category term="Numerical Stability"/><category term="Reproducibility"/><summary type="html"><![CDATA[A engineering reflection on why two RTX 4090 machines produced diverging RL curves despite identical code, seeds, and configurations. And what this reveals about RL’s numerical sensitivity.]]></summary></entry><entry><title type="html">Using Local v2rayN Proxy for Cloud Servers via SSH Reverse Tunnel</title><link href="https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel/" rel="alternate" type="text/html" title="Using Local v2rayN Proxy for Cloud Servers via SSH Reverse Tunnel"/><published>2024-02-02T00:38:10+00:00</published><updated>2024-02-02T00:38:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Using_Local_v2rayN_Proxy_for_Cloud_Servers_via_SSH_Reverse_Tunnel/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In mainland China, cloud servers frequently experience unstable or inconsistent access to foreign open-source and academic platforms. This is often not due to targeted restrictions on specific technical resources, but rather to conservative traffic filtering and risk-averse network configurations adopted by some service providers.</p> <p>As a result, essential platforms such as HuggingFace, GitHub, and PyPI may occasionally become unreachable or suffer from intermittent resets, even when the server is intended solely for standard machine learning or research workloads.</p> <p>This post documents the connectivity issues I encountered on a cloud server and the final stable solution using SSH reverse tunneling.</p> <hr/> <h2 id="attempt-1-running-a-proxy-directly-on-the-server">Attempt 1: Running a Proxy Directly on the Server</h2> <p>My first attempt was to deploy a proxy environment directly on the cloud server using sing-box or Xray, and to reuse my existing Shadowsocks 2022 (SS2022) nodes from my Windows machine.</p> <p>This approach ran into multiple problems:</p> <ul> <li>Different implementations of SS2022 use different field names</li> <li>The key format (base64) requirements vary by version</li> <li>Certain fields (e.g., <code class="language-plaintext highlighter-rouge">secondary</code>, <code class="language-plaintext highlighter-rouge">psk</code>) are not supported in older releases</li> <li>Some configurations pass syntax checks but fail during actual traffic forwarding</li> <li>sing-box support for SS2022 differs significantly among releases</li> </ul> <p>Even after repeated adjustments, the connection remained unstable or unusable.</p> <hr/> <h2 id="attempt-2-letting-the-server-reuse-my-local-v2rayn-proxy-successful">Attempt 2: Letting the Server Reuse My Local v2rayN Proxy (Successful)</h2> <p>Since my local Windows environment with v2rayN worked reliably, I attempted to make the cloud server reuse <strong>my desktop’s proxy</strong> via <strong>SSH reverse port forwarding</strong>.</p> <p>I established an SSH session from the cloud server:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-o</span> <span class="nv">ServerAliveInterval</span><span class="o">=</span>60 <span class="se">\</span>
    <span class="nt">-o</span> <span class="nv">ServerAliveCountMax</span><span class="o">=</span>3 <span class="se">\</span>
    <span class="nt">-R</span> 0.0.0.0:10808:127.0.0.1:10808 <span class="se">\</span>
    root@&lt;server-address&gt;
</code></pre></div></div> <p>This command:</p> <ul> <li>Opens <code class="language-plaintext highlighter-rouge">10808</code> on the cloud server</li> <li>Forwards all traffic from that port</li> <li>Back through the SSH tunnel to my Windows machine</li> <li>Where v2rayN listens on local port <code class="language-plaintext highlighter-rouge">10808</code></li> </ul> <p>Testing the proxy:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--socks5</span> 127.0.0.1:10808 https://ipinfo.io/ip
</code></pre></div></div> <p>The returned IP was exactly the exit node of my v2rayN setup, confirming that the cloud server was successfully using my local proxy.</p> <hr/> <h2 id="python-configuration">Python Configuration</h2> <p>Once <code class="language-plaintext highlighter-rouge">curl</code> worked, I needed Python (especially <code class="language-plaintext highlighter-rouge">requests</code> and HuggingFace Hub) to use SOCKS5.</p> <p>Python does not support SOCKS by default, so I installed:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pysocks
</code></pre></div></div> <p>Then set the environment variables:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">http_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
<span class="nb">export </span><span class="nv">https_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
<span class="nb">export </span><span class="nv">all_proxy</span><span class="o">=</span>socks5h://127.0.0.1:10808
</code></pre></div></div> <p>After this, both <code class="language-plaintext highlighter-rouge">requests.get()</code> and HuggingFace model downloads worked correctly.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>It is important to note that deploying a proxy service directly on the server is not only feasible but is, in fact, the more standard, professional, and long-term maintainable approach. Whether using sing-box, Xray, Hysteria, or Tuic, one can build a fully independent outbound capability on the cloud server, which aligns better with the engineering practice of “managing your own network boundary.”</p> <p>However, this approach typically involves multiple layers of complexity: protocol specifications, key formats, server–client version compatibility, differences in supported cipher suites, and firewall behavior. This is especially true for SS2022, which lacks unified documentation and consistent implementation across projects. As a result, several subtle issues may arise during configuration and require careful troubleshooting.</p> <p>In contrast, reusing a local proxy through SSH reverse port forwarding serves as a fast, low-overhead, and almost configuration-free alternative. Its main advantage is that it works immediately and is not affected by the cloud provider’s network policies. The drawback, of course, is that it depends on the local machine being online, making it unsuitable as a long-term infrastructure solution.</p> <p>Based on this distinction: for time-sensitive situations, such as urgently needing to download model weights from HuggingFace, reusing a local proxy is extremely convenient. But for long-term project environments, if the goal is to maintain an autonomous and stable outbound capability on the server, deploying a proper proxy service on the server remains the recommended final solution.</p>]]></content><author><name></name></author><category term="Networking"/><category term="Proxy"/><category term="SSH"/><category term="DevOps"/><summary type="html"><![CDATA[A practical record of troubleshooting outbound network restrictions on Chinese cloud servers and enabling stable access to foreign academic resources.]]></summary></entry></feed>