<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shuhongdai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shuhongdai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-13T05:26:56+00:00</updated><id>https://shuhongdai.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Rethinking an Olefin Oligomerization from Three Years Ago – Chapter 1: Problem Statement and Some Initial Thoughts from Back Then</title><link href="https://shuhongdai.github.io/blog/2024/Rethinking-1/" rel="alternate" type="text/html" title="Rethinking an Olefin Oligomerization from Three Years Ago – Chapter 1: Problem Statement and Some Initial Thoughts from Back Then"/><published>2024-09-03T00:00:00+00:00</published><updated>2024-09-03T00:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Rethinking-1</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Rethinking-1/"><![CDATA[<p>In September 2021, following an intensive two-month summer training program in mathematical modeling, I embarked on my third year as an electrical engineering undergraduate and prepared for the grueling 72-hour National Mathematical Modeling Contest. Yet, the most memorable part of this experience was not the 20 hours I lost wrestling with a complex telecommunications problem—one I barely understood at the time, labeled as Problem A. Instead, it was the incident at the gym just before the contest began. Following my usual routine, I was midway through a shoulder lateral raise on my fourth set when I received an unexpected call from the dean of the mathematics faculty. He asked, rather bluntly, why I wasn’t preparing for the competition. Shortly after, I found myself hastily reporting to his office, where he proceeded to berate me in no uncertain terms.</p> <pre><code class="language-typograms">.------------------------------------------.
|.----------------------------------------.|
||      "https://www.mcm.edu.cn"          ||
|'----------------------------------------'|
| ________________________________________ |
||                                        ||
||   * Problem A                          ||
||   Shape Adjustment of a FAST           ||
||   Active Reflector                     ||
||                                        ||
||   * Problem B                          ||
||   Ethanol Coupling to Prepare          ||
||   C4 Olefins                           ||
||                                        ||
||   * Problem C                          ||
||   Raw Material Ordering and            ||
||   Transportation for Manufacturing     ||
||                                        ||
||   * Problem D                          ||
||   Online Optimization of               ||
||   Continuous Casting and Cutting       ||
||                                        ||
||   * Problem E                          ||
||   Identification of Traditional        ||
||   Chinese Medicinal Materials          ||
||                                        ||
|+----------------------------------------+|
.------------------------------------------.
</code></pre> <p>I often recount this episode as a humorous anecdote to friends, one of those peculiar experiences that, in hindsight, makes for a good story. But to return to the topic at hand: recently, I’ve been coming across the term “computational chemistry” more frequently. My close friend, Carlos Zhou, was an undergraduate in this field before shifting to computer science for his master’s. As a complete outsider to the world of chemistry, I may have some grounding in scientific computing and a passing interest in the trending field of AI4Chemistry, keeping tabs on its applications in drug discovery and protein structure prediction. But when it comes to the intricacies of organic chemistry, I remain wholly ignorant.</p> <p>Nevertheless, this hasn’t stopped me from recalling my choice during that September competition to pivot to Problem B—<em>Ethanol Coupling to Prepare C4 Olefins</em>—after abandoning Problem E. Looking back, I can’t quite remember why I chose this particular problem, especially with no background in organic chemistry to speak of. Perhaps I assumed, rather simplistically, that a mathematical modeling competition would primarily demand mathematical rigor, with little reliance on actual chemistry knowledge (an assumption that proved largely correct). Or perhaps I dismissed it as a straightforward data analysis exercise.</p> <p>Even now, I’m uncertain whether I’d be capable of fully solving that problem today (though, at the time, I did win a first prize). Logically speaking, my current thinking and methodological skills are undoubtedly far superior to what they were back then. Yet I still remember the unease I felt when submitting the MD5 hash after finishing the competition, feeling that my work had fallen short. As I watch today’s luminaries repeatedly “rethink” familiar concepts in top-tier computer science conferences, I’m often anxious over my own lack of publications. But, knowing my limitations, I find myself confined to my zero-impact-factor blog, where I “rethink” this foundational problem in mathematical modeling. Perhaps one day, this too will become another amusing anecdote.</p> <p>In short, I plan to launch this column, aiming to write four to six blog posts as part of a “rethinking” series. My goal is to revisit this problem, attempting to solve it again three years later, while sharing various related musings along the way. To start, I’ll present the original problem statement and dataset in Chinese, along with my English translation. Then, I’ll give a brief overview of the approach used in my award-winning paper from back then (though most of it has faded from memory—primarily some straightforward machine learning algorithms). Finally, I’ll enter the “rethinking” phase, where my present self takes on my younger, less experienced self—a playful exercise in intellectual overmatch. (In Chinese, there’s a phrase I like: “用前朝的剑斩本朝的官.” While the nuances are quite different here, I enjoy the irreverent tone.) Of course, you can also question this “rethinking” as lacking substance, meaning, novelty, feasibility… lacking everything. That’s your freedom, and it’s very likely everyone’s consensus, haha.</p>]]></content><author><name>Shuhong Dai</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry><entry><title type="html">Rethinking an Olefin Oligomerization from Three Years Ago – Chapter 0: Preface</title><link href="https://shuhongdai.github.io/blog/2024/Rethinking-0/" rel="alternate" type="text/html" title="Rethinking an Olefin Oligomerization from Three Years Ago – Chapter 0: Preface"/><published>2024-09-01T00:00:00+00:00</published><updated>2024-09-01T00:00:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Rethinking-0</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Rethinking-0/"><![CDATA[<p>In September 2021, following an intensive two-month summer training program in mathematical modeling, I embarked on my third year as an electrical engineering undergraduate and prepared for the grueling 72-hour National Mathematical Modeling Contest. Yet, the most memorable part of this experience was not the 20 hours I lost wrestling with a complex telecommunications problem—one I barely understood at the time, labeled as Problem A. Instead, it was the incident at the gym just before the contest began. Following my usual routine, I was midway through a shoulder lateral raise on my fourth set when I received an unexpected call from the dean of the mathematics faculty. He asked, rather bluntly, why I wasn’t preparing for the competition. Shortly after, I found myself hastily reporting to his office, where he proceeded to berate me in no uncertain terms.</p> <pre><code class="language-typograms">.------------------------------------------.
|.----------------------------------------.|
||      "https://www.mcm.edu.cn"          ||
|'----------------------------------------'|
| ________________________________________ |
||                                        ||
||   * Problem A                          ||
||   Shape Adjustment of a FAST           ||
||   Active Reflector                     ||
||                                        ||
||   * Problem B                          ||
||   Ethanol Coupling to Prepare          ||
||   C4 Olefins                           ||
||                                        ||
||   * Problem C                          ||
||   Raw Material Ordering and            ||
||   Transportation for Manufacturing     ||
||                                        ||
||   * Problem D                          ||
||   Online Optimization of               ||
||   Continuous Casting and Cutting       ||
||                                        ||
||   * Problem E                          ||
||   Identification of Traditional        ||
||   Chinese Medicinal Materials          ||
||                                        ||
|+----------------------------------------+|
.------------------------------------------.
</code></pre> <p>I often recount this episode as a humorous anecdote to friends, one of those peculiar experiences that, in hindsight, makes for a good story. But to return to the topic at hand: recently, I’ve been coming across the term “computational chemistry” more frequently. My close friend, Carlos Zhou, was an undergraduate in this field before shifting to computer science for his master’s. As a complete outsider to the world of chemistry, I may have some grounding in scientific computing and a passing interest in the trending field of AI4Chemistry, keeping tabs on its applications in drug discovery and protein structure prediction. But when it comes to the intricacies of organic chemistry, I remain wholly ignorant.</p> <p>Nevertheless, this hasn’t stopped me from recalling my choice during that September competition to pivot to Problem B—<em>Ethanol Coupling to Prepare C4 Olefins</em>—after abandoning Problem E. Looking back, I can’t quite remember why I chose this particular problem, especially with no background in organic chemistry to speak of. Perhaps I assumed, rather simplistically, that a mathematical modeling competition would primarily demand mathematical rigor, with little reliance on actual chemistry knowledge (an assumption that proved largely correct). Or perhaps I dismissed it as a straightforward data analysis exercise.</p> <p>Even now, I’m uncertain whether I’d be capable of fully solving that problem today (though, at the time, I did win a first prize). Logically speaking, my current thinking and methodological skills are undoubtedly far superior to what they were back then. Yet I still remember the unease I felt when submitting the MD5 hash after finishing the competition, feeling that my work had fallen short. As I watch today’s luminaries repeatedly “rethink” familiar concepts in top-tier computer science conferences, I’m often anxious over my own lack of publications. But, knowing my limitations, I find myself confined to my zero-impact-factor blog, where I “rethink” this foundational problem in mathematical modeling. Perhaps one day, this too will become another amusing anecdote.</p> <p>In short, I plan to launch this column, aiming to write four to six blog posts as part of a “rethinking” series. My goal is to revisit this problem, attempting to solve it again three years later, while sharing various related musings along the way. To start, I’ll present the original problem statement and dataset in Chinese, along with my English translation. Then, I’ll give a brief overview of the approach used in my award-winning paper from back then (though most of it has faded from memory—primarily some straightforward machine learning algorithms). Finally, I’ll enter the “rethinking” phase, where my present self takes on my younger, less experienced self—a playful exercise in intellectual overmatch. (In Chinese, there’s a phrase I like: “用前朝的剑斩本朝的官.” While the nuances are quite different here, I enjoy the irreverent tone.) Of course, you can also question this “rethinking” as lacking substance, meaning, novelty, feasibility… lacking everything. That’s your freedom, and it’s very likely everyone’s consensus, haha.</p>]]></content><author><name>Shuhong Dai</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry><entry><title type="html">A Supplementary Discussion on Correlation Coefficients</title><link href="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/" rel="alternate" type="text/html" title="A Supplementary Discussion on Correlation Coefficients"/><published>2024-06-22T00:32:10+00:00</published><updated>2024-06-22T00:32:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Correlation_Coefficients</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In my <a href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">previous blog post</a>, we examined covariance matrices, looking into their derivation, properties, and the geometric insights they offer for understanding multidimensional data. Covariance matrices provide a solid foundation for understanding how variables interact, yet they have certain limitations—especially when we want a clearer measure of the strength of these relationships.</p> <p>This brings us to correlation and the correlation matrix, essential concepts for interpreting the strength of relationships between variables independently of their original units or scales. Given their importance, this follow-up post serves as a brief guide to correlation coefficients and the correlation matrix. Here, we’ll cover the basics of correlation, the structure and derivation of the correlation matrix, and some of its most useful properties and applications.</p> <p>In the previous discussion on covariance, we established a foundational understanding of how variables co-vary, yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales. Here, the correlation coefficient, \(\rho_{X,Y}\), refines this measure by standardizing the relationship between two variables, allowing a comparison of their linear association independent of units.</p> <hr/> <h2 id="the-correlation-coefficient">The Correlation Coefficient</h2> <p>In the previous discussion on covariance, we established a foundational understanding of how variables co-vary, yet covariance itself is sensitive to the original units of measurement, limiting its direct interpretability across different data scales. Here, the correlation coefficient, \(\rho_{X,Y}\), refines this measure by standardizing the relationship between two variables, allowing a comparison of their linear association independent of units.</p> <h3 id="definition">Definition</h3> <p>The correlation coefficient \(\rho_{X,Y}\) between two random variables \(X\) and \(Y\) is formally defined as:</p> \[\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\] <p>where \(\text{Cov}(X, Y)\) represents the covariance between \(X\) and \(Y\), \(\sigma_X\) and \(\sigma_Y\) are the standard deviations of \(X\) and \(Y\), respectively.</p> <p>This formula can be viewed as a “normalized” covariance, essentially adjusting the relationship between \(X\) and \(Y\) by their individual dispersions, or spreads. This normalization is the key: it removes the influence of scale, making \(\rho_{X,Y}\) a dimensionless quantity that ranges from -1 to 1.</p> <h3 id="derivation-from-covariance">Derivation from Covariance</h3> <p>To delve deeper, consider the covariance definition between two random variables \(X\) and \(Y\):</p> \[\text{Cov}(X, Y) = E[(X - \mu_X)(Y - \mu_Y)]\] <p>where \(\mu_X = E[X]\) and \(\mu_Y = E[Y]\) are the expected values (means) of \(X\) and \(Y\). Covariance, while useful, retains the units of \(X\) and \(Y\), which complicates comparisons across variables with differing units or scales.</p> <p>The correlation coefficient refines this by dividing covariance by the product of the standard deviations of \(X\) and \(Y\):</p> \[\sigma_X = \sqrt{E[(X - \mu_X)^2]}, \quad \sigma_Y = \sqrt{E[(Y - \mu_Y)^2]}\] <p>Thus, correlation can be expressed as:</p> \[\rho_{X,Y} = \frac{E[(X - \mu_X)(Y - \mu_Y)]}{\sigma_X \sigma_Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\] <p>This reformulation gives us a measure of association on a standardized scale. Specifically, if \(\rho_{X,Y} = 1\), there exists a perfect positive linear relationship between \(X\) and \(Y\): as \(X\) increases, \(Y\) increases proportionally. If \(\rho_{X,Y} = -1\), the variables exhibit perfect negative linear correlation, where \(Y\) decreases as \(X\) increases. If \(\rho_{X,Y} = 0\), there is no linear relationship between \(X\) and \(Y\).</p> <h3 id="other-properties">Other Properties</h3> <p>One of the most useful features of \(\rho_{X,Y}\) is its unit invariance. Unlike covariance, which scales with the units of \(X\) and \(Y\), correlation removes these effects by normalizing with standard deviations, making \(\rho_{X,Y}\) dimensionless. This quality enables comparisons across variables with different units or scales, ensuring that the strength of relationships is evaluated consistently, regardless of measurement. Furthermore, the symmetry of correlation is another noteworthy property: \(\rho_{X,Y} = \rho_{Y,X}\). This symmetry arises naturally from the covariance term and reinforces that correlation measures a mutual relationship between variables, independent of which one is considered first. These properties, together, establish the correlation coefficient as a powerful, standardized metric for interpreting linear dependencies across diverse contexts</p> <hr/> <h2 id="the-correlation-matrix">The Correlation Matrix</h2> <p>When moving from a single pair of variables to multidimensional data, we naturally extend the concept of correlation to capture all pairwise relationships at once. This is where the correlation matrix \(R\) comes in, giving us a compact summary of the linear associations across an entire dataset. For a random vector \(X = [X_1, X_2, \dots, X_n]^T\), the correlation matrix \(R\) is an \(n \times n\) matrix, where each element \(R_{ij}\) represents the correlation coefficient between \(X_i\) and \(X_j\):</p> \[R_{ij} = \rho_{X_i, X_j} = \frac{\text{Cov}(X_i, X_j)}{\sigma_{X_i} \sigma_{X_j}}\] <h3 id="definition-and-structure">Definition and Structure</h3> <p>In other words, if \(X\) is a random vector with a covariance matrix \(\Sigma\), then \(R\) is constructed element-by-element as:</p> \[R = \begin{bmatrix} \rho_{X_1, X_1} &amp; \rho_{X_1, X_2} &amp; \dots &amp; \rho_{X_1, X_n} \\ \rho_{X_2, X_1} &amp; \rho_{X_2, X_2} &amp; \dots &amp; \rho_{X_2, X_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \rho_{X_n, X_1} &amp; \rho_{X_n, X_2} &amp; \dots &amp; \rho_{X_n, X_n} \end{bmatrix}.\] <p>Each diagonal entry \(R_{ii}\) equals 1, since \(\rho_{X_i, X_i} = \frac{\text{Cov}(X_i, X_i)}{\sigma_{X_i}^2} = 1\). This makes sense because a variable is perfectly correlated with itself. The off-diagonal elements \(R_{ij}\) (for \(i \neq j\)) give us the correlation between different variables, capturing their linear relationships in a standardized form that’s easy to interpret across the entire matrix.</p> <h3 id="relationship-to-the-covariance-matrix">Relationship to the Covariance Matrix</h3> <p>The correlation matrix \(R\) is directly derived from the covariance matrix \(\Sigma\) by a process of standardization. The idea here is to convert the units and scale-dependent covariances into unit-free, comparable correlation values. To achieve this, we divide each covariance by the product of the standard deviations of the relevant variables, so we arrive at the expression:</p> \[R = D^{-1} \Sigma D^{-1}\] <p>where \(D\) is the diagonal matrix of standard deviations, given by:</p> \[D = \text{diag}(\sigma_{X_1}, \sigma_{X_2}, \dots, \sigma_{X_n}).\] <p>Expanding this calculation, let’s see how each entry in \(R\) is computed in terms of \(\Sigma\) and \(D\):</p> <ol> <li>Start with the covariance matrix element \(\Sigma_{ij} = \text{Cov}(X_i, X_j)\).</li> <li>Divide \(\Sigma_{ij}\) by \(\sigma_{X_i} \sigma_{X_j}\), where \(\sigma_{X_i} = \sqrt{\Sigma_{ii}}\) and \(\sigma_{X_j} = \sqrt{\Sigma_{jj}}\).</li> <li> <p>This yields each element in \(R\) as:</p> \[R_{ij} = \frac{\Sigma_{ij}}{\sigma_{X_i} \sigma_{X_j}}.\] </li> </ol> <p>In matrix form, we achieve this by pre-multiplying and post-multiplying \(\Sigma\) with \(D^{-1}\), transforming the raw covariance entries into standardized, unitless correlation coefficients. This transformation is crucial when comparing variables measured on different scales, as it removes any units, letting us focus purely on the strength of the relationships.</p> <h3 id="some-properties">Some Properties</h3> <p>The correlation matrix \(R\) possesses a few key properties that make it an elegant and powerful tool for analyzing multidimensional data. First, it is symmetric by nature, since \(\rho_{X_i, X_j} = \rho_{X_j, X_i}\) for any pair of variables \(X_i\) and \(X_j\). This symmetry ensures that each pairwise correlation is mutual and gives the matrix a balanced, mirror-like structure around its diagonal. This diagonal, in turn, consists entirely of ones, as each variable is perfectly correlated with itself—a subtle reminder that correlation is inherently a self-consistent measure.</p> <p>In addition to its symmetry, \(R\) is positive semi-definite, meaning that for any vector \(z\), the quadratic form \(z^T R z\) is non-negative. This positive semi-definiteness implies that all eigenvalues of \(R\) are non-negative, which is significant because it confirms that \(R\) has a stable variance structure. This property becomes particularly valuable in applications like principal component analysis (PCA), where the correlation matrix’s eigenvalues reflect the spread of the data along different directions.</p> <p>Furthermore, every element in \(R\) falls within the interval \([-1, 1]\), a direct result of the correlation coefficient’s own bounded nature. This bounded range ensures that each entry in \(R\) is a pure, unitless indicator of linear association strength. Regardless of the scale or units of the original variables, the values in \(R\) give a consistent, standardized view of how variables align with each other.</p> <hr/> <h2 id="the-geometric-meaning">The Geometric Meaning</h2> <h3 id="geometric-interpretation-angles-and-alignments">Geometric Interpretation: Angles and Alignments</h3> <p>Consider two random variables \(X_i\) and \(X_j\), each represented as vectors in an \(n\)-dimensional data space. The correlation coefficient \(\rho_{X_i, X_j}\) between \(X_i\) and \(X_j\) can be understood as the cosine of the angle \(\theta\) between these two vectors. Formally, this relationship is given by:</p> \[\rho_{X_i, X_j} = \cos \theta_{ij}\] <p>where \(\theta_{ij}\) is the angle between the vectors corresponding to \(X_i\) and \(X_j\). When \(\rho_{X_i, X_j} = 1\), the vectors point in the same direction (\(\theta = 0^\circ\)), indicating a perfect positive linear relationship. Conversely, if \(\rho_{X_i, X_j} = -1\), the vectors point in opposite directions (\(\theta = 180^\circ\)), representing a perfect negative linear relationship. A correlation of zero corresponds to \(\theta = 90^\circ\), suggesting that the vectors are orthogonal and thus linearly uncorrelated.</p> <p>This geometric interpretation gives a clear, visual sense of the relationships encoded in \(R\): the closer the angle between two variables’ vectors is to zero, the stronger and more positive their correlation; the closer the angle is to \(180^\circ\), the stronger and more negative the correlation. And when the vectors are perpendicular, they are uncorrelated in a linear sense, even though nonlinear relationships might still exist.</p> <h3 id="eigenvalues-and-eigenvectors-the-structure-of-r">Eigenvalues and Eigenvectors: The Structure of \(R\)</h3> <p>The geometric story of \(R\) deepens when we consider its eigenvalues and eigenvectors. By performing an eigen-decomposition on \(R\), we can break it down as follows:</p> \[R = Q \Lambda Q^T\] <p>where \(Q\) is an orthogonal matrix whose columns are the eigenvectors of \(R\), and \(\Lambda\) is a diagonal matrix containing the eigenvalues of \(R\). Each eigenvalue \(\lambda_i\) in \(\Lambda\) represents the variance explained along the direction specified by the corresponding eigenvector \(q_i\) in \(Q\).</p> <p>Geometrically, the eigenvectors of \(R\) indicate the principal directions of variance in the data. The eigenvalues, on the other hand, tell us the “lengths” or “strengths” of these directions. A large eigenvalue associated with an eigenvector indicates that the data has significant variance along that direction, meaning the variables exhibit strong alignment with each other in this space. Conversely, smaller eigenvalues correspond to directions with less variance, suggesting that the data is more tightly clustered or linearly dependent in those directions.</p> <p>Since \(R\) is positive semi-definite, all eigenvalues are non-negative, and the sum of the eigenvalues equals the dimensionality of the space. Each eigenvalue-eigenvector pair offers a glimpse into the “shape” of the data cloud in terms of its stretch and orientation in different directions.</p> <h3 id="pca-extracting-linear-patterns">PCA: Extracting Linear Patterns</h3> <p>The correlation matrix \(R\) is a natural tool for conducting PCA, a technique used to identify the main linear patterns in data by transforming it into a new coordinate system based on the eigenvectors of \(R\). In PCA, the data is projected onto the eigenvectors of \(R\), with each projection representing a principal component. The eigenvalue associated with each eigenvector measures the variance along that principal component, so ordering the eigenvalues from largest to smallest provides a ranking of the directions of greatest variability.</p> <p>To perform PCA, we start by obtaining the eigen-decomposition of \(R\):</p> \[R = Q \Lambda Q^T\] <p>The columns of \(Q\), the eigenvectors, form the new basis in which the data is represented. Each eigenvector corresponds to a principal component, and the associated eigenvalue indicates the amount of variance captured by that component. For instance, the first principal component (corresponding to the largest eigenvalue) captures the direction of maximum variance in the data, providing the best one-dimensional summary of the data’s spread. Adding successive principal components gives progressively refined approximations of the data, capturing most of the structure with far fewer dimensions than the original dataset.</p> <p>By using \(R\) in PCA, we effectively perform dimensionality reduction based on the strength of the linear relationships between variables, preserving the most informative aspects of the data while discarding redundancy.</p> <h3 id="demo">Demo</h3> <p>To illustrate how PCA leverages the correlation matrix \(R\) to extract meaningful linear patterns, let’s walk through a concrete example. Suppose we have a dataset with three variables—say, height, weight, and age—measured across a sample of individuals. Let’s assume that after standardizing the data, we calculate the correlation matrix \(R\) as follows:</p> \[R = \begin{bmatrix} 1 &amp; 0.8 &amp; 0.5 \\ 0.8 &amp; 1 &amp; 0.4 \\ 0.5 &amp; 0.4 &amp; 1 \end{bmatrix}\] <p>Each entry \(R_{ij}\) represents the correlation between pairs of variables. For example, the correlation between height and weight is 0.8, indicating a strong positive linear relationship, while the correlation between height and age is 0.5, a moderate positive association.</p> <p><strong>Step 1: Eigen-Decomposition of the Correlation Matrix</strong></p> <p>The first step in PCA is to perform an eigen-decomposition of \(R\) to identify the principal directions of variance. By finding the eigenvalues and eigenvectors of \(R\), we can understand the main directions in which the data varies most.</p> <p>For our example, suppose the eigenvalues and their corresponding eigenvectors for \(R\) are as follows:</p> <ul> <li>Eigenvalue \(\lambda_1 = 1.8\), with eigenvector \(q_1 = \begin{bmatrix} 0.7 \\ 0.6 \\ 0.4 \end{bmatrix}\)</li> <li>Eigenvalue \(\lambda_2 = 0.9\), with eigenvector \(q_2 = \begin{bmatrix} -0.5 \\ 0.7 \\ 0.5 \end{bmatrix}\)</li> <li>Eigenvalue \(\lambda_3 = 0.3\), with eigenvector \(q_3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}\)</li> </ul> <p>Each eigenvalue represents the variance explained by its corresponding eigenvector direction. The largest eigenvalue, \(\lambda_1 = 1.8\), tells us that the first principal component explains the most variance in the data—roughly \(\frac{1.8}{3} = 60\%\) of the total variance (since the sum of the eigenvalues is 3 in a three-variable system). The second eigenvalue \(\lambda_2 = 0.9\) accounts for \(30\%\) of the variance, and the third eigenvalue \(\lambda_3 = 0.3\) contributes \(10\%\).</p> <p><strong>Step 2: Interpreting the Principal Components</strong></p> <p>Each eigenvector represents a direction in the original variable space (height, weight, age) along which there is a certain level of variability in the data. Let’s break down what each principal component reveals:</p> <ol> <li> <p><strong>First Principal Component (PC1)</strong>: The first eigenvector \(q_1 = \begin{bmatrix} 0.7 \\ 0.6 \\ 0.4 \end{bmatrix}\) suggests that PC1 is a weighted combination of all three variables, with the largest weights on height and weight. This component captures the common variability between height and weight, reflecting a general “size” factor—individuals with larger heights tend to have larger weights. Since this component explains 60% of the variance, it’s the most informative single direction for summarizing the data.</p> </li> <li> <p><strong>Second Principal Component (PC2)</strong>: The second eigenvector \(q_2 = \begin{bmatrix} -0.5 \\ 0.7 \\ 0.5 \end{bmatrix}\) has significant contributions from all three variables, but with a negative sign for height and positive signs for weight and age. This suggests that PC2 captures a contrast between height and a combined weight-age factor, which might reflect a tendency where, for a given age, people with smaller heights have relatively higher weights.</p> </li> <li> <p><strong>Third Principal Component (PC3)</strong>: The third eigenvector \(q_3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}\) explains only 10% of the total variance. This component primarily reflects variability in age with some influence from height and a negative contribution from weight, capturing a pattern that is less pronounced in the data.</p> </li> </ol> <p><strong>Step 3: Projecting the Data onto the Principal Components</strong></p> <p>With the principal components identified, we can project our original data onto these components to transform it into a new coordinate system defined by \(q_1\), \(q_2\), and \(q_3\). This projection can be computed as:</p> \[\text{Projected Data} = Q^T X\] <p>where \(Q\) is the matrix of eigenvectors:</p> \[Q = \begin{bmatrix} 0.7 &amp; -0.5 &amp; 0.5 \\ 0.6 &amp; 0.7 &amp; -0.2 \\ 0.4 &amp; 0.5 &amp; 0.8 \end{bmatrix}\] <p>For each individual in the dataset, their original measurements in terms of height, weight, and age are transformed into scores along PC1, PC2, and PC3. These scores reflect the new, simplified representation of each individual in terms of the most significant patterns in the data. For example, projecting the original data onto PC1 (the direction of greatest variance) gives a one-dimensional summary of “size” variation across individuals, effectively condensing the information from three variables into a single informative score.</p> <p><strong>Step 4: Reducing Dimensionality</strong></p> <p>Since PC1 and PC2 together account for 90% of the total variance, we might decide to approximate our data using only these two components, discarding PC3, which contributes relatively little. By retaining only PC1 and PC2, we reduce the dimensionality of the dataset from three to two while preserving the bulk of the information.</p> <p>This reduced representation is particularly useful for visualization: each individual can now be represented by a point in a two-dimensional plane defined by PC1 and PC2. The positions of these points reflect the primary structure and relationships in the original data, without the “noise” from minor variations that PC3 captures. Moreover, patterns and clusters within the data often become more apparent in this reduced-dimensional space, revealing insights that might not be obvious in the original three-dimensional view.</p> <script type="text/tikz">
\begin{tikzpicture}

% Darker blue fill for the front Weight-Height square plane
\fill[blue!40, opacity=0.6] (0,0,0) -- (2.8,0,0) -- (2.8,2.8,0) -- (0,2.8,0) -- cycle;

% Lighter blue fill for the Weight-Age side parallelogram
\fill[blue!30, opacity=0.4] (0,0,0) -- (0,0,2.8) -- (0,2.8,2.8) -- (0,2.8,0) -- cycle;

% Lighter blue fill for the Height-Age top parallelogram
\fill[blue!30, opacity=0.4] (0,0,2.8) -- (2.8,0,2.8) -- (2.8,2.8,2.8) -- (0,2.8,2.8) -- cycle;

% Original 3D coordinate system (Height, Weight, Age)
\draw[->, thick] (0,0,0) -- (2.8,0,0) node[anchor=north east] {Height};
\draw[->, thick] (0,0,0) -- (0,2.8,0) node[anchor=north west] {Weight};
\draw[->, thick] (0,0,0) -- (0,0,2.8) node[anchor=south] {Age};

% Original data points in the 3D coordinate system (blue)
\fill[blue] (1,2,1.5) circle (2pt);
\fill[blue] (1.8,1.2,1.8) circle (2pt);
\fill[blue] (1.3,1.8,1.3) circle (2pt);
\fill[blue] (1.6,1.5,1) circle (2pt);

% Arrow indicating projection to the PC1-PC2 plane, with reduced distance
\draw[->, thick, dashed] (3, 1.8, 1.5) -- (5.8, 1.8, 0) node[midway, above, sloped] {Projection onto PC1-PC2 plane};

% Darker orange fill for the 2D PC1-PC2 plane, with slight shadow effect
\fill[orange!40, opacity=0.5] (6.5,0,0) -- (9,0,0) -- (8.7,2.3,0) -- (6.2,2.3,0) -- cycle;

% 2D PC1-PC2 plane axes
\draw[->, color=orange, thick] (6.5,0,0) -- (9,0,0) node[anchor=north east] {PC1(60\%)};
\draw[->, color=orange, thick] (6.5,0,0) -- (6.2,2.3,0) node[anchor=north west] {PC2(30\%)};

% Projected data points on the PC1-PC2 plane (red)
\fill[red] (7.2,1.8,0) circle (2pt);
\fill[red] (7.9,1,0) circle (2pt);
\fill[red] (7.4,1.5,0) circle (2pt);
\fill[red] (7.8,1.2,0) circle (2pt);

% Dashed lines connecting original points to their projections
\draw[dashed, gray] (1,2,1.5) -- (7.2,1.8,0);
\draw[dashed, gray] (1.8,1.2,1.8) -- (7.9,1,0);
\draw[dashed, gray] (1.3,1.8,1.3) -- (7.4,1.5,0);
\draw[dashed, gray] (1.6,1.5,1) -- (7.8,1.2,0);

\end{tikzpicture}
</script> <p> </p> <p><strong>Summary</strong></p> <p>In this example, PCA has taken a dataset with three interrelated variables and transformed it into a new set of uncorrelated components that reveal the primary patterns of variability. By examining the eigenvalues and eigenvectors of the correlation matrix \(R\), we extracted principal components that provided both a compact and interpretable representation of the data.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>In conclusion, this post serves as a natural extension to <a href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/">our previous discussion on covariance</a>, diving into the aspects of correlation that were left unexplored. While the covariance matrix lays the groundwork for understanding how variables interact, it lacks the standardization needed for clear, direct comparisons. The correlation matrix fills this gap, distilling complex relationships into a standardized, unit-free format that provides immediate insights into linear dependencies across variables. Each entry in the matrix reflects a pairwise relationship as an angle or alignment.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[A smart approach to capturing diverse data patterns without overwhelming resources.]]></summary></entry><entry><title type="html">An Introductory Look at Covariance and the Mean Vector</title><link href="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/" rel="alternate" type="text/html" title="An Introductory Look at Covariance and the Mean Vector"/><published>2024-06-11T00:32:10+00:00</published><updated>2024-06-11T00:32:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/An_Introductory_Look_at_Covariance_and_the_Mean_Vector/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>When you first encounter the world of multivariate Gaussian distributions, it’s easy to feel like you’ve entered a labyrinth of equations, variables, and matrices. But beneath the mathematical machinery lies a beautifully structured framework that helps us understand complex data, and in many ways, it’s as elegant as it is powerful.</p> <p>In statistics, the Gaussian, or “normal” distribution, is often our first stop when we dive into data analysis. We’ve all seen its familiar bell-shaped curve, neatly centered around a mean, showing us the most probable values a single variable might take. But in reality, data rarely exists in a vacuum. Many variables are interconnected, forming a “multivariate” data landscape where each variable influences and interacts with others. Here, the multivariate Gaussian distribution steps in as a natural extension of the single-variable Gaussian, providing us a way to model these multidimensional relationships.</p> <p>At the heart of this distribution are two key players: the mean vector and the covariance matrix. The mean vector is the multivariate equivalent of the single-variable mean, summarizing the central tendencies of all variables in one go. It tells us where the “center” of our data cloud lies, capturing the average or typical values across all dimensions.</p> <blockquote> <p>Updated on June 22, 2024: Additionally, <a href="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/">a new blog post</a> has been published that expands on this topic with content covering correlation coefficients and the correlation coefficient matrix.</p> </blockquote> <p>The covariance matrix, on the other hand, is a bit like a backstage operator. It describes how variables interact with each other, revealing not just their individual spreads but also how they move in tandem. Each entry in this matrix provides insights into the relationship between pairs of variables, showing whether they rise and fall together or behave independently. Together, the mean vector and covariance matrix form a powerful duo, shaping the geometry of the distribution and giving us a complete picture of how our data points are scattered and related.</p> <p>In this post, we’ll explore the roles of the mean vector and covariance matrix within the multivariate Gaussian distribution, diving into how they help us grasp and model complex data structures.</p> <hr/> <h2 id="the-mean-vector--definition-and-properties">The Mean Vector – Definition and Properties</h2> <p>Now that we’ve opened the door to the multivariate Gaussian, let’s take a closer look at one of its core components: the mean vector. You can think of the mean vector as the “centroid” or the “anchor” point of the distribution—a snapshot of where the average of each variable in a dataset tends to lie. In a multivariate world, we don’t just care about one mean; we want to know the average value in each dimension, and that’s where the mean vector steps in.</p> <h3 id="definition">Definition</h3> <p>For a multivariate random variable \(X\) with \(n\) dimensions, the mean vector \(\mu\) is defined as:</p> \[\mu = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix},\] <p>where each \(\mu_i = E[X_i]\) represents the expected value of the \(i\)-th variable. Essentially, the mean vector \(\mu\) gives us a one-stop summary of the “average” position of all dimensions, capturing the expected value along each axis of the multidimensional data space.</p> <h3 id="derivation-expectation-of-the-mean-vector">Derivation: Expectation of the Mean Vector</h3> <p>To fully appreciate the mean vector, let’s delve into its calculation using the expectation operator. Suppose \(X = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix}\) is our multivariate random variable, where each \(X_i\) is a random variable in itself. The mean vector \(\mu\) is simply the expected value of \(X\):</p> \[\mu = E[X].\] <p>Breaking this down, <strong>the expectation of \(X\) is computed component-wise</strong>. That is,</p> \[\mu = E[X] = \begin{bmatrix} E[X_1] \\ E[X_2] \\ \vdots \\ E[X_n] \end{bmatrix} = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix}.\] <p>This form allows us to treat each dimension independently when calculating the mean, <strong>even though they may be interdependent in terms of their distributions.</strong></p> <h3 id="properties-of-the-mean-vector">Properties of the Mean Vector</h3> <p>The mean vector isn’t just a passive summary of averages; it’s highly responsive to transformations, particularly linear ones. Let’s explore one of its key properties: how it behaves under a linear transformation. Consider a linear transformation where we define a new random vector \(Y\) based on \(X\) as:</p> \[Y = AX + b,\] <p>where \(A\) is a constant matrix of dimensions \(m \times n\) and \(b\) is a constant vector of dimension \(m \times 1\). This setup is common in multivariate analysis, where we often transform data to new coordinate systems or scales. Here, we want to understand how this transformation impacts the mean vector.</p> <p>To derive the expected value of \(Y\), we use the linearity of expectation:</p> \[E[Y] = E[AX + b].\] <p>Since \(A\) and \(b\) are constants, we can simplify:</p> \[E[Y] = AE[X] + b = A\mu + b.\] <p>So, the transformed mean vector of \(Y\) is given by \(A\mu + b\). This tells us that linear transformations shift and stretch the mean vector in predictable ways: multiplying by \(A\) scales or rotates \(\mu\), while adding \(b\) translates it. Let’s formalize this with a quick proof. Suppose \(X\) is a multivariate random variable with mean vector \(\mu = E[X]\), and we define \(Y = AX + b\). By the definition of expectation, we have:</p> \[E[Y] = E[AX + b] = E[AX] + E[b].\] <p>Since \(b\) is constant, \(E[b] = b\). Additionally, because expectation is a linear operator, we get \(E[AX] = A E[X] = A \mu\). Thus,</p> \[E[Y] = A \mu + b.\] <p>This property is not only elegant but incredibly useful. It implies that, regardless of the transformation (as long as it’s linear), we can predict how the mean vector shifts without recalculating everything from scratch. This “transformation invariance” simplifies a lot of practical work in data analysis, letting us predict and manipulate mean vectors in transformed spaces.</p> <p>In sum, the mean vector \(\mu\) isn’t just a set of averages. It’s a fundamental descriptor that tells us where our data is centered, and its behavior under transformations is both consistent and computationally friendly. <strong>Whether you’re scaling, rotating, or translating your data, the mean vector adjusts accordingly, maintaining its role as the central anchor of your multivariate Gaussian distribution.</strong></p> <h3 id="demo">Demo</h3> <p>To understand how the mean vector \(\mu\) behaves, particularly under transformations, let’s start by generating some multivariate data. We’ll create a simple 2D Gaussian distribution, calculate its mean vector, and then apply a linear transformation to see how \(\mu\) changes in response.</p> <p>Here’s a Python script that does exactly this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Set a random seed for reproducibility
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Step 1: Define the original mean vector and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>  <span class="c1"># Positive correlation between X1 and X2
</span>
<span class="c1"># Generate a sample of 500 points from the 2D Gaussian distribution
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Plot the original distribution
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Original Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector (Original)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Original Multivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, we initialize a mean vector \(\mu = [2, 3]\) and a covariance matrix with a positive correlation between the two variables. We then generate 500 points to visually represent the data cloud centered around \(\mu\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution-480.webp 480w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution-800.webp 800w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution.png" class="img-fluid" width="600" height="400" alt="Original Multivariate Gaussian Distribution" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now, let’s apply a linear transformation to this data. For demonstration, we’ll use a transformation matrix \(A = \begin{bmatrix} 1.5 &amp; 0 \\ 0.5 &amp; 1 \end{bmatrix}\) and a translation vector \(b = \begin{bmatrix} -1 \\ 2 \end{bmatrix}\). According to our derivation, we expect the mean vector to change to \(A\mu + b\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 2: Define the transformation matrix A and translation vector b
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Apply the transformation to each data point
</span><span class="n">transformed_data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Calculate the transformed mean vector
</span><span class="n">transformed_mu</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Plot the transformed distribution
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Transformed Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">transformed_mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">transformed_mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector (Transformed)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Transformed Multivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>This second snippet applies our transformation and then calculates the new mean vector using \(A\mu + b\), aligning with our theoretical result. Here’s what we observe from the plot:</p> <ol> <li><strong>Data Shift and Rotation</strong>: The data cloud shifts according to the translation vector \(b\) and stretches based on the transformation matrix \(A\).</li> <li><strong>Mean Vector Update</strong>: The new mean vector \(\mu\) moves precisely to \(A\mu + b\), as predicted.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution-480.webp 480w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution-800.webp 800w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution.png" class="img-fluid" width="600" height="400" alt="Transformed Multivariate Gaussian Distribution" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="covariance-matrix--definition-and-computation">Covariance Matrix – Definition and Computation</h2> <p>If the mean vector \(\mu\) gives us a sense of location, <strong>then the covariance matrix \(\Sigma\) gives us a sense of shape.</strong> It describes how variables are spread and how they relate to each other, capturing both individual variances and pairwise covariances.</p> <h3 id="definition-1">Definition</h3> <p>The covariance matrix \(\Sigma\) of a multivariate random variable \(X = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix}\) is defined as:</p> \[\Sigma = \begin{bmatrix} \text{Cov}(X_1, X_1) &amp; \text{Cov}(X_1, X_2) &amp; \cdots &amp; \text{Cov}(X_1, X_n) \\ \text{Cov}(X_2, X_1) &amp; \text{Cov}(X_2, X_2) &amp; \cdots &amp; \text{Cov}(X_2, X_n) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \text{Cov}(X_n, X_1) &amp; \text{Cov}(X_n, X_2) &amp; \cdots &amp; \text{Cov}(X_n, X_n) \end{bmatrix},\] <p>where each element \(\Sigma\_{ij} = \text{Cov}(X_i, X_j)\) is the covariance between \(X_i\) and \(X_j\).</p> <p>Covariance, in essence, measures the degree to which two variables vary together. When \(i = j\), \(\Sigma\_{ii} = \text{Var}(X_i)\), representing the variance of \(X_i\) itself.</p> <h3 id="derivation-the-formula-for-covariance">Derivation: The Formula for Covariance</h3> <p>To formally compute \(\Sigma\), let’s start with the mean vector \(\mu\) of \(X\), defined as:</p> \[\mu = E[X] = \begin{bmatrix} E[X_1] \\ E[X_2] \\ \vdots \\ E[X_n] \end{bmatrix}.\] <p>Now, the covariance matrix is calculated as the expectation of the outer product of the deviations of \(X\) from its mean:</p> \[\Sigma = E[(X - \mu)(X - \mu)^T].\] <p>This formula may look abstract, but it’s grounded in a straightforward concept: by centering \(X\) around its mean (i.e., subtracting \(\mu\)) and then taking the outer product, we capture the spread and co-spread of each variable pair.</p> <h4 id="step-by-step-derivation-of-sigma">Step-by-Step Derivation of \(\Sigma\)</h4> <p>Let’s expand the formula a bit to understand the inner workings. We have:</p> \[\Sigma = E \left[ \begin{bmatrix} X_1 - \mu_1 \\ X_2 - \mu_2 \\ \vdots \\ X_n - \mu_n \end{bmatrix} \begin{bmatrix} X_1 - \mu_1 &amp; X_2 - \mu_2 &amp; \cdots &amp; X_n - \mu_n \end{bmatrix} \right].\] <p>When we take the expectation, each element \(\Sigma\_{ij}\) becomes:</p> \[\Sigma_{ij} = E[(X_i - \mu_i)(X_j - \mu_j)].\] <p>This is simply the definition of covariance between \(X_i\) and \(X_j\). As such, \(\Sigma\) encodes all pairwise relationships in one matrix, giving us a complete picture of how our variables are interconnected.</p> <h3 id="properties-of-the-covariance-matrix">Properties of the Covariance Matrix</h3> <p>The covariance matrix isn’t just a convenient summary; it has some fascinating mathematical properties that make it a powerful tool in multivariate analysis.</p> <h4 id="symmetry">Symmetry</h4> <p>One of the most fundamental properties of \(\Sigma\) is that it is symmetric. To see why, consider the definition of covariance:</p> \[\Sigma_{ij} = \text{Cov}(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)].\] <p>By the commutative property of multiplication, \((X_i - \mu_i)(X_j - \mu_j) = (X_j - \mu_j)(X_i - \mu_i)\). Thus:</p> \[\Sigma_{ij} = E[(X_i - \mu_i)(X_j - \mu_j)] = E[(X_j - \mu_j)(X_i - \mu_i)] = \Sigma_{ji}.\] <p>This symmetry property tells us that \(\Sigma\) is equal to its own transpose, or \(\Sigma = \Sigma^T\). This is crucial because it ensures that the eigenvalues of \(\Sigma\) are real, a feature that will come in handy when interpreting the distribution’s geometry.</p> <h4 id="positive-semi-definiteness">Positive Semi-Definiteness</h4> <p>Another key property of the covariance matrix is that it is positive semi-definite. Mathematically, this means that for any vector \(z\), we have:</p> \[z^T \Sigma z \geq 0.\] <p><strong>Intuitively, this property tells us that the “spread” of the data is never negative</strong>—a fundamental requirement for any meaningful measure of variance. Let’s quickly prove this property.</p> <p>For any vector \(z \in \mathbb{R}^n\), we have:</p> \[z^T \Sigma z = z^T E[(X - \mu)(X - \mu)^T] z = E[z^T (X - \mu)(X - \mu)^T z] = E[(z^T(X - \mu))^2].\] <p>Since \((z^T(X - \mu))^2\) is a square, it is always non-negative, which implies that \(z^T \Sigma z \geq 0\). Thus, \(\Sigma\) is positive semi-definite, meaning it has non-negative eigenvalues, another crucial feature for understanding data spread.</p> <h4 id="covariance-under-linear-transformation">Covariance under Linear Transformation</h4> <p>One of the most powerful aspects of the covariance matrix is how it transforms under linear operations. Suppose we apply a linear transformation \(Y = AX + b\), where \(A\) is a constant matrix and \(b\) is a constant vector. Just as we saw with the mean vector, we want to understand how the covariance matrix \(\Sigma\) changes under this transformation.</p> <p>The covariance of \(Y\), denoted \(\text{Cov}(Y)\), is given by:</p> \[\text{Cov}(Y) = E[(Y - E[Y])(Y - E[Y])^T].\] <p>Since \(Y = AX + b\), we can substitute and simplify:</p> \[Y - E[Y] = AX + b - (AE[X] + b) = A(X - E[X]).\] <p>Thus:</p> \[\text{Cov}(Y) = E[A(X - E[X])(X - E[X])^T A^T] = A E[(X - E[X])(X - E[X])^T] A^T = A \Sigma A^T.\] <p>This result, \(\text{Cov}(Y) = A \Sigma A^T\), tells us that under a linear transformation, the covariance matrix \(\Sigma\) transforms in a predictable manner. This property is critical in fields like machine learning and statistics, where data is often scaled or rotated to enhance interpretability or improve model performance.</p> <h3 id="demo-1">Demo</h3> <p>To illustrate the covariance matrix in action, let’s revisit our earlier 2D Gaussian example, but now focus on how the covariance affects the spread and orientation of the data cloud. We’ll also examine how the covariance matrix changes under a linear transformation, connecting this back to our theoretical results.</p> <h4 id="step-1-visualize-the-original-covariance-structure">Step 1: Visualize the Original Covariance Structure</h4> <p>In this example, we’ll use the same mean vector \(\mu = \begin{bmatrix} 2 \\ 3 \end{bmatrix}\) and a covariance matrix \(\Sigma = \begin{bmatrix} 1 &amp; 0.8 \\ 0.8 &amp; 1 \end{bmatrix}\). The positive off-diagonal values indicate a positive correlation between the two variables, meaning they tend to vary together.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Define the mean vector and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Generate data points from a 2D Gaussian distribution
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Plot the data cloud and the covariance structure
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Original Multivariate Gaussian Distribution with Covariance Structure</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="c1"># Visualize the covariance as an ellipse
</span><span class="kn">from</span> <span class="n">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>

<span class="c1"># Eigenvalues and eigenvectors of the covariance matrix
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># Width and height of the ellipse based on the eigenvalues (scaled for visibility)
</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>
<span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">'</span><span class="s">None</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Covariance Ellipse</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, we:</p> <ol> <li>Generate a data cloud based on \(\Sigma\), capturing the positive correlation between \(X_1\) and \(X_2\).</li> <li>Plot the data points along with the mean vector.</li> <li>Use the eigenvalues and eigenvectors of \(\Sigma\) to plot an ellipse representing the covariance structure. The orientation and size of this ellipse reflect the spread and correlation encoded in \(\Sigma\), with the longer axis aligned along the direction of greatest variance.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure-480.webp 480w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure-800.webp 800w,/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Original%20Multivariate%20Gaussian%20Distribution%20with%20Covariance%20Structure.png" class="img-fluid" width="600" height="400" alt="Original Multivariate Gaussian Distribution with Covariance Structure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="step-2-apply-a-linear-transformation-and-observe-the-covariance-matrix-change">Step 2: Apply a Linear Transformation and Observe the Covariance Matrix Change</h4> <p>Next, let’s apply a linear transformation to our data. We’ll use a transformation matrix \(A = \begin{bmatrix} 1.2 &amp; 0.5 \\ 0.3 &amp; 0.8 \end{bmatrix}\), which will stretch and rotate the data. According to our earlier derivation, the new covariance matrix should be \(A \Sigma A^T\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the transformation matrix A
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>

<span class="c1"># Transform the data
</span><span class="n">transformed_data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span>

<span class="c1"># Calculate the transformed covariance matrix
</span><span class="n">transformed_cov</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">cov</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span>

<span class="c1"># Plot the transformed data and new covariance structure
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">transformed_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Transformed Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Transformed Multivariate Gaussian Distribution with New Covariance Structure</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Calculate and plot the new covariance ellipse
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">transformed_cov</span><span class="p">)</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>
<span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">A</span> <span class="o">@</span> <span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">'</span><span class="s">None</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Transformed Covariance Ellipse</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this visualization:</p> <ol> <li>We transform the data by applying \(A\), which stretches and rotates the original distribution.</li> <li>We compute the transformed covariance matrix as \(A \Sigma A^T\) and plot the new covariance ellipse to represent its structure.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure-480.webp 480w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure-800.webp 800w,/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Transformed%20Multivariate%20Gaussian%20Distribution%20with%20New%20Covariance%20Structure.png" class="img-fluid" width="600" height="400" alt="Transformed Multivariate Gaussian Distribution with New Covariance Structure" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="observations">Observations</h3> <p>With this code, we see the impact of a linear transformation on the covariance matrix:</p> <ul> <li><strong>Spread and Orientation</strong>: The transformed covariance ellipse is reshaped and reoriented. The principal axes of the ellipse align with the directions of greatest and least variance in the transformed data, which reflect the new covariance structure encoded by \(A \Sigma A^T\).</li> <li><strong>Predicted Transformation</strong>: The calculation \(A \Sigma A^T\) matches the new spread, showing that even though the data has shifted in space, we can predict exactly how its variability changes.</li> </ul> <hr/> <h2 id="the-geometric-meaning-of-the-covariance-matrix">The Geometric Meaning of the Covariance Matrix</h2> <p>At this point, we know the covariance matrix \(\Sigma\) defines the structure of a multivariate Gaussian distribution in terms of variance and covariance. But what does \(\Sigma\) really <em>look like</em>? In a two-dimensional space, the covariance matrix paints an elegant picture of geometry, describing a distribution’s shape as an ellipse. This section is a journey into the geometric implications of \(\Sigma\), where each feature of the covariance matrix corresponds to a unique aspect of the data’s spatial spread.</p> <h3 id="elliptical-contours-the-shape-of-data-in-2d">Elliptical Contours: The Shape of Data in 2D</h3> <p>In the case of a two-dimensional Gaussian distribution, the contours of equal density—think of these as the “outlines” or “borders” where data points tend to cluster—form concentric ellipses. These ellipses reveal the spread of data around the mean vector \(\mu\) and are directly determined by the covariance matrix \(\Sigma\).</p> <p>Each ellipse’s geometry—the length and direction of its axes—offers a visual interpretation of \(\Sigma\):</p> <ol> <li><strong>Principal Axes (Direction)</strong>: The directions of the ellipse’s axes correspond to the eigenvectors of \(\Sigma\). These eigenvectors are vectors in space that point along the directions where the data varies the most (the “principal directions”).</li> <li><strong>Axis Lengths (Spread)</strong>: The lengths of these axes are proportional to the square roots of the eigenvalues of \(\Sigma\). The larger an eigenvalue, the longer the axis, meaning that data stretches more along this direction. Smaller eigenvalues correspond to shorter axes, indicating less variability in that direction.</li> </ol> <h3 id="eigenvalue-decomposition-of-the-covariance-matrix">Eigenvalue Decomposition of the Covariance Matrix</h3> <p>To understand the ellipse’s structure more deeply, let’s look at the eigenvalue decomposition of \(\Sigma\). The covariance matrix \(\Sigma\) is symmetric, meaning it can be decomposed as:</p> \[\Sigma = Q \Lambda Q^T,\] <p>where:</p> <ul> <li>\(Q\) is a matrix of eigenvectors of \(\Sigma\), and</li> <li>\(\Lambda\) is a diagonal matrix of eigenvalues of \(\Sigma\), with each eigenvalue corresponding to the variance along a principal direction.</li> </ul> <p>This decomposition provides a clean geometric interpretation. If we rewrite our multivariate random variable \(X\) (centered at the origin for simplicity) as:</p> \[X = Q D Z,\] <p>where \(D = \sqrt{\Lambda}\) is the matrix of square roots of the eigenvalues and \(Z\) is a vector of standard normal variables, we can see that \(Q\) rotates the data along the principal directions and \(D\) scales it according to the variances.</p> <p>Let’s illustrate this in two steps.</p> <h4 id="step-1-visualize-the-original-data-with-eigenvectors">Step 1: Visualize the Original Data with Eigenvectors</h4> <p>We can start by plotting the original data and overlaying the eigenvectors of \(\Sigma\) to visualize the principal directions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Define mean and covariance matrix
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>

<span class="c1"># Generate data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Plot the data
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Multivariate Gaussian with Principal Directions</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Compute eigenvalues and eigenvectors
</span><span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

<span class="c1"># Plot eigenvectors as principal directions
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">eigvecs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]],</span>
             <span class="p">[</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">eigvecs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]],</span>
             <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Principal Direction </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, we:</p> <ol> <li>Generate data according to the mean \(\mu\) and covariance matrix \(\Sigma\).</li> <li>Plot the data along with the mean vector.</li> <li>Compute the eigenvalues and eigenvectors of \(\Sigma\), and overlay the eigenvectors on the data, scaled by the square root of their corresponding eigenvalues to represent the primary directions and spread.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions-480.webp 480w,/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions-800.webp 800w,/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Multivariate%20Gaussian%20with%20Principal%20Directions.png" class="img-fluid" width="600" height="400" alt="Multivariate Gaussian with Principal Directions" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The eigenvectors point along the primary axes of the data, while their lengths, scaled by the square roots of the eigenvalues, indicate how far the data stretches along these axes. This visualization immediately tells us the directions in which our data is most (or least) spread out.</p> <h4 id="step-2-visualizing-the-covariance-matrix-as-an-ellipse">Step 2: Visualizing the Covariance Matrix as an Ellipse</h4> <p>To highlight the covariance structure even further, we can draw the ellipse representing a particular level set of our Gaussian density. Here’s the code to do so, using the eigenvalues and eigenvectors to construct an ellipse around the data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot data with covariance ellipse
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data Points</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Vector</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Covariance Ellipse of the Multivariate Gaussian Distribution</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Draw the ellipse
</span><span class="kn">from</span> <span class="n">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">degrees</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>  <span class="c1"># Ellipse axes scaled to represent variance
</span><span class="n">ellipse</span> <span class="o">=</span> <span class="nc">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="sh">'</span><span class="s">None</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Covariance Ellipse</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>This plot gives us a clear visual representation of the covariance matrix’s “footprint” on the data:</p> <ul> <li><strong>Direction</strong>: The ellipse’s major and minor axes correspond to the principal directions (eigenvectors) of the data spread.</li> <li><strong>Length of Axes</strong>: The lengths of these axes are proportional to the square roots of the eigenvalues of \(\Sigma\), indicating the variance along each direction.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution-480.webp 480w,/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution-800.webp 800w,/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-06-11/Covariance%20Ellipse%20of%20the%20Multivariate%20Gaussian%20Distribution.png" class="img-fluid" width="600" height="400" alt="Covariance Ellipse of the Multivariate Gaussian Distribution" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="geometric-takeaways">Geometric Takeaways</h3> <p>So, what does this ellipse tell us about the data’s geometry?</p> <ol> <li><strong>Primary Spread</strong>: The major axis shows where the data is most dispersed, aligned along the eigenvector with the largest eigenvalue. This direction represents the direction of greatest variance.</li> <li><strong>Secondary Spread</strong>: The minor axis, perpendicular to the major axis, aligns with the eigenvector of the smaller eigenvalue, showing where the data is more tightly clustered.</li> </ol> <p>This elliptical geometry offers a powerful intuition: <strong>the covariance matrix defines an oriented and scaled ellipse that encapsulates the data’s spread and directionality. By examining this shape, we can quickly grasp the underlying structure of the distribution—its direction of spread, symmetry (or lack thereof), and how tightly data points are packed.</strong></p> <hr/> <h2 id="a-practical-example-of-deriving-the-covariance-matrix">A Practical Example of Deriving the Covariance Matrix</h2> <p>To ground our understanding of the covariance matrix in something concrete, let’s work through a hands-on example. We’ll take a simple two-dimensional random variable, calculate its mean vector and covariance matrix from a small dataset, and observe how these values summarize our data.</p> <p>Imagine we have a dataset representing the measurements of two variables, say \(X_1\) and \(X_2\), for simplicity. This could be anything—a set of financial returns, height and weight pairs, or temperatures at different locations. Let’s define our random variable \(X\) as:</p> \[X = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix}.\] <p>We’ll calculate the sample mean vector \(\hat{\mu}\) and the sample covariance matrix \(\hat{\Sigma}\) based on observed data.</p> <h3 id="step-1-calculate-the-sample-mean-vector">Step 1: Calculate the Sample Mean Vector</h3> <p>Let’s start with the sample mean vector, which captures the “central location” of the data. Given \(N\) observations, we define the sample mean \(\hat{\mu}\) as:</p> \[\hat{\mu} = \frac{1}{N} \sum_{i=1}^N X_i,\] <p>where \(X_i\) represents the \(i\)-th observation of our random variable \(X\).</p> <p>For instance, suppose we have the following five observations:</p> \[X_1 = \begin{bmatrix} 2 \\ 3 \end{bmatrix}, \quad X_2 = \begin{bmatrix} 3 \\ 5 \end{bmatrix}, \quad X_3 = \begin{bmatrix} 5 \\ 7 \end{bmatrix}, \quad X_4 = \begin{bmatrix} 6 \\ 8 \end{bmatrix}, \quad X_5 = \begin{bmatrix} 8 \\ 10 \end{bmatrix}.\] <p>The sample mean vector \(\hat{\mu}\) is then calculated as:</p> \[\hat{\mu} = \frac{1}{5} \left( \begin{bmatrix} 2 \\ 3 \end{bmatrix} + \begin{bmatrix} 3 \\ 5 \end{bmatrix} + \begin{bmatrix} 5 \\ 7 \end{bmatrix} + \begin{bmatrix} 6 \\ 8 \end{bmatrix} + \begin{bmatrix} 8 \\ 10 \end{bmatrix} \right).\] <p>Breaking it down component-wise:</p> \[\hat{\mu}_1 = \frac{1}{5} (2 + 3 + 5 + 6 + 8) = 4.8, \quad \hat{\mu}_2 = \frac{1}{5} (3 + 5 + 7 + 8 + 10) = 6.6.\] <p>Thus, our mean vector is:</p> \[\hat{\mu} = \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix}.\] <h3 id="step-2-calculate-the-sample-covariance-matrix">Step 2: Calculate the Sample Covariance Matrix</h3> <p>Next, let’s calculate the sample covariance matrix, which tells us not only the variability of each variable but also how \(X_1\) and \(X_2\) move in relation to each other. The sample covariance matrix \(\hat{\Sigma}\) is defined as:</p> \[\hat{\Sigma} = \frac{1}{N-1} \sum_{i=1}^N (X_i - \hat{\mu})(X_i - \hat{\mu})^T.\] <p>For our five observations, \(N = 5\), so we’ll divide by \(4\) (that’s \(N - 1\)).</p> <p>Let’s compute each term \((X_i - \hat{\mu})(X_i - \hat{\mu})^T\) for each observation:</p> <ol> <li> <p><strong>For \(X_1 = \begin{bmatrix} 2 \\ 3 \end{bmatrix}\):</strong></p> \[X_1 - \hat{\mu} = \begin{bmatrix} 2 \\ 3 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} -2.8 \\ -3.6 \end{bmatrix},\] <p>and</p> \[(X_1 - \hat{\mu})(X_1 - \hat{\mu})^T = \begin{bmatrix} -2.8 \\ -3.6 \end{bmatrix} \begin{bmatrix} -2.8 &amp; -3.6 \end{bmatrix} = \begin{bmatrix} 7.84 &amp; 10.08 \\ 10.08 &amp; 12.96 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_2 = \begin{bmatrix} 3 \\ 5 \end{bmatrix}\):</strong></p> \[X_2 - \hat{\mu} = \begin{bmatrix} 3 \\ 5 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} -1.8 \\ -1.6 \end{bmatrix},\] <p>and</p> \[(X_2 - \hat{\mu})(X_2 - \hat{\mu})^T = \begin{bmatrix} -1.8 \\ -1.6 \end{bmatrix} \begin{bmatrix} -1.8 &amp; -1.6 \end{bmatrix} = \begin{bmatrix} 3.24 &amp; 2.88 \\ 2.88 &amp; 2.56 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_3 = \begin{bmatrix} 5 \\ 7 \end{bmatrix}\):</strong></p> \[X_3 - \hat{\mu} = \begin{bmatrix} 5 \\ 7 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.4 \end{bmatrix},\] <p>and</p> \[(X_3 - \hat{\mu})(X_3 - \hat{\mu})^T = \begin{bmatrix} 0.2 \\ 0.4 \end{bmatrix} \begin{bmatrix} 0.2 &amp; 0.4 \end{bmatrix} = \begin{bmatrix} 0.04 &amp; 0.08 \\ 0.08 &amp; 0.16 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_4 = \begin{bmatrix} 6 \\ 8 \end{bmatrix}\):</strong></p> \[X_4 - \hat{\mu} = \begin{bmatrix} 6 \\ 8 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 1.4 \end{bmatrix},\] <p>and</p> \[(X_4 - \hat{\mu})(X_4 - \hat{\mu})^T = \begin{bmatrix} 1.2 \\ 1.4 \end{bmatrix} \begin{bmatrix} 1.2 &amp; 1.4 \end{bmatrix} = \begin{bmatrix} 1.44 &amp; 1.68 \\ 1.68 &amp; 1.96 \end{bmatrix}.\] </li> <li> <p><strong>For \(X_5 = \begin{bmatrix} 8 \\ 10 \end{bmatrix}\):</strong> \(X_5 - \hat{\mu} = \begin{bmatrix} 8 \\ 10 \end{bmatrix} - \begin{bmatrix} 4.8 \\ 6.6 \end{bmatrix} = \begin{bmatrix} 3.2 \\ 3.4 \end{bmatrix},\) and \((X_5 - \hat{\mu})(X_5 - \hat{\mu})^T = \begin{bmatrix} 3.2 \\ 3.4 \end{bmatrix} \begin{bmatrix} 3.2 &amp; 3.4 \end{bmatrix} = \begin{bmatrix} 10.24 &amp; 10.88 \\ 10.88 &amp; 11.56 \end{bmatrix}.\)</p> </li> </ol> <p>Now, we add these matrices and divide by \(N - 1 = 4\) to get the sample covariance matrix:</p> \[\hat{\Sigma} = \frac{1}{4} \left( \begin{bmatrix} 7.84 &amp; 10.08 \\ 10.08 &amp; 12.96 \end{bmatrix} + \begin{bmatrix} 3.24 &amp; 2.88 \\ 2.88 &amp; 2.56 \end{bmatrix} + \begin{bmatrix} 0.04 &amp; 0.08 \\ 0.08 &amp; 0.16 \end{bmatrix} + \begin{bmatrix} 1.44 &amp; 1.68 \\ 1.68 &amp; 1.96 \end{bmatrix} + \begin{bmatrix} 10.24 &amp; 10.88 \\ 10.88 &amp; 11.56 \end{bmatrix} \right).\] <p>After summing the matrices:</p> \[\hat{\Sigma} = \frac{1}{4} \begin{bmatrix} 22.8 &amp; 25.6 \\ 25.6 &amp; 29.2 \end{bmatrix} = \begin{bmatrix} 5.7 &amp; 6.4 \\ 6.4 &amp; 7.3 \end{bmatrix}.\] <h3 id="summary">Summary</h3> <p>In this example, we’ve calculated both the sample mean vector and the sample covariance matrix. Our sample covariance matrix, \(\hat{\Sigma} = \begin{bmatrix} 5.7 &amp; 6.4 \\ 6.4 &amp; 7.3 \end{bmatrix}\), encapsulates the spread and relationship between \(X_1\) and \(X_2\). The off-diagonal terms (6.4) represent the covariance between \(X_1\) and \(X_2\), indicating a positive correlation, while the diagonal terms represent the variances of \(X_1\) and \(X_2\) individually.</p> <p>This simple calculation reinforces the power of the covariance matrix—it captures both the “spread” of each variable and their interaction, providing a complete picture of our data’s geometric and statistical structure.</p> <hr/> <h2 id="covariance-matrix-and-independence">Covariance Matrix and Independence</h2> <p>As we wrap up our journey into the world of covariance matrices, it’s fitting to address one of the most common misconceptions: the relationship between <strong>independence</strong> and <strong>uncorrelatedness</strong>. In the world of multivariate distributions, understanding whether two variables are independent or merely uncorrelated is crucial. The covariance matrix can give us valuable insights, but it has its limitations—especially when it comes to verifying true independence.</p> <h3 id="defining-independence">Defining Independence</h3> <p>In probability theory, two random variables \(X_i\) and \(X_j\) are defined to be <strong>independent</strong> if the occurrence of one has no influence on the probability distribution of the other. Mathematically, this means:</p> \[P(X_i \leq x, X_j \leq y) = P(X_i \leq x) \cdot P(X_j \leq y),\] <p>for all \(x\) and \(y\). In simpler terms, knowing the value of \(X_i\) provides no information about \(X_j\) and vice versa.</p> <h3 id="covariance-and-independence-a-subtle-distinction">Covariance and Independence: A Subtle Distinction</h3> <p>The covariance matrix tells us about the <strong>linear relationships</strong> between variables, but not necessarily about their independence. For two variables \(X_i\) and \(X_j\), the covariance \(\text{Cov}(X_i, X_j)\) is defined as:</p> \[\text{Cov}(X_i, X_j) = E[(X_i - E[X_i])(X_j - E[X_j])].\] <p>If \(X_i\) and \(X_j\) are independent, then \(\text{Cov}(X_i, X_j) = 0\). Independence implies that there is no relationship between the variables at all, which includes a lack of linear correlation. However, the reverse is not true: <strong>zero covariance does not imply independence</strong>.</p> <p>To see why, let’s explore this distinction mathematically and with an example.</p> <h3 id="proof-that-zero-covariance-does-not-imply-independence">Proof that Zero Covariance Does Not Imply Independence</h3> <p>To understand why zero covariance does not necessarily mean independence, consider two random variables \(X\) and \(Y\) that are uncorrelated (i.e., \(\text{Cov}(X, Y) = 0\)) but not independent.</p> <p>Let \(X\) be a standard normal random variable: \(X \sim N(0, 1)\). Now define \(Y = X^2\). Clearly, \(Y\) depends on \(X\); in fact, \(Y\) is entirely determined by \(X\), so \(X\) and \(Y\) are not independent.</p> <ol> <li> <p><strong>Calculating the Covariance</strong>: We’ll calculate \(\text{Cov}(X, Y)\) to see if they’re uncorrelated.</p> \[\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])].\] <p>Since \(X \sim N(0, 1)\), we have \(E[X] = 0\) and \(E[Y] = E[X^2] = \text{Var}(X) = 1\). So,</p> \[\text{Cov}(X, Y) = E[X(Y - 1)] = E[X(X^2 - 1)] = E[X^3 - X].\] </li> <li> <p><strong>Expectation of Odd Moments</strong>: Given that \(X\) is normally distributed with mean 0, all odd moments of \(X\) (such as \(E[X]\) and \(E[X^3]\)) are zero. Therefore,</p> \[\text{Cov}(X, Y) = E[X^3] - E[X] = 0 - 0 = 0.\] <p>Thus, \(\text{Cov}(X, Y) = 0\), indicating that \(X\) and \(Y\) are uncorrelated. But as we know, \(Y = X^2\), which is entirely determined by \(X\). Therefore, they are not independent, even though they are uncorrelated.</p> </li> </ol> <p>This example highlights a key takeaway: <strong>uncorrelated variables are not necessarily independent</strong>. Covariance only captures linear relationships, so two variables could have a non-linear dependency and still exhibit zero covariance.</p> <h3 id="role-of-the-covariance-matrix-in-independence-analysis">Role of the Covariance Matrix in Independence Analysis</h3> <p>The covariance matrix \(\Sigma\) provides us with a way to examine linear dependencies between variables. Each off-diagonal element \(\Sigma\_{ij} = \text{Cov}(X_i, X_j)\) indicates the extent to which two variables vary together. If all off-diagonal entries of \(\Sigma\) are zero, we know that each pair of variables is <strong>uncorrelated</strong>. However, this does not guarantee independence.</p> <p>For multivariate normal distributions, however, the story is simpler. In a multivariate normal distribution, uncorrelated variables are indeed independent. Specifically, if \(X \sim N(\mu, \Sigma)\), then zero off-diagonal entries in \(\Sigma\) imply that the components of \(X\) are independent. This is a special property of the Gaussian distribution and does not hold in general.</p> <h3 id="practical-implications">Practical Implications</h3> <p>When analyzing real-world data, it’s essential to remember that zero covariance should not be mistaken for independence unless you’re specifically working with a multivariate Gaussian. In most other cases, zero covariance merely indicates a lack of linear relationship. Non-linear dependencies, which are common in fields like finance, biology, and machine learning, can often “hide” behind zero covariance.</p> <p>In summary, the covariance matrix is a powerful tool for understanding linear relationships but is limited in detecting true independence. When we need a full assessment of independence, especially in non-Gaussian contexts, we have to look beyond \(\Sigma\) and consider additional techniques, such as analyzing joint distributions or using tests for independence.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>The mean vector serves as the “center of gravity” for the data, shifting predictably under transformations and providing a concise summary of central tendencies across dimensions. Meanwhile, the covariance matrix is a lens into the data’s geometry. It encodes not only the individual variances of each variable but also the pairwise interactions that reveal whether variables rise and fall together or act independently. This matrix’s symmetry and positive semi-definiteness give it a unique role in shaping data contours and helping analysts visualize distribution shape, orientation, and spread.</p> <p>But as we’ve seen, the covariance matrix also has its limits. While it efficiently captures linear relationships, it cannot capture the entire complexity of data dependencies. As our exploration of independence versus uncorrelatedness shows, true independence requires more than just a zero covariance—especially in non-Gaussian settings where nonlinear dependencies can lie hidden. Understanding these nuances is crucial for effective data analysis. In real-world applications, whether in finance, biology, or machine learning, knowing when the covariance matrix can (and cannot) tell the full story allows us to apply these tools more accurately and creatively.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[A smart approach to capturing diverse data patterns without overwhelming resources.]]></summary></entry><entry><title type="html">Sampling Smarter: Unlocking the Power of Latin Hypercube Sampling</title><link href="https://shuhongdai.github.io/blog/2024/Latin_Hypercube_Sampling/" rel="alternate" type="text/html" title="Sampling Smarter: Unlocking the Power of Latin Hypercube Sampling"/><published>2024-05-03T00:32:10+00:00</published><updated>2024-05-03T00:32:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/Latin_Hypercube_Sampling</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/Latin_Hypercube_Sampling/"><![CDATA[<hr/> <h2 id="what-is-latin-hypercube-sampling">What is Latin Hypercube Sampling?</h2> <p>Imagine you’re standing over a large chessboard, each square representing a potential outcome of some experiment. The squares are spread out, spanning the entire board, and the goal is to gather a sample of outcomes that gives you the best possible understanding of the whole chessboard—not just a corner, not just a few scattered patches, but everywhere. Latin Hypercube Sampling (LHS) is a smart way to do this, ensuring that each row and column of the board has exactly one selected square. It’s like a carefully orchestrated game where you end up with one piece in every row and every column, giving you a complete sense of the landscape. In the world of data science, this chessboard metaphor expands into multidimensional space. Instead of just two dimensions—like the chessboard’s rows and columns—think of an entire universe of variables, each one adding a new dimension. If you’re trying to model something complex, like how different factors affect climate or how various inputs influence the outcome of an engineering system, you need a way to efficiently sample from all these different dimensions.</p> <p>That’s where LHS shines. Unlike random sampling, which might leave some regions underrepresented while others get chosen repeatedly, LHS spreads the samples evenly across each dimension. Think of it as ensuring every corner of the universe of possibilities gets its fair share of attention. Each sample is like a probe that’s perfectly positioned to gather information from every aspect of the system, without clustering too much in any one place.</p> <p>The key idea behind LHS is balance—the same kind of balance you see when picking one square from each row and column on the chessboard. In a higher-dimensional space, this means that every slice, or segment, of each variable range is covered, guaranteeing that the entire spectrum of potential values is represented. Whether you’re dealing with three variables or thirty, LHS keeps the sampling efficient and comprehensive, giving you a representative cross-section of all possible outcomes without redundancy or waste. This balanced sampling can be crucial in many practical applications. Imagine trying to predict the success of a new product launch by varying factors like price, marketing budget, and target audience. With LHS, you’re not just randomly throwing darts; you’re making sure that every aspect—from low budgets to high, from niche audiences to broad appeal—is represented in a structured way. The end result? A clearer picture of how each factor interacts, and ultimately, better insights and more informed decisions.</p> <p>LHS isn’t just about getting data—it’s about getting the right data, the kind that captures complexity without unnecessary repetition. In the chapters ahead, we’ll dive deeper into how LHS works in practice, explore its benefits, and compare it to other sampling methods, illustrating why this approach has become a favorite among statisticians and engineers alike.</p> <hr/> <h2 id="why-do-we-need-latin-hypercube-sampling">Why Do We Need Latin Hypercube Sampling?</h2> <p>LHS is a powerful tool for efficiently understanding complex systems when resources are limited. In the real world, whether we are studying natural phenomena, building financial models, or designing new technology, we often face too many possible combinations of factors to analyze exhaustively. LHS provides a smart way to sample the most relevant data without having to examine every possibility.</p> <p>Think of tasting a big pot of vegetable soup. If you randomly scoop just once, you might get mostly broth or only one type of vegetable, missing the full flavor. LHS ensures that each spoonful represents every major ingredient, giving a balanced understanding of the whole pot. Similarly, LHS ensures that every part of a complex system is represented, giving us a clearer, more complete picture. This method is particularly useful in high-dimensional problems, like understanding energy consumption in a building, which depends on factors such as temperature, occupancy, lighting, and insulation. Random sampling might leave some factor combinations out, leading to gaps in understanding. LHS ensures that all relevant combinations are covered, making it easier to see the relationships without redundant or overlooked data. The efficiency of LHS is one of its biggest advantages. For instance, in modeling the effects of a new drug, with many variables like dosage and genetic markers, LHS allows for fewer, but strategically chosen, trials that still provide a comprehensive view. This means we can extract meaningful insights without the need for an overwhelming number of experiments.</p> <p>In practical applications, whether constrained by budget, time, or logistics, LHS proves invaluable. Engineers can use it to test a manageable number of car engine configurations, while scientists can employ it to study pollutant spread efficiently. LHS provides a strategic way to cover the landscape, ensuring no critical area is neglected.</p> <h2 id="how-does-latin-hypercube-sampling-work">How Does Latin Hypercube Sampling Work?</h2> <p>To truly understand LHS and appreciate its unique capabilities, we’ll delve into the mathematics underpinning it. Through a sequence of structured steps, we’ll see how LHS ensures efficient, balanced sampling in multidimensional space. This method systematically constructs sample points to provide a representative cross-section of complex systems, thereby capturing the essence of multidimensional data without unnecessary redundancy.</p> <h3 id="1-problem-setup">1. Problem Setup</h3> <p>Consider a \(d\)-dimensional parameter space where each parameter \(x_i\) can take values within a defined interval \([a_i, b_i]\). Our objective is to generate \(N\) distinct sample points \(x^{(1)}, x^{(2)}, \dots, x^{(N)}\), where each \(x^{(k)} = (x_1^{(k)}, x_2^{(k)}, \dots, x_d^{(k)})\) lies within this \(d\)-dimensional space. The challenge is to arrange these sample points such that they collectively cover the entire parameter space in a balanced and representative manner, avoiding clusters or sparse areas.</p> <h3 id="2-dividing-each-dimension-into-intervals">2. Dividing Each Dimension into Intervals</h3> <p>For each dimension \(x_i\), we begin by dividing the interval \([a_i, b_i]\) into \(N\) non-overlapping subintervals. Formally, we can define these intervals as:</p> \[[a_i, b_i] = \bigcup_{j=1}^N \left[ a_i + \frac{j-1}{N} (b_i - a_i), a_i + \frac{j}{N} (b_i - a_i) \right]\] <p>where each subinterval \(I\_{i,j}\) for dimension \(x_i\) is given by:</p> \[I_{i,j} = \left[ a_i + \frac{j-1}{N} (b_i - a_i), a_i + \frac{j}{N} (b_i - a_i) \right]\] <p>The length of each interval, \(\Delta x_i\), is consistent across \(x_i\):</p> \[\Delta x_i = \frac{b_i - a_i}{N}\] <p>This systematic partitioning ensures that each dimension \(x_i\) is split into \(N\) equal sections, which lays the groundwork for comprehensive sampling.</p> <h3 id="3-random-sampling-within-each-interval">3. Random Sampling within Each Interval</h3> <p>Within each subinterval \(I*{i,j}\) of dimension \(x_i\), we randomly select a point \(x*{i,j}\). This point can be represented mathematically as:</p> \[x_{i, j} = a_i + \frac{j-1}{N} (b_i - a_i) + U_{i,j} \cdot \Delta x_i\] <p>where \(U*{i,j} \sim \text{Uniform}(0,1)\) represents a uniformly distributed random variable within \([0,1]\). This formulation ensures that \(x*{i,j}\) falls randomly within the subinterval \(I\_{i,j}\), providing a sample point that respects the interval boundaries.</p> <h3 id="4-constructing-multidimensional-sample-points">4. Constructing Multidimensional Sample Points</h3> <p>To generate the complete set of \(N\) sample points in \(d\)-dimensional space, we must combine these dimension-specific sample points. This is done by assigning each sample from \(x_i\) to a unique subinterval in each dimension using a random permutation function \(\pi_i\). The permutation \(\pi_i\) for each dimension \(x_i\) randomly orders the indices \(\{1, 2, \dots, N\}\) such that each subinterval is represented exactly once.</p> <p>The \(k\)-th sample point in \(d\)-dimensional space is thus constructed as:</p> \[x^{(k)} = \left( x_{1, \pi_1(k)}, x_{2, \pi_2(k)}, \dots, x_{d, \pi_d(k)} \right)\] <p>This arrangement guarantees that each sample point spans a unique combination of subintervals across all dimensions, achieving an even spread and complete coverage of the parameter space.</p> <h3 id="5-proof-of-marginal-uniformity">5. Proof of Marginal Uniformity</h3> <p>One of the core strengths of LHS lies in its marginal uniformity. This property ensures that the samples are uniformly distributed along each individual dimension \(x_i\), even as they span multiple dimensions. Let’s delve into a formal explanation:</p> <ol> <li>Each dimension \(x*i\) is divided into \(N\) intervals \(I*{i,1}, I*{i,2}, \dots, I*{i,N}\), with a single sample \(x\_{i,j}\) taken from each interval.</li> <li>Each sample \(x\_{i,j}\) is drawn uniformly from within its interval, meaning that the probability of sampling any particular region within \([a_i, b_i]\) is evenly distributed.</li> <li>Consequently, for each dimension \(x_i\), the probability distribution of the sample points across intervals is uniform, with each subinterval receiving exactly one sample point.</li> </ol> <p>This uniformity across intervals ensures that the samples are well-distributed along each dimension, resulting in comprehensive and unbiased coverage of the space.</p> <h3 id="6-variance-reduction-in-latin-hypercube-sampling">6. Variance Reduction in Latin Hypercube Sampling</h3> <p>An essential advantage of LHS is its ability to reduce variance in the estimated outcomes, particularly when compared to simple random sampling (SRS). This reduction in variance leads to more accurate estimations with fewer samples. To illustrate this, we consider the variance of the sample mean \(\hat{f}\) when estimating the expectation \(\mathbb{E}[f(x)]\) for a function \(f(x)\) over our \(d\)-dimensional space.</p> <p>Given \(f(x)\), the sample mean \(\hat{f}\) based on \(N\) samples is:</p> \[\hat{f} = \frac{1}{N} \sum_{k=1}^N f(x^{(k)})\] <p>We can express the variance of \(\hat{f}\) as:</p> \[\text{Var}(\hat{f}) = \frac{1}{N^2} \sum_{k=1}^N \sum_{l=1}^N \text{Cov}(f(x^{(k)}), f(x^{(l)}))\] <p>In LHS, because each sample point \(f(x^{(k)})\) is drawn independently across distinct subintervals, the covariance \(\text{Cov}(f(x^{(k)}), f(x^{(l)}))\) for \(k \neq l\) is zero. This simplifies the variance expression to:</p> \[\text{Var}(\hat{f}) = \frac{1}{N^2} \sum_{k=1}^N \text{Var}(f(x^{(k)}))\] <h3 id="result-variance-reduction-in-lhs">Result: Variance Reduction in LHS</h3> <p>Since each sample \(f(x^{(k)})\) is uniformly distributed across the intervals, the variance in LHS is inherently reduced compared to simple random sampling. In SRS, the variance is:</p> \[\text{Var}_{\text{SRS}}(\hat{f}) = \frac{\sigma^2}{N}\] <p>where \(\sigma^2\) represents the population variance of \(f(x)\). In contrast, LHS ensures that each dimension’s intervals are well-covered, which results in:</p> \[\text{Var}_{\text{LHS}}(\hat{f}) \leq \frac{\sigma^2}{N}\] <p>This inequality demonstrates that LHS consistently achieves lower variance than simple random sampling, making it a more efficient and precise sampling method.</p> <hr/> <h2 id="example-optimizing-a-drug-dosage-experiment-with-lhs">Example: Optimizing a Drug Dosage Experiment with LHS</h2> <p>To truly appreciate how LHS works, let’s dive into a practical example in drug dosage research. Suppose we’re conducting a study to understand how different drug doses, patient ages, and body weights affect blood concentration levels. Instead of relying on simple random sampling, which might overlook certain important combinations, LHS allows us to achieve balanced sampling across multiple dimensions, making the experiment both efficient and insightful.</p> <h3 id="experiment-setup-and-goals">Experiment Setup and Goals</h3> <p>In this study, we have three key variables:</p> <ol> <li><strong>Dosage (\(x_1\))</strong>: Ranges from \([50, 200]\) mg.</li> <li><strong>Age (\(x_2\))</strong>: Ranges from \([20, 80]\) years.</li> <li><strong>Weight (\(x_3\))</strong>: Ranges from \([50, 100]\) kg.</li> </ol> <p>Our goal is to generate \(N = 5\) sample points that represent this multi-dimensional space well, ensuring each factor’s range is adequately covered. This approach will allow us to identify patterns without conducting an exhaustive set of tests.</p> <h3 id="step-1-dividing-each-dimension-into-intervals">Step 1: Dividing Each Dimension into Intervals</h3> <p>With \(N = 5\) samples, we divide each variable’s range into five non-overlapping intervals:</p> <ul> <li> <p><strong>Dosage (\(x_1\))</strong>: The range \([50, 200]\) mg is divided into five intervals of length \(\Delta x_1 = \frac{200 - 50}{5} = 30\) mg:</p> \[[50, 80], [80, 110], [110, 140], [140, 170], [170, 200]\] </li> <li> <p><strong>Age (\(x_2\))</strong>: The range \([20, 80]\) years is divided into five intervals of length \(\Delta x_2 = \frac{80 - 20}{5} = 12\) years:</p> \[[20, 32], [32, 44], [44, 56], [56, 68], [68, 80]\] </li> <li> <p><strong>Weight (\(x_3\))</strong>: The range \([50, 100]\) kg is divided into five intervals of length \(\Delta x_3 = \frac{100 - 50}{5} = 10\) kg:</p> \[[50, 60], [60, 70], [70, 80], [80, 90], [90, 100]\] </li> </ul> <p>This structured partitioning provides a foundation for balanced sampling within each dimension.</p> <h3 id="step-2-random-sampling-within-each-interval">Step 2: Random Sampling within Each Interval</h3> <p>Next, we randomly select a sample point within each subinterval. For example, within the first dosage interval \([50, 80]\), we select a random point \(x\_{1,1}\). We do the same for other intervals, ensuring a point is chosen within each range. This process guarantees that every segment of each variable’s range contributes to our sample set.</p> <p>Let’s say we get the following randomly chosen points for each dimension:</p> <ul> <li><strong>Dosage (\(x_1\))</strong>: 63, 94, 125, 157, 189</li> <li><strong>Age (\(x_2\))</strong>: 24, 39, 50, 63, 76</li> <li><strong>Weight (\(x_3\))</strong>: 52, 66, 78, 85, 93</li> </ul> <h3 id="step-3-constructing-multi-dimensional-sample-points">Step 3: Constructing Multi-Dimensional Sample Points</h3> <p>To create our final five sample points in three-dimensional space, we combine these points by applying random permutations to each dimension. For example, we can randomly shuffle each dimension’s values, resulting in the following combinations:</p> <ul> <li><strong>Dosage permutation</strong>: 125, 63, 189, 94, 157</li> <li><strong>Age permutation</strong>: 63, 24, 76, 39, 50</li> <li><strong>Weight permutation</strong>: 66, 78, 93, 52, 85</li> </ul> <p>Thus, our final sample points are:</p> \[(125, 63, 66)\] \[(63, 24, 78)\] \[(189, 76, 93)\] \[(94, 39, 52)\] \[(157, 50, 85)\] <h3 id="step-4-evaluating-the-balance-of-our-sample-set">Step 4: Evaluating the Balance of Our Sample Set</h3> <p>This arrangement ensures that each variable is evenly represented across its range. Every sample point combines different segments of each dimension, avoiding excessive clustering in any one area. Compared to simple random sampling, LHS guarantees a well-rounded representation of our parameter space, which is critical for accurately studying relationships between variables.</p> <h3 id="analyzing-the-results">Analyzing the Results</h3> <p>In this example, LHS allows us to effectively sample a three-dimensional space, covering a comprehensive range of dosage, age, and weight combinations with only five samples. This balanced approach gives us a holistic view of how these variables interact, helping researchers understand dosage effects across diverse patient profiles while minimizing experimental overhead.</p> <hr/> <h2 id="real-world-applications-of-latin-hypercube-sampling">Real-World Applications of Latin Hypercube Sampling</h2> <p>LHS may sound like a tool for abstract math, but its practical impact is very real. Across engineering, environmental science, finance, healthcare, and energy modeling, LHS is a quietly transformative technique, allowing researchers to gain clear insights without drowning in data. Imagine trying to paint a detailed landscape but having only a handful of colors and brushstrokes to work with—LHS is like using those few strokes in exactly the right places to capture the whole scene with remarkable accuracy. Here’s how LHS works its magic across different fields.</p> <p>In engineering, for example, LHS helps streamline design and testing. Think of a car manufacturer simulating thousands of design combinations to improve performance under diverse conditions like temperature, load, and speed. Instead of running endless trials, LHS allows engineers to test a fraction of those designs in a structured way, covering the full range of conditions without redundancy. The result? They get the insights needed to enhance performance with far fewer tests, saving time and resources without sacrificing precision.</p> <p>Environmental science is another area where LHS is indispensable. Imagine trying to model how pollutants might spread across a city. Factors like wind speed, direction, and geographic features all interact to shape pollution patterns. With LHS, researchers can simulate these combinations in a balanced way, ensuring that different scenarios are well represented without oversampling any particular case. This approach not only sharpens predictions but also provides critical insights for health policies and urban planning—essential when resources are tight, but accuracy is crucial.</p> <p>Finance is yet another domain where LHS proves its worth, especially in risk analysis. Financial analysts need to understand how a portfolio might behave across a range of market conditions—interest rates, currency fluctuations, inflation, and so on. Rather than randomly selecting scenarios, LHS strategically samples across these factors, capturing both typical and extreme market conditions. This balanced approach provides a clearer risk profile and enables better-informed investment decisions, all while keeping data requirements manageable.</p> <p>In healthcare, LHS is a valuable ally in clinical trials and biomedical research. Imagine testing a new drug across diverse patient characteristics like age, genetic profile, and lifestyle. Testing every combination is impossible, so LHS helps researchers select a representative set of patient scenarios. By covering each variable without redundancy, LHS ensures that no group is overlooked, giving a comprehensive view of the drug’s impact across different demographics, often revealing critical insights early on.</p> <p>Even energy modeling for buildings benefits from LHS. When designing sustainable buildings, architects need to know how energy usage changes based on insulation, occupancy, weather, and other factors. LHS allows for sampling across these variables in a way that efficiently covers the range of possible conditions. By using LHS to model these combinations, analysts can optimize energy efficiency without testing every single scenario, resulting in smarter, greener design choices.</p> <p>In each of these fields, LHS is the secret ingredient that transforms limited samples into broad, balanced insights, ensuring that complex systems are thoroughly represented without overwhelming resources. It’s a perfect example of sampling smarter, not harder, proving that sometimes, a carefully chosen subset can be just as powerful as an exhaustive dataset. As data-driven decisions become more central to progress, LHS is one tool that ensures those decisions are both efficient and well-informed.</p> <hr/> <h2 id="limitations-of-latin-hypercube-sampling">Limitations of Latin Hypercube Sampling</h2> <p>While LHS is a remarkably efficient method for capturing multidimensional data, it has limitations, particularly when dealing with high-dimensional spaces and dynamic systems. These challenges underscore that even advanced sampling techniques like LHS must sometimes be augmented or adapted to maintain efficiency and accuracy.</p> <h3 id="computational-demands-in-high-dimensions">Computational Demands in High Dimensions</h3> <p>One of the primary challenges with LHS arises in high-dimensional spaces. As the number of dimensions \(d\) grows, the complexity of generating \(N\) samples with balanced coverage across each dimension increases significantly. In lower-dimensional spaces, LHS achieves a clear advantage by ensuring that every interval in each dimension is represented. However, as \(d\) rises, this method begins to experience what’s known as the “curse of dimensionality,” where the sample space becomes exponentially large.</p> <p>To understand this more formally, consider that LHS divides each dimension’s interval into \(N\) equal parts, requiring \(N^d\) unique combinations to ensure complete coverage in the \(d\)-dimensional space. However, practical constraints often limit the total number of samples \(N\), leading to fewer possible combinations in the high-dimensional setting, which means that not all regions of the space are sampled as evenly. When the sampling coverage is incomplete, the variance reduction properties of LHS also diminish. The variance of a sample mean \(\hat{f}\) from LHS in high-dimensional spaces approximates as:</p> \[\text{Var}_{\text{LHS}}(\hat{f}) \approx \frac{\sigma^2}{N} \cdot \left( 1 + \frac{d - 1}{N} \right)\] <p>where \(\sigma^2\) is the population variance. This variance formula highlights that as \(d\) approaches \(N\), variance grows, and LHS’s advantage over simple random sampling (SRS) begins to diminish. In high-dimensional scenarios, achieving a representative sample with LHS can require exponentially more points to maintain the same precision, potentially making it less efficient than anticipated.</p> <h3 id="challenges-with-dynamic-systems">Challenges with Dynamic Systems</h3> <p>Another notable limitation of LHS lies in its static nature, which can be less effective for systems where variables have time-dependent relationships or feedback loops. LHS operates under the assumption that each variable can be sampled independently within its interval, which is reasonable for many static or quasi-static systems. However, in dynamic systems—such as those seen in financial markets, climate models, or real-time simulations—dependencies between variables often evolve over time, meaning the state of one variable may directly influence the others.</p> <p>For example, in a climate model where temperature, humidity, and wind speed are interdependent and change over time, simply sampling each dimension independently may miss critical interdependencies. Mathematically, if we denote a system state at time \(t\) as \(\mathbf{x}(t) = (x_1(t), x_2(t), \dots, x_d(t))\), then dynamic relationships between variables \(x_i(t)\) might require joint distribution sampling, something that LHS in its classical form doesn’t inherently accommodate.</p> <p>For dynamic models, we would ideally sample from the conditional distributions \(P(x*i(t) \vert x*{-i}(t))\), where \(x\_{-i}(t)\) represents the set of all other variables at time \(t\). However, traditional LHS treats each dimension independently, lacking the ability to conditionally update samples based on evolving states of other variables. As a result, alternative sampling techniques—such as sequential Monte Carlo (SMC) or particle filtering, which adapt to these dependencies—are often more appropriate for dynamic systems.</p> <h3 id="addressing-dependencies-and-dimensionality-constraints">Addressing Dependencies and Dimensionality Constraints</h3> <p>One way to address these limitations is by hybridizing LHS with other sampling techniques. For high-dimensional spaces, combining LHS with stratified sampling or Sobol sequences can mitigate the curse of dimensionality, ensuring better coverage in each dimension without requiring an impractical number of samples. In dynamic systems, integrating LHS with adaptive sampling techniques, where the sample distribution updates based on real-time data, may offer a way to retain the efficiency of LHS while accommodating evolving dependencies.</p> <hr/> <h2 id="demo">Demo</h2> <p>Here’s a demonstration of LHS across different dimensions. In 2D, we see LHS distributing sample points evenly across the grid, ensuring each part of the space is represented. In 3D, this principle extends gracefully, capturing a well-balanced spread across all three dimensions. Finally, in 4D, we employ dimensionality reduction to visualize the sampling density, revealing how LHS continues to provide comprehensive coverage even in complex, multi-dimensional settings.</p> <blockquote> <p>Updated on June 22, 2024: Additionally, the PCA method used for dimensionality reduction in 4D sampling is detailed with examples in <a href="https://shuhongdai.github.io/blog/2024/Correlation_Coefficients/#pca-extracting-linear-patterns">our latest blog post</a>.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-05-03/2d-480.webp 480w,/assets/posts_img/2024-05-03/2d-800.webp 800w,/assets/posts_img/2024-05-03/2d-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-05-03/2d.png" class="img-fluid" width="600" height="400" alt="2d" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-05-03/3d-480.webp 480w,/assets/posts_img/2024-05-03/3d-800.webp 800w,/assets/posts_img/2024-05-03/3d-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-05-03/3d.png" class="img-fluid" width="600" height="400" alt="3d" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/posts_img/2024-05-03/4d-480.webp 480w,/assets/posts_img/2024-05-03/4d-800.webp 800w,/assets/posts_img/2024-05-03/4d-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/posts_img/2024-05-03/4d.png" class="img-fluid" width="600" height="400" alt="4d" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">qmc</span>

<span class="c1"># Set global Seaborn style for consistent visualization
</span><span class="n">sns</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="sh">"</span><span class="s">whitegrid</span><span class="sh">"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="sh">"</span><span class="s">muted</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Number of samples and different dimensions to illustrate LHS sampling
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">dimensions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="c1"># Function to generate Latin Hypercube Sampling points
</span><span class="k">def</span> <span class="nf">generate_lhs_samples</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">qmc</span><span class="p">.</span><span class="nc">LatinHypercube</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">.</span><span class="nf">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sample</span>

<span class="c1"># Function to reduce dimensions using PCA (for high-dimensional data)
</span><span class="k">def</span> <span class="nf">reduce_dimension</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Plotting 2D LHS samples
</span><span class="k">def</span> <span class="nf">plot_2d_lhs</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">2D LHS Sampling</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 1</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Plotting 3D LHS samples with a 3D perspective
</span><span class="k">def</span> <span class="nf">plot_3d_lhs</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">3D LHS Sampling</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="sh">'</span><span class="s">3d</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># Create 3D projection
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 1</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Dimension 3</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Plotting 4D LHS samples reduced to 2D with density map
</span><span class="k">def</span> <span class="nf">plot_4d_lhs_with_density</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">4D LHS Sampling (PCA Reduced)</span><span class="sh">"</span><span class="p">):</span>
    <span class="c1"># Apply PCA to reduce the 4D data to 2D
</span>    <span class="n">reduced_samples</span> <span class="o">=</span> <span class="nf">reduce_dimension</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">reduced_samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="c1"># Plot density heatmap using kdeplot
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Blues</span><span class="sh">"</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="c1"># Overlay the scatter plot for sampled points
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">PCA Dimension 1</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">PCA Dimension 2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Loop through the specified dimensions and generate the corresponding plots
</span><span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">dimensions</span><span class="p">:</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="nf">generate_lhs_samples</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Plot for 2D LHS sampling
</span>        <span class="nf">plot_2d_lhs</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">2D LHS Sampling</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Plot for 3D LHS sampling with 3D view
</span>        <span class="nf">plot_3d_lhs</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">3D LHS Sampling</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="c1"># Plot for 4D LHS sampling after PCA reduction with density visualization
</span>        <span class="nf">plot_4d_lhs_with_density</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">4D LHS Sampling (PCA Reduced)</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="wrapping-it-all-up">Wrapping It All Up</h2> <p>LHS is, without a doubt, an impressive technique. It’s one of those methods that shows just how much smarter sampling can be than a simple “grab a few random points and hope for the best” approach. By strategically covering each dimension and ensuring no corner of our data space is left unexplored, LHS brings precision and balance to the chaotic world of complex systems. Whether you’re testing car engines, predicting financial risk, modeling climate change, or designing clinical trials, LHS has a unique knack for extracting the most insight with the least effort. It’s efficiency and elegance wrapped into one neat package.</p> <p>That said, LHS is not a cure-all. As we explored, it starts to stumble in high dimensions, where the dreaded curse of dimensionality can make even the most elegant sampling methods feel a bit sluggish. And when variables are in flux, like in dynamic systems, LHS’s simple structure can’t quite capture the dance of interdependencies over time. In those cases, it’s like trying to catch a shadow—by the time you sample one point, the system has already changed. But here’s the fun part about a method like LHS: it has this air of adaptability. While it might not suit every situation perfectly, it can be hybridized, adjusted, and even reinvented to suit new needs. I think that’s why it’s so appealing to both engineers and data scientists alike.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[A smart approach to capturing diverse data patterns without overwhelming resources.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://shuhongdai.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://shuhongdai.github.io/blog/2024/tabs</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="a497a973-d530-4241-b9b9-b1545c0de54b" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="a497a973-d530-4241-b9b9-b1545c0de54b" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="2928ce5a-1841-4203-a086-0165efb4e305" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="2928ce5a-1841-4203-a086-0165efb4e305" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="4d77b120-dc5f-434f-9012-ce64508a7858" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="4d77b120-dc5f-434f-9012-ce64508a7858" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://shuhongdai.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://shuhongdai.github.io/blog/2024/typograms</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://shuhongdai.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://shuhongdai.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://shuhongdai.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://shuhongdai.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://shuhongdai.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry></feed>