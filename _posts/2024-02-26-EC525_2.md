---
layout: distill
title: "Problem Ⅱ: Linear Transformations"
typograms: true
tikzjax: true
featured: 
thumbnail:
category:  A Commentary of the Pre-Practice for Boston University's EC525 Course
tags: ["Mathematics"]
date: 2024-02-26


toc:
  - name: Given Conditions
  - name: '(a) Let $V=\mathbb{R}$ and let the addition and multiplication operations be the standard addition and multiplication in $\mathbb{R}$. Show that $V$ is a vector space. '
  - name: (b) Let $X$ be some arbitrary space and let $F$ be the set of functions $f:X\to\mathbb{R}.$ Given any two functions $f$ and $g$, we can define their sum $k:=f+g$ as the function $k(x)=f(x)+g(x).$ Further, given a scalar $s$ and a function $f$, we define the scalar multiplication $k:=s\cdot f$ as the function $k(x)=s\times f(x)$, where $\times$ indicates standard multiplication of real numbers. Show that $F$ with these addition and scalar multiplication operators is a vector space.
  - name: (c) Show that the additive identity is unique. Specifically, $i\in V$ is such that there exists some $w\in V$ such that $i+w=w$, then $i=\vec{0}$.
  - name: (d) Show that $\vec{0}$ satisfes $s\cdot\vec{0}=\vec{0}$ for all $s\in\mathbb{R}$.
  - name: (e) Show that $0\in\mathbb{R}$ satisfies $0\cdot w=\vec{0}$ for all $w\in V$.
  - name: (f) Suppose (just for this question) that the vector spaces need not have additive inverses, so that you cannot rely on that axiom when proving this statement. Show that nevertheless, if $i\in V$ is such that $i+w=w$ $\tilde{\text{for all }w}$,then $i=\vec{0}.$ Note that this is a weaker result than question (c) (why?).
  - name: (g) Show that the additive inverses are unique. That is, if $v\in V$ and $w$ and $w^{\prime}$ are two additive inverses for $v$, then $w=w^{\prime}$.
  - name: (h) Show that if $w \neq v$ and $v'$ is the additive inverse of $v$, then $w + v' \neq \vec{0}$.
  - name: (i) Show that $-1 \in \mathbb{R}$ satisfies $(-1) \cdot v$ is the additive inverse of $v$ for all $v \in V$. This justifies the notation $-v$ to indicate the additive inverse of $v$.
  - name: (j) Show that if $v \neq \vec{0}$, then $s \cdot V \neq \vec{0}$ for all $s \neq 0$.
  - name: (k) Show that if $v$ is equal to its own additive inverse, then $v = \vec{0}$.


---

## Given Conditions
 If $V$ and $W$ are two different vector spaces, a \textit{linear map} $L$ from $V$ to $W$ is a function $L:V\to W$ such that
- $L(v+v^{\prime})=L(v)+L(v^{\prime})$ for all $v,v^{\prime}\in V$.
- $L(s\cdot v)=s\cdot L(v)$ for all $s\in\mathbb{R}$, $v\in V$.



**Quick Navigation** ⬇️

- [Preface: Motivation and Overview](https://shuhongdai.github.io/blog/2023/EC525_0/)

- [Problem Ⅰ: Vector Spaces](https://shuhongdai.github.io/blog/2023/EC525_1/) 

- **Problem Ⅱ: Linear Transformations** (You are currently browsing this post)

- [Problem Ⅲ: Group Theory](https://shuhongdai.github.io/blog/2024/EC525_3/) 

- [Problem Ⅳ: Dual Spaces and Functional Analysis](https://shuhongdai.github.io/blog/2024/EC525_4/) 

- [Problem Ⅴ: Infinite Sequences and Combinatorial Principles](https://shuhongdai.github.io/blog/2024/EC525_5/) 

- [Problem Ⅵ: Set Theory and Cardinality](https://shuhongdai.github.io/blog/2024/EC525_6/) 

- Conclusion

---


## （a) Show that if $L:V\to W$ and $H:W\to X$ are both linear maps, then the composition $H\circ L:V\to X$ is a linear map.

**Proposition 2.a.1.** *Let $V, W, X$ be vector spaces over a field $\mathbb{R}$, and let $L: V \to W$ and $H: W \to X$ be linear maps. Then the composition $H \circ L: V \to X$ defined by $(H \circ L)(v) = H(L(v))$ for all $v \in V$ is linear*

*Proof.* By hypothesis, $L: V \to W$ is linear, hence for all $v_{1}, v_{2} \in V$ and all scalars $s \in \mathbb{R}$, the following identities hold:  

$$
L(v_{1} + v_{2}) = L(v_{1}) + L(v_{2})
\quad\text{and}\quad
L(s \cdot v_{1}) = s \cdot L(v_{1}).
$$  

Similarly, since $H: W \to X$ is linear, for all $w_{1}, w_{2} \in W$ and all $t \in \mathbb{R}$, one has:  

$$
H(w_{1} + w_{2}) = H(w_{1}) + H(w_{2})
\quad\text{and}\quad
H(t \cdot w_{1}) = t \cdot H(w_{1}).
$$

To establish linearity of the composition $H \circ L: V \to X$, one must verify that for all $v_{1}, v_{2} \in V$ and all $r \in \mathbb{R}$, the following equalities hold:  
$$
(H \circ L)(v_{1} + v_{2}) = (H \circ L)(v_{1}) + (H \circ L)(v_{2})
\quad\text{and}\quad
$$

$$
(H \circ L)(r \cdot v_{1}) = r \cdot (H \circ L)(v_{1}).
$$

Consider arbitrary vectors $v_{1}, v_{2} \in V$. Since $H \circ L$ is defined by composition, one has  
$$
(H \circ L)(v_{1} + v_{2}) = H(L(v_{1} + v_{2})).
$$  
By linearity of $L$,  
$$
L(v_{1} + v_{2}) = L(v_{1}) + L(v_{2}),
$$  
thus  
$$
(H \circ L)(v_{1} + v_{2}) = H(L(v_{1}) + L(v_{2})).
$$  
Since $H$ is linear, it preserves addition, so  
$$
H(L(v_{1}) + L(v_{2})) = H(L(v_{1})) + H(L(v_{2})).
$$  
Substitute back the definition of composition:  
$$
H(L(v_{1})) + H(L(v_{2})) = (H \circ L)(v_{1}) + (H \circ L)(v_{2}).
$$  
Hence we have shown that for all $v_{1}, v_{2} \in V$,  
$$
(H \circ L)(v_{1} + v_{2}) = (H \circ L)(v_{1}) + (H \circ L)(v_{2}).
$$

Next, consider an arbitrary vector $v \in V$ and an arbitrary scalar $r \in \mathbb{R}$. By the definition of composition,  
$$
(H \circ L)(r \cdot v) = H(L(r \cdot v)).
$$  
Since $L$ is linear, it respects scalar multiplication, yielding  
$$
L(r \cdot v) = r \cdot L(v).
$$  
Therefore,  
$$
(H \circ L)(r \cdot v) = H(r \cdot L(v)).
$$  
Because $H$ is linear, it also respects scalar multiplication, implying  
$$
H(r \cdot L(v)) = r \cdot H(L(v)).
$$  
Rewriting in terms of composition, this becomes  
$$
r \cdot H(L(v)) = r \cdot (H \circ L)(v).
$$  
Hence, for all $r \in \mathbb{R}$ and $v \in V$,  
$$
(H \circ L)(r \cdot v) = r \cdot (H \circ L)(v).
$$

Both of the defining properties of linear maps, namely the preservation of vector addition and scalar multiplication, have been verified for the map $H \circ L$. Consequently, $H \circ L$ is linear. $\textbf{Q.E.D.}$

> **Commentary**
>
>The essential elements of the proof rest on the axioms of linearity, which are stable under composition. Thus, the space of linear maps between vector spaces is closed under composition.

---

##  (b) Show that if $L: V \to W$ is a linear map, then $L(\vec{0}_{V}) = \vec{0}_{W}$, where $\vec{0}_{V}$ indicates the additive identity in $V$ and $\vec{0}_{W}$ indicates the additive identity in $W$.

**Proposition 2.b.1.** *Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$, and let $L: V \to W$ be a linear map. Then $L(\vec{0}_V) = \vec{0}_W$, where $\vec{0}_V$ and $\vec{0}_W$ denote the additive identities in $V$ and $W$, respectively.*

*Proof.* By the definition of a linear map, $L$ satisfies the following properties for all vectors $v, v' \in V$ and all scalars $s \in \mathbb{R}$:

$$
L(v + v') = L(v) + L(v') \quad \text{and} \quad L(s \cdot v) = s \cdot L(v).
$$

In particular, consider the vector $\vec{0}_V$, the additive identity in $V$. By the property of additive identities in vector spaces, for any vector $v \in V$, we have:

$$
v + \vec{0}_V = v.
$$

Applying the linear map $L$ to both sides of this equation yields:
$$
L(v + \vec{0}_V) = L(v).
$$
Using the additivity of $L$, the left-hand side can be expressed as:

$$
L(v + \vec{0}_V) = L(v) + L(\vec{0}_V).
$$

Thus, we have:

$$
L(v) + L(\vec{0}_V) = L(v).
$$

To isolate $L(\vec{0}_V)$, subtract $L(v)$ from both sides:

$$
L(v) + L(\vec{0}_V) - L(v) = L(v) - L(v).
$$

Simplifying both sides, we obtain:

$$
L(\vec{0}_V) = \vec{0}_W,
$$

where $\vec{0}_W$ is the additive identity in $W$, since the only element in $W$ that satisfies $w + \vec{0}_W = w$ for all $w \in W$ is $\vec{0}_W$ itself.

Alternatively, one may employ the homogeneity property of linear maps. Consider the scalar multiplication by zero, which yields:

$$
L(0 \cdot v) = 0 \cdot L(v).
$$

Since $0 \cdot v = \vec{0}_V$ for any $v \in V$, and $0 \cdot L(v) = \vec{0}_W$ in $W$, it follows that:

$$
L(\vec{0}_V) = \vec{0}_W.
$$

$\textbf{Q.E.D.}$

---

## \(c\) The kernel of a linear map $L:V\to W$ is the set of points $v\in V$ such that $L(v)=\vec{0}_{W}$. Show that a linear map $L:V\to W$ is 1-1 if and only if the kernel of $L$ consists of only the identity element $\vec{0}_{V}$.

**Proposition 2.c.1.** *Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$, and let $L: V \to W$ be a linear transformation. Then $L$ is injective (one-to-one) if and only if the kernel of $L$, denoted by $\ker(L)$, consists solely of the zero vector in $V$, that is, $\ker(L) = \{ \vec{0}_V \}$.*

*Proof.* We proceed by establishing both implications of the equivalence: 
1. Necessity: If $L$ is injective, then $\ker(L) = \{ \vec{0}_V \}$.
2. Sufficiency: If $\ker(L) = \{ \vec{0}_V \}$, then $L$ is injective.

Necessity: Assume that $L: V \to W$ is injective. We aim to show that $\ker(L) = \{ \vec{0}_V \}$.

By definition, the kernel of $L$ is the set:

$$
\ker(L) = \{ v \in V \vert L(v) = \vec{0}_W \}.
$$

To demonstrate that $\ker(L) = \{ \vec{0}_V \}$, we must show two inclusions:

$$
\ker(L) \subseteq \{ \vec{0}_V \} \quad \text{and} \quad \{ \vec{0}_V \} \subseteq \ker(L).
$$

The second inclusion is trivial since $L(\vec{0}_V) = \vec{0}_W$ by the linearity of $L$ (as established in Proposition 2.b.1).

For the first inclusion, suppose $v \in \ker(L)$. Then, by definition:

$$
L(v) = \vec{0}_W.
$$

Given that $L$ is injective, the only solution to $L(v) = \vec{0}_W$ is $v = \vec{0}_V$. To see this, consider the injectivity of $L$, which implies that if $L(v) = L(v')$, then $v = v'$. Specifically, taking $v' = \vec{0}_V$, we have:

$$
L(v) = \vec{0}_W = L(\vec{0}_V) \implies v = \vec{0}_V.
$$

Thus, $v$ must be the zero vector in $V$, and therefore:

$$
\ker(L) \subseteq \{ \vec{0}_V \}.
$$

Combining both inclusions, we conclude:

$$
\ker(L) = \{ \vec{0}_V \}.
$$

This establishes the necessity part of the proposition.

Sufficiency: Now, assume that $\ker(L) = \{ \vec{0}_V \}$. We aim to demonstrate that $L$ is injective.

To prove that $L$ is injective, we must show that for any $v_1, v_2 \in V$, if $L(v_1) = L(v_2)$, then $v_1 = v_2$.

Consider arbitrary vectors $v_1, v_2 \in V$ such that:

$$
L(v_1) = L(v_2).
$$

Subtracting $L(v_2)$ from both sides yields:

$$
L(v_1) - L(v_2) = \vec{0}_W.
$$

Utilizing the linearity of $L$, this can be rewritten as:

$$
L(v_1 - v_2) = \vec{0}_W.
$$

By the definition of the kernel, this implies:

$$
v_1 - v_2 \in \ker(L).
$$

Given that $\ker(L) = \{ \vec{0}_V \}$, it follows that:

$$
v_1 - v_2 = \vec{0}_V.
$$

Thus, we conclude:

$$
v_1 = v_2.
$$

This establishes that $L$ is injective, as required. $\textbf{Q.E.D.}$

---

##  (d) A *subspace* of a vector space $V$ is a subset $S \subset V$ such that for all $v, w \in S$, $v + w \in S$ and for all $v \in S$ and $c \in \mathbb{R}$, $c \cdot v \in S$. That is, the subspace is *closed* under the addition and scalar multiplication operations. Show that the kernel of a linear map $L: V \to W$ forms a subspace of the space $V$.


**Proposition 2.d.1** *Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$, and let $L: V \to W$ be a linear transformation. Then the kernel of $L$, denoted by $\ker(L)$, defined as
$$
\ker(L) = \{ v \in V \vert L(v) = \vec{0}_W \},
$$
is a subspace of $V$.*

*Proof.* To establish that $\ker(L)$ is a subspace of $V$, we must verify that $\ker(L)$ satisfies the three axioms defining a subspace within a vector space. Specifically, we need to confirm that:
1. $\ker(L)$ is non-empty.
2. $\ker(L)$ is closed under vector addition.
3. $\ker(L)$ is closed under scalar multiplication.

We proceed by verifying each of these properties in turn.

Non-emptiness of $\ker(L)$: By definition, a subspace must contain the zero vector of the ambient vector space. Consider the additive identity $\vec{0}_V \in V$. Applying the linear transformation $L$ to $\vec{0}_V$, we obtain:

$$
L(\vec{0}_V) = \vec{0}_W,
$$

as established in Proposition 2.b.1. Therefore, $\vec{0}_V \in \ker(L)$, which implies that $\ker(L)$ is non-empty.

 Closure under Vector Addition: Let $u, v \in \ker(L)$. By the definition of the kernel, this means:

$$
L(u) = \vec{0}_W \quad \text{and} \quad L(v) = \vec{0}_W.
$$

We must show that $u + v \in \ker(L)$, i.e., $L(u + v) = \vec{0}_W$.

Applying the linearity of $L$, we have:

$$
L(u + v) = L(u) + L(v).
$$

Substituting the known values from the kernel, this becomes:

$$
L(u + v) = \vec{0}_W + \vec{0}_W = \vec{0}_W.
$$

Thus, $u + v \in \ker(L)$, establishing closure under vector addition.

Closure under Scalar Multiplication: Let $v \in \ker(L)$ and let $c \in \mathbb{R}$ be an arbitrary scalar. We must demonstrate that $c \cdot v \in \ker(L)$, i.e., $L(c \cdot v) = \vec{0}_W$.

Applying the linearity of $L$ with respect to scalar multiplication, we obtain:

$$
L(c \cdot v) = c \cdot L(v).
$$

Since $v \in \ker(L)$, it follows that $L(v) = \vec{0}_W$. Substituting this into the equation above yields:

$$
L(c \cdot v) = c \cdot \vec{0}_W = \vec{0}_W.
$$

Therefore, $c \cdot v \in \ker(L)$, establishing closure under scalar multiplication. $\textbf{Q.E.D.}$

---

## (e) Show that the image of a linear map $L:V\to W$ is a subspace of $W$.

**Proposition 2.e.1.** *Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$, and let $L: V \to W$ be a linear map. Then the image of $L$, defined by
$$
\text{Im}(L) = \{ L(v) \mid v \in V \},
$$
is a subspace of $W$.*

*Proof.* To establish that $\text{Im}(L)$ is a subspace of $W$, it is necessary to verify that it satisfies the three axioms defining a subspace. Specifically, we must demonstrate that:
1. The zero vector $\vec{0}_W$ of $W$ is an element of $\text{Im}(L)$.
2.  $\text{Im}(L)$ is closed under vector addition.
3. $\text{Im}(L)$ is closed under scalar multiplication.


Containment of the Zero Vector: By the linearity of $L$, for any vector space $V$, the map $L$ satisfies

$$
L(\vec{0}_V) = \vec{0}_W.
$$

Here, $\vec{0}_V$ denotes the additive identity in $V$, and $\vec{0}_W$ denotes the additive identity in $W$. Since $\vec{0}_V \in V$, it follows directly from the definition of the image that

$$
\vec{0}_W = L(\vec{0}_V) \in \text{Im}(L).
$$

Thus, the zero vector of $W$ is contained within $\text{Im}(L)$.

Closure Under Vector Addition: Let $u, v \in \text{Im}(L)$. By the definition of the image, there exist vectors $u', v' \in V$ such that

$$
u = L(u') \quad \text{and} \quad v = L(v').
$$

Consider the sum $u + v$ in $W$. Applying the linearity of $L$, we have

$$
u + v = L(u') + L(v') = L(u' + v').
$$

Since $u' + v' \in V$ (as $V$ is a vector space and thus closed under addition), it follows that

$$
u + v = L(u' + v') \in \text{Im}(L).
$$

Therefore, $\text{Im}(L)$ is closed under vector addition.

Closure Under Scalar Multiplication: Let $v \in \text{Im}(L)$ and let $c \in \mathbb{R}$ be an arbitrary scalar. By the definition of the image, there exists a vector $v' \in V$ such that

$$
v = L(v').
$$

Consider the scalar multiple $c \cdot v$ in $W$. Utilizing the linearity of $L$, we obtain

$$
c \cdot v = c \cdot L(v') = L(c \cdot v').
$$

Since $c \cdot v' \in V$ (as $V$ is a vector space and thus closed under scalar multiplication), it follows that

$$
c \cdot v = L(c \cdot v') \in \text{Im}(L).
$$

Hence, $\text{Im}(L)$ is closed under scalar multiplication. $\textbf{Q.E.D.}$

---

##  (f) The set of linear maps from $V$ to $W$ is typically denoted $\mathcal{L}(V,W)$. Define an addition operation on linear maps as follows: given $F,G\in\mathcal{L}(V,W)$, set $K=F+G$ by $K(v)=F(v)+G(v)$, where the addition on the RHS here is the addition operation in $W$. Similarly, given $c\in\mathbb{R}$, we define a scalar multiplication $K=c\cdot F$ by $K(v)=c\cdot F(v)$, where again the scalar multiplication is the operation in $W$. Show that with these operations, $\mathcal{L}(V,W)$ is a vector space.

**Proposition 2.f.1.** *Let $V$ and $W$ be vector spaces over the field $\mathbb{R}$. Define the set $\mathcal{L}(V, W)$ to consist of all linear maps from $V$ to $W$. Equip $\mathcal{L}(V, W)$ with the operations of addition and scalar multiplication as defined below:

$$
(F + G)(v) = F(v) + G(v) \quad \text{for all } F, G \in \mathcal{L}(V, W) \text{ and } v \in V,
$$

$$
(c \cdot F)(v) = c \cdot F(v) \quad \text{for all } F \in \mathcal{L}(V, W), \, c \in \mathbb{R}, \text{ and } v \in V.
$$

Then $\mathcal{L}(V, W)$ endowed with these operations constitutes a vector space over $\mathbb{R}$.*

*Proof.* To establish that $\mathcal{L}(V, W)$ constitutes a vector space under the defined operations of addition and scalar multiplication, it is requisite to verify that it adheres to all the fundamental axioms that characterize a vector space. Specifically, we will verify the following properties:   

1. Closure under Addition:*For all $F, G \in \mathcal{L}(V, W)$, the map $F + G$ defined by $(F + G)(v) = F(v) + G(v)$ for all $v \in V$ is also an element of $\mathcal{L}(V, W)$.

2. Closure under Scalar Multiplication: For all $F \in \mathcal{L}(V, W)$ and $c \in \mathbb{R}$, the map $c \cdot F$ defined by $(c \cdot F)(v) = c \cdot F(v)$ for all $v \in V$ is also an element of $\mathcal{L}(V, W)$.

3. Associativity of Addition: For all $F, G, H \in \mathcal{L}(V, W)$, $(F + G) + H = F + (G + H)$.

4. Commutativity of Addition: For all $F, G \in \mathcal{L}(V, W)$, $F + G = G + F$.

5. Existence of Additive Identity: There exists a linear map $\Theta \in \mathcal{L}(V, W)$ such that for all $F \in \mathcal{L}(V, W)$, $F + \Theta = F$.

6. Existence of Additive Inverses: For each $F \in \mathcal{L}(V, W)$, there exists a linear map $-F \in \mathcal{L}(V, W)$ such that $F + (-F) = \Theta$.

7. Distributivity of Scalar Multiplication with Respect to Vector Addition: For all $c \in \mathbb{R}$ and $F, G \in \mathcal{L}(V, W)$, $c \cdot (F + G) = c \cdot F + c \cdot G$.

8. Distributivity of Scalar Multiplication with Respect to Field Addition: For all $c, d \in \mathbb{R}$ and $F \in \mathcal{L}(V, W)$, $(c + d) \cdot F = c \cdot F + d \cdot F$.

9. Compatibility of Scalar Multiplication with Field Multiplication: For all $c, d \in \mathbb{R}$ and $F \in \mathcal{L}(V, W)$, $c \cdot (d \cdot F) = (c d) \cdot F$.

10. Identity Element of Scalar Multiplication: For the scalar $1 \in \mathbb{R}$ and for all $F \in \mathcal{L}(V, W)$, $1 \cdot F = F$.

Closure under Addition: Let $F, G \in \mathcal{L}(V, W)$. We must show that $F + G$ is a linear map from $V$ to $W$, i.e., $F + G \in \mathcal{L}(V, W)$.

Additivity: For all $v_1, v_2 \in V$,

$$
(F + G)(v_1 + v_2) = F(v_1 + v_2) + G(v_1 + v_2).
$$

Since $F$ and $G$ are linear maps,

$$
F(v_1 + v_2) = F(v_1) + F(v_2),
$$

$$
G(v_1 + v_2) = G(v_1) + G(v_2).
$$

Thus,

$$
(F + G)(v_1 + v_2) = [F(v_1) + F(v_2)] + [G(v_1) + G(v_2)] 
$$

$$
= [F(v_1) + G(v_1)] + [F(v_2) + G(v_2)] = (F + G)(v_1) + (F + G)(v_2).
$$

Homogeneity: For all $c \in \mathbb{R}$ and $v \in V$,

$$
(F + G)(c \cdot v) = F(c \cdot v) + G(c \cdot v).
$$

Again, since $F$ and $G$ are linear maps,

$$
F(c \cdot v) = c \cdot F(v),
$$

$$
G(c \cdot v) = c \cdot G(v).
$$

Thus,

$$
(F + G)(c \cdot v) = c \cdot F(v) + c \cdot G(v) = c \cdot [F(v) + G(v)] = c \cdot (F + G)(v).
$$

Therefore, $F + G$ preserves both vector addition and scalar multiplication, and hence $F + G$ is linear. Consequently, $F + G \in \mathcal{L}(V, W)$, establishing closure under addition.

Closure under Scalar Multiplication: Let $F \in \mathcal{L}(V, W)$ and $c \in \mathbb{R}$. We must show that $c \cdot F$ is a linear map from $V$ to $W$, i.e., $c \cdot F \in \mathcal{L}(V, W)$.

Additivity: For all $v_1, v_2 \in V$,

$$
(c \cdot F)(v_1 + v_2) = c \cdot F(v_1 + v_2).
$$

Since $F$ is linear,

$$
F(v_1 + v_2) = F(v_1) + F(v_2).
$$

Thus,

$$
(c \cdot F)(v_1 + v_2) = c \cdot [F(v_1) + F(v_2)] = 
$$

$$
c \cdot F(v_1) + c \cdot F(v_2) = (c \cdot F)(v_1) + (c \cdot F)(v_2).
$$

Homogeneity: For all $d \in \mathbb{R}$ and $v \in V$,

$$
(c \cdot F)(d \cdot v) = c \cdot F(d \cdot v).
$$

Since $F$ is linear,

$$
F(d \cdot v) = d \cdot F(v).
$$

Thus,

$$
(c \cdot F)(d \cdot v) = c \cdot [d \cdot F(v)] = (c d) \cdot F(v) = d \cdot (c \cdot F)(v).
$$

Therefore, $c \cdot F$ preserves both vector addition and scalar multiplication, and hence $c \cdot F$ is linear. Consequently, $c \cdot F \in \mathcal{L}(V, W)$, establishing closure under scalar multiplication.

Associativity of Addition: Let $F, G, H \in \mathcal{L}(V, W)$. We must show that $(F + G) + H = F + (G + H)$.

For all $v \in V$,

$$
[(F + G) + H](v) = (F + G)(v) + H(v) = [F(v) + G(v)] + H(v) = 
$$

$$
F(v) + [G(v) + H(v)] = F(v) + (G + H)(v) = [F + (G + H)](v).
$$

Since this holds for all $v \in V$, we conclude that $(F + G) + H = F + (G + H)$.

Commutativity of Addition: Let $F, G \in \mathcal{L}(V, W)$. We must show that $F + G = G + F$.

For all $v \in V$,

$$
(F + G)(v) = F(v) + G(v) = G(v) + F(v) = (G + F)(v).
$$

Since this holds for all $v \in V$, we conclude that $F + G = G + F$.

Existence of Additive Identity: We must exhibit an element $\Theta \in \mathcal{L}(V, W)$ such that for all $F \in \mathcal{L}(V, W)$, $F + \Theta = F$.

Define $\Theta: V \to W$ by $\Theta(v) = \vec{0}_W$ for all $v \in V$, where $\vec{0}_W$ is the zero vector in $W$.

Linearity of $\Theta$: For all $v_1, v_2 \in V$ and $c \in \mathbb{R}$,

$$
\Theta(v_1 + v_2) = \vec{0}_W = \vec{0}_W + \vec{0}_W = \Theta(v_1) + \Theta(v_2),
$$

$$
\Theta(c \cdot v) = \vec{0}_W = c \cdot \vec{0}_W = c \cdot \Theta(v).
$$

Thus, $\Theta$ is linear, and hence $\Theta \in \mathcal{L}(V, W)$.

Additive Identity Property: For all $F \in \mathcal{L}(V, W)$ and $v \in V$,

$$
(F + \Theta)(v) = F(v) + \Theta(v) = F(v) + \vec{0}_W = F(v).
$$

Therefore, $F + \Theta = F$ for all $F \in \mathcal{L}(V, W)$, establishing the existence of an additive identity in $\mathcal{L}(V, W)$.

Existence of Additive Inverses: For each $F \in \mathcal{L}(V, W)$, we must exhibit a linear map $-F \in \mathcal{L}(V, W)$ such that $F + (-F) = \Theta$, where $\Theta$ is the additive identity in $\mathcal{L}(V, W)$.

Define $-F: V \to W$ by $(-F)(v) = -F(v)$ for all $v \in V$.

Linearity of $-F$: For all $v_1, v_2 \in V$ and $c \in \mathbb{R}$,

$$
(-F)(v_1 + v_2) = -F(v_1 + v_2) = -[F(v_1) + F(v_2)] =
$$

$$
 -F(v_1) - F(v_2) = (-F)(v_1) + (-F)(v_2),
$$

$$
(-F)(c \cdot v) = -F(c \cdot v) = -[c \cdot F(v)] = c \cdot (-F(v)) = c \cdot (-F)(v).
$$

Thus, $-F$ is linear, and hence $-F \in \mathcal{L}(V, W)$.

Additive Inverse Property: For all $F \in \mathcal{L}(V, W)$ and $v \in V$,

$$
(F + (-F))(v) = F(v) + (-F)(v) = F(v) - F(v) = \vec{0}_W = \Theta(v).
$$

Therefore, $F + (-F) = \Theta$, establishing the existence of additive inverses in $\mathcal{L}(V, W)$.

Distributivity of Scalar Multiplication with Respect to Vector Addition: Let $c \in \mathbb{R}$ and $F, G \in \mathcal{L}(V, W)$. We must show that

$$
c \cdot (F + G) = c \cdot F + c \cdot G.
$$

For all $v \in V$,

$$
[c \cdot (F + G)](v) = c \cdot (F + G)(v) = c \cdot [F(v) + G(v)] = 
$$

$$
c \cdot F(v) + c \cdot G(v) = [c \cdot F](v) + [c \cdot G](v) = [c \cdot F + c \cdot G](v).
$$

Since this holds for all $v \in V$, we conclude that $c \cdot (F + G) = c \cdot F + c \cdot G$.

Distributivity of Scalar Multiplication with Respect to Field Addition: Let $c, d \in \mathbb{R}$ and $F \in \mathcal{L}(V, W)$. We must show that

$$
(c + d) \cdot F = c \cdot F + d \cdot F.
$$

For all $v \in V$,

$$
[(c + d) \cdot F](v) = (c + d) \cdot F(v) = c \cdot F(v) + d \cdot F(v) = 
$$

$$
[c \cdot F](v) + [d \cdot F](v) = [c \cdot F + d \cdot F](v).
$$

Since this holds for all $v \in V$, we conclude that $(c + d) \cdot F = c \cdot F + d \cdot F$.

Compatibility of Scalar Multiplication with Field Multiplication: Let $c, d \in \mathbb{R}$ and $F \in \mathcal{L}(V, W)$. We must show that

$$
c \cdot (d \cdot F) = (c d) \cdot F.
$$

For all $v \in V$,

$$
[c \cdot (d \cdot F)](v) = c \cdot [d \cdot F(v)] = (c d) \cdot F(v) = [(c d) \cdot F](v).
$$

Since this holds for all $v \in V$, we conclude that $c \cdot (d \cdot F) = (c d) \cdot F$.

Identity Element of Scalar Multiplication: Let $F \in \mathcal{L}(V, W)$. We must show that

$$
1 \cdot F = F.
$$

For all $v \in V$,

$$
[1 \cdot F](v) = 1 \cdot F(v) = F(v).
$$

Thus, $1 \cdot F = F$. $\textbf{Q.E.D.}$

>**Commentray**
>
>In infinite-dimensional settings, while some properties may differ, the vector space structure of $\mathcal{L}(V, W)$ remains valid as long as operations are well-defined.
